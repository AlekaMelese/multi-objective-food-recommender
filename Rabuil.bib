
@misc{zhao_improving_2024,
	title = {Improving {Expert} {Radiology} {Report} {Summarization} by {Prompting} {Large} {Language} {Models} with a {Layperson} {Summary}},
	url = {http://arxiv.org/abs/2406.14500},
	doi = {10.48550/arXiv.2406.14500},
	abstract = {Radiology report summarization (RRS) is crucial for patient care, requiring concise "Impressions" from detailed "Findings." This paper introduces a novel prompting strategy to enhance RRS by first generating a layperson summary. This approach normalizes key observations and simplifies complex information using non-expert communication techniques inspired by doctor-patient interactions. Combined with few-shot in-context learning, this method improves the model's ability to link general terms to specific findings. We evaluate this approach on the MIMIC-CXR, CheXpert, and MIMIC-III datasets, benchmarking it against 7B/8B parameter state-of-the-art open-source large language models (LLMs) like Meta-Llama-3-8B-Instruct. Our results demonstrate improvements in summarization accuracy and accessibility, particularly in out-of-domain tests, with improvements as high as 5\% for some metrics.},
	urldate = {2025-07-10},
	publisher = {arXiv},
	author = {Zhao, Xingmeng and Wang, Tongnian and Rios, Anthony},
	month = jun,
	year = {2024},
	note = {arXiv:2406.14500 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{boll_distillnote_2025,
	title = {{DistillNote}: {LLM}-based clinical note summaries improve heart failure diagnosis},
	shorttitle = {{DistillNote}},
	url = {http://arxiv.org/abs/2506.16777},
	doi = {10.48550/arXiv.2506.16777},
	abstract = {Large language models (LLMs) offer unprecedented opportunities to generate concise summaries of patient information and alleviate the burden of clinical documentation that overwhelms healthcare providers. We present Distillnote, a framework for LLM-based clinical note summarization, and generate over 64,000 admission note summaries through three techniques: (1) One-step, direct summarization, and a divide-and-conquer approach involving (2) Structured summarization focused on independent clinical insights, and (3) Distilled summarization that further condenses the Structured summaries. We test how useful are the summaries by using them to predict heart failure compared to a model trained on the original notes. Distilled summaries achieve 79\% text compression and up to 18.2\% improvement in AUPRC compared to an LLM trained on the full notes. We also evaluate the quality of the generated summaries in an LLM-as-judge evaluation as well as through blinded pairwise comparisons with clinicians. Evaluations indicate that one-step summaries are favoured by clinicians according to relevance and clinical actionability, while distilled summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio) and significantly reduce hallucinations. We release our summaries on PhysioNet to encourage future research.},
	urldate = {2025-07-10},
	publisher = {arXiv},
	author = {Boll, Heloisa Oss and Boll, Antonio Oss and Boll, Leticia Puttlitz and Hanna, Ameen Abu and Calixto, Iacer},
	month = jun,
	year = {2025},
	note = {arXiv:2506.16777 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{jung_large_2025,
	title = {Large language models in medicine: {Clinical} applications, technical challenges, and ethical considerations},
	volume = {31},
	shorttitle = {Large language models in medicine},
	url = {https://synapse.koreamed.org/articles/1516090747},
	number = {2},
	urldate = {2025-07-10},
	journal = {Healthcare Informatics Research},
	author = {Jung, Kyu-Hwan},
	year = {2025},
	note = {Publisher: Korean Society of Medical Informatics},
	pages = {114--124},
}

@inproceedings{wu_epfl-make_2024,
	title = {Epfl-make at “discharge me!”: {An} llm system for automatically generating discharge summaries of clinical electronic health record},
	shorttitle = {Epfl-make at “discharge me!”},
	url = {https://aclanthology.org/2024.bionlp-1.61/},
	urldate = {2025-07-10},
	booktitle = {Proceedings of the 23rd {Workshop} on {Biomedical} {Natural} {Language} {Processing}},
	author = {Wu, Haotian and Boulenger, Paul and Faure, Antonin and Céspedes, Berta and Boukil, Farouk and Morel, Nastasia and Chen, Zeming and Bosselut, Antoine},
	year = {2024},
	pages = {696--711},
}

@article{kweon_asclepius-r_nodate,
	title = {Asclepius-{R}: {Clinical} {Large} {Language} {Model} {Built} {On} {MIMIC}-{III} {Discharge} {Summaries}},
	shorttitle = {Asclepius-{R}},
	url = {https://physionet.org/content/asclepius-r/},
	urldate = {2025-07-10},
	author = {Kweon, Sunjun and Kim, Junu and Kim, Jiyoun and Im, Sujeong and Cho, Eunbyeol and Bae, Seongsu and Oh, Jungwoo and Lee, Gyubok and Moon, Jong Hak and You, Seng Chan},
}

@article{goldberger_physiobank_2000,
	title = {{PhysioBank}, {PhysioToolkit}, and {PhysioNet}: {Components} of a {New} {Research} {Resource} for {Complex} {Physiologic} {Signals}},
	volume = {101},
	issn = {0009-7322, 1524-4539},
	shorttitle = {{PhysioBank}, {PhysioToolkit}, and {PhysioNet}},
	url = {https://www.ahajournals.org/doi/10.1161/01.CIR.101.23.e215},
	doi = {10.1161/01.cir.101.23.e215},
	abstract = {Abstract—The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet.org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
	language = {en},
	number = {23},
	urldate = {2025-07-08},
	journal = {Circulation},
	author = {Goldberger, Ary L. and Amaral, Luis A. N. and Glass, Leon and Hausdorff, Jeffrey M. and Ivanov, Plamen Ch. and Mark, Roger G. and Mietus, Joseph E. and Moody, George B. and Peng, Chung-Kang and Stanley, H. Eugene},
	month = jun,
	year = {2000},
	note = {Publisher: Ovid Technologies (Wolters Kluwer Health)},
}

@misc{johnson_mimic-iv-note_2023,
	title = {{MIMIC}-{IV}-{Note}: {Deidentified} free-text clinical notes (version 2.2). {PhysioNet}},
	shorttitle = {{MIMIC}-{IV}-{Note}},
	author = {Johnson, Alistair and Pollard, Tom and Horng, Steven and Celi, Leo Anthony and Mark, Roger},
	year = {2023},
}

@misc{christophe_med42_2024,
	title = {Med42 -- {Evaluating} {Fine}-{Tuning} {Strategies} for {Medical} {LLMs}: {Full}-{Parameter} vs. {Parameter}-{Efficient} {Approaches}},
	shorttitle = {Med42 -- {Evaluating} {Fine}-{Tuning} {Strategies} for {Medical} {LLMs}},
	url = {http://arxiv.org/abs/2404.14779},
	doi = {10.48550/arXiv.2404.14779},
	abstract = {This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72\% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Christophe, Clément and Kanithi, Praveen K. and Munjal, Prateek and Raha, Tathagata and Hayat, Nasir and Rajan, Ronnie and Al-Mahrooqi, Ahmed and Gupta, Avani and Salman, Muhammad Umar and Gosal, Gurpreet and Kanakiya, Bhargav and Chen, Charles and Vassilieva, Natalia and Amor, Boulbaba Ben and Pimentel, Marco AF and Khan, Shadab},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14779 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wang_survey_2024,
	title = {A {Survey} for {Large} {Language} {Models} in {Biomedicine}},
	url = {http://arxiv.org/abs/2409.00133},
	doi = {10.48550/arXiv.2409.00133},
	abstract = {Recent breakthroughs in large language models (LLMs) offer unprecedented natural language understanding and generation capabilities. However, existing surveys on LLMs in biomedicine often focus on specific applications or model architectures, lacking a comprehensive analysis that integrates the latest advancements across various biomedical domains. This review, based on an analysis of 484 publications sourced from databases including PubMed, Web of Science, and arXiv, provides an in-depth examination of the current landscape, applications, challenges, and prospects of LLMs in biomedicine, distinguishing itself by focusing on the practical implications of these models in real-world biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning across a broad spectrum of biomedical tasks, including diagnostic assistance, drug discovery, and personalized medicine, among others, with insights drawn from 137 key studies. Then, we discuss adaptation strategies of LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their performance in specialized biomedical contexts where zero-shot fails to achieve, such as medical question answering and efficient processing of biomedical literature. Finally, we discuss the challenges that LLMs face in the biomedicine domain including data privacy concerns, limited model interpretability, issues with dataset quality, and ethics due to the sensitive nature of biomedical data, the need for highly reliable model outputs, and the ethical implications of deploying AI in healthcare. To address these challenges, we also identify future research directions of LLM in biomedicine including federated learning methods to preserve data privacy and integrating explainable AI methodologies to enhance the transparency of LLMs.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Wang, Chong and Li, Mengyao and He, Junjun and Wang, Zhongruo and Darzi, Erfan and Chen, Zan and Ye, Jin and Li, Tianbin and Su, Yanzhou and Ke, Jing and Qu, Kaili and Li, Shuxin and Yu, Yi and Liò, Pietro and Wang, Tianyun and Wang, Yu Guang and Shen, Yiqing},
	month = aug,
	year = {2024},
	note = {arXiv:2409.00133 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{abou_baker_parameter-efficient_2024,
	title = {Parameter-{Efficient} {Fine}-{Tuning} of {Large} {Pretrained} {Models} for {Instance} {Segmentation} {Tasks}},
	volume = {6},
	url = {https://www.mdpi.com/2504-4990/6/4/133},
	number = {4},
	urldate = {2025-07-07},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Abou Baker, Nermeen and Rohrschneider, David and Handmann, Uwe},
	year = {2024},
	note = {Publisher: MDPI},
	pages = {2783--2807},
}

@misc{lester_power_2021,
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	url = {http://arxiv.org/abs/2104.08691},
	doi = {10.48550/arXiv.2104.08691},
	abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	month = sep,
	year = {2021},
	note = {arXiv:2104.08691 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{li_prefix-tuning_2021,
	title = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
	shorttitle = {Prefix-{Tuning}},
	url = {http://arxiv.org/abs/2101.00190},
	doi = {10.48550/arXiv.2101.00190},
	abstract = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1{\textbackslash}\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Li, Xiang Lisa and Liang, Percy},
	month = jan,
	year = {2021},
	note = {arXiv:2101.00190 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{houlsby_parameter-efficient_2019,
	title = {Parameter-efficient transfer learning for {NLP}},
	url = {http://proceedings.mlr.press/v97/houlsby19a.html},
	urldate = {2025-07-07},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	year = {2019},
	pages = {2790--2799},
}

@misc{liao_ming-moe_2024,
	title = {{MING}-{MOE}: {Enhancing} {Medical} {Multi}-{Task} {Learning} in {Large} {Language} {Models} with {Sparse} {Mixture} of {Low}-{Rank} {Adapter} {Experts}},
	shorttitle = {{MING}-{MOE}},
	url = {http://arxiv.org/abs/2404.09027},
	doi = {10.48550/arXiv.2404.09027},
	abstract = {Large language models like ChatGPT have shown substantial progress in natural language understanding and generation, proving valuable across various disciplines, including the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks which often require multi-task learning capabilities. Previous approaches, although beneficial, fall short in real-world applications because they necessitate task-specific annotations at inference time, limiting broader generalization. This paper introduces MING-MOE, a novel Mixture-of-Expert{\textasciitilde}(MOE)-based medical large language model designed to manage diverse and complex medical tasks without requiring task-specific annotations, thus enhancing its usability across extensive datasets. MING-MOE employs a Mixture of Low-Rank Adaptation (MoLoRA) technique, allowing for efficient parameter usage by maintaining base model parameters static while adapting through a minimal set of trainable parameters. We demonstrate that MING-MOE achieves state-of-the-art (SOTA) performance on over 20 medical tasks, illustrating a significant improvement over existing models. This approach not only extends the capabilities of medical language models but also improves inference efficiency.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Liao, Yusheng and Jiang, Shuyang and Wang, Yu and Wang, Yanfeng},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09027 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{hu_lora_2022,
	title = {Lora: {Low}-rank adaptation of large language models.},
	volume = {1},
	shorttitle = {Lora},
	url = {https://arxiv.org/pdf/2106.09685v1/1000},
	number = {2},
	urldate = {2025-07-07},
	journal = {ICLR},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	year = {2022},
	pages = {3},
}

@misc{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	doi = {10.48550/arXiv.2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv:2004.05150 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{yuan_biobart_2022,
	title = {{BioBART}: {Pretraining} and {Evaluation} of {A} {Biomedical} {Generative} {Language} {Model}},
	shorttitle = {{BioBART}},
	url = {http://arxiv.org/abs/2204.03905},
	doi = {10.48550/arXiv.2204.03905},
	abstract = {Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Yuan, Hongyi and Yuan, Zheng and Gan, Ruyi and Zhang, Jiaxing and Xie, Yutao and Yu, Sheng},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03905 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yuan_biobart_2022-1,
	title = {{BioBART}: {Pretraining} and {Evaluation} of {A} {Biomedical} {Generative} {Language} {Model}},
	shorttitle = {{BioBART}},
	url = {https://arxiv.org/abs/2204.03905v2},
	abstract = {Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks.},
	language = {en},
	urldate = {2025-07-07},
	journal = {arXiv.org},
	author = {Yuan, Hongyi and Yuan, Zheng and Gan, Ruyi and Zhang, Jiaxing and Xie, Yutao and Yu, Sheng},
	month = apr,
	year = {2022},
}

@misc{wei_finetuned_2022,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2109.01652},
	doi = {10.48550/arXiv.2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.01652 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{sun_generative_2025,
	title = {Generative {Large} {Language} {Models} {Trained} for {Detecting} {Errors} in {Radiology} {Reports}},
	url = {http://arxiv.org/abs/2504.04336},
	doi = {10.48550/arXiv.2504.04336},
	abstract = {In this retrospective study, a dataset was constructed with two parts. The first part included 1,656 synthetic chest radiology reports generated by GPT-4 using specified prompts, with 828 being error-free synthetic reports and 828 containing errors. The second part included 614 reports: 307 error-free reports between 2011 and 2016 from the MIMIC-CXR database and 307 corresponding synthetic reports with errors generated by GPT-4 on the basis of these MIMIC-CXR reports and specified prompts. All errors were categorized into four types: negation, left/right, interval change, and transcription errors. Then, several models, including Llama-3, GPT-4, and BiomedBERT, were refined using zero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally, the performance of these models was evaluated using the F1 score, 95{\textbackslash}\% confidence interval (CI) and paired-sample t-tests on our constructed dataset, with the prediction results further assessed by radiologists. Using zero-shot prompting, the fine-tuned Llama-3-70B-Instruct model achieved the best performance with the following F1 scores: 0.769 for negation errors, 0.772 for left/right errors, 0.750 for interval change errors, 0.828 for transcription errors, and 0.780 overall. In the real-world evaluation phase, two radiologists reviewed 200 randomly selected reports output by the model. Of these, 99 were confirmed to contain errors detected by the models by both radiologists, and 163 were confirmed to contain model-detected errors by at least one radiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology reports, greatly enhanced error detection in radiology reports.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Sun, Cong and Teichman, Kurt and Zhou, Yiliang and Critelli, Brian and Nauheim, David and Keir, Graham and Wang, Xindi and Zhong, Judy and Flanders, Adam E. and Shih, George and Peng, Yifan},
	month = apr,
	year = {2025},
	note = {arXiv:2504.04336 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{zimokha_exploring_nodate,
	title = {Exploring the role of transformer-based language models in medical transcript summarization ⋆},
	abstract = {The increasing complexity of medical documentation necessitates effective summarization tools that enable healthcare professionals to swiftly access critical patient information. This study investigates the efficacy of four transformer-based models—T5, Flan-T5, GPT-4 Mini, and BART—in summarizing medical transcripts. We conducted a comprehensive comparative analysis utilizing various learning paradigms, including zero-shot learning, one-shot learning, and pretraining methods, to assess the models' capabilities. For evaluation, we employed a diverse set of metrics: ROUGE, METEOR, and BERTScore, providing a holistic view of each model's performance. Our findings indicate that the BART model consistently outperformed the others in summarizing medical texts, demonstrating superior fluency and coherence. In contrast, the GPT-4 Mini model exhibited notable flexibility in domain-specific fine-tuning through zero-shot learning. These results highlight the potential of leveraging transformer-based models to enhance the efficiency of medical documentation processes, ultimately contributing to improved patient care and clinical outcomes. This study underscores the importance of integrating advanced natural language processing techniques into healthcare practices to address the challenges posed by complex medical information.},
	language = {en},
	author = {Zimokha, Mariia and Yemets, Kyrylo},
}

@article{zimokha_exploring_nodate-1,
	title = {Exploring the role of transformer-based language models in medical transcript summarization ⋆},
	abstract = {The increasing complexity of medical documentation necessitates effective summarization tools that enable healthcare professionals to swiftly access critical patient information. This study investigates the efficacy of four transformer-based models—T5, Flan-T5, GPT-4 Mini, and BART—in summarizing medical transcripts. We conducted a comprehensive comparative analysis utilizing various learning paradigms, including zero-shot learning, one-shot learning, and pretraining methods, to assess the models' capabilities. For evaluation, we employed a diverse set of metrics: ROUGE, METEOR, and BERTScore, providing a holistic view of each model's performance. Our findings indicate that the BART model consistently outperformed the others in summarizing medical texts, demonstrating superior fluency and coherence. In contrast, the GPT-4 Mini model exhibited notable flexibility in domain-specific fine-tuning through zero-shot learning. These results highlight the potential of leveraging transformer-based models to enhance the efficiency of medical documentation processes, ultimately contributing to improved patient care and clinical outcomes. This study underscores the importance of integrating advanced natural language processing techniques into healthcare practices to address the challenges posed by complex medical information.},
	language = {en},
	author = {Zimokha, Mariia and Yemets, Kyrylo},
}

@article{hu_zero-shot_2024,
	title = {Zero-shot information extraction from radiological reports using {ChatGPT}},
	volume = {183},
	issn = {1386-5056},
	url = {http://arxiv.org/abs/2309.01398},
	doi = {10.1016/j.ijmedinf.2023.105321},
	abstract = {Electronic health records contain an enormous amount of valuable information, but many are recorded in free text. Information extraction is the strategy to transform the sequence of characters into structured data, which can be employed for secondary analysis. However, the traditional information extraction components, such as named entity recognition and relation extraction, require annotated data to optimize the model parameters, which has become one of the major bottlenecks in building information extraction systems. With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction. In this study, we aim to explore whether the most popular large language model, ChatGPT, can extract useful information from the radiological reports. We first design the prompt template for the interested information in the CT reports. Then, we generate the prompts by combining the prompt template with the CT reports as the inputs of ChatGPT to obtain the responses. A post-processing module is developed to transform the responses into structured extraction results. We conducted the experiments with 847 CT reports collected from Peking University Cancer Hospital. The experimental results indicate that ChatGPT can achieve competitive performances for some extraction tasks compared with the baseline information extraction system, but some limitations need to be further improved.},
	urldate = {2025-07-07},
	journal = {International Journal of Medical Informatics},
	author = {Hu, Danqing and Liu, Bing and Zhu, Xiaofeng and Lu, Xudong and Wu, Nan},
	month = mar,
	year = {2024},
	note = {arXiv:2309.01398 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {105321},
}

@article{ganzinger_automated_2025,
	title = {Automated generation of discharge summaries: leveraging large language models with clinical data},
	volume = {15},
	copyright = {2025 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Automated generation of discharge summaries},
	url = {https://www.nature.com/articles/s41598-025-01618-7},
	doi = {10.1038/s41598-025-01618-7},
	abstract = {This study explores the use of open-source large language models (LLMs) to automate generation of German discharge summaries from structured clinical data. The structured data used to produce AI-generated summaries were manually extracted from electronic health records (EHRs) by a trained medical professional. By leveraging structured documentation collected for research and quality management, the goal is to assist physicians with editable draft summaries. After de-identifying 25 patient datasets, we optimized the output of the LLaMA3 model through prompt engineering and evaluated it using error analysis, as well as quantitative and qualitative metrics. The LLM-generated summaries were rated by physicians on comprehensiveness, conciseness, correctness, and fluency. Key results include an error rate of 2.84 mistakes per summary, and low-to-moderate alignment between generated and physician-written summaries (ROUGE-1: 0.25, BERTScore: 0.64). Medical professionals rated the summaries 3.72 ± 0.89 for comprehensiveness and 3.88 ± 0.97 for factual correctness on a 5-point Likert-scale; however, only 60\% rated the comprehensiveness as good (4 or 5 out of 5). Despite overall informativeness, essential details—such as patient history, lifestyle factors, and intraoperative findings—were frequently omitted, reflecting gaps in summary completeness. While the LLaMA3 model captured much of the clinical information, complex cases and temporal reasoning presented challenges, leading to factual inaccuracies, such as incorrect age calculations. Limitations include a small dataset size, missing structured data elements, and the model’s limited proficiency with German medical terminology, highlighting the need for large, more complete datasets and potential model fine-tuning. In conclusion, this work provides a set of real-world methods, findings, experiences, insights, and descriptive results for a focused use case that may be useful to guide future work in the LLM generation of discharge summaries, perhaps especially for those working with German and possibly other non-English content.},
	language = {en},
	number = {1},
	urldate = {2025-07-07},
	journal = {Scientific Reports},
	author = {Ganzinger, Matthias and Kunz, Nicola and Fuchs, Pascal and Lyu, Cornelia K. and Loos, Martin and Dugas, Martin and Pausch, Thomas M.},
	month = may,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Health care, Pancreatic disease},
	pages = {16466},
}

@article{noauthor_pdf_2025,
	title = {({PDF}) {Automated} generation of discharge summaries: leveraging large language models with clinical data},
	shorttitle = {({PDF}) {Automated} generation of discharge summaries},
	url = {https://www.researchgate.net/publication/391675841_Automated_generation_of_discharge_summaries_leveraging_large_language_models_with_clinical_data},
	doi = {10.1038/s41598-025-01618-7},
	abstract = {PDF {\textbar} This study explores the use of open-source large language models (LLMs) to automate generation of German discharge summaries from structured... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-07-07},
	journal = {ResearchGate},
	month = may,
	year = {2025},
}

@inproceedings{zhang_pegasus_2020,
	title = {Pegasus: {Pre}-training with extracted gap-sentences for abstractive summarization},
	shorttitle = {Pegasus},
	url = {http://proceedings.mlr.press/v119/zhang20ae},
	urldate = {2025-07-07},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
	year = {2020},
	pages = {11328--11339},
}

@article{van_veen_clinical_2023,
	title = {Clinical text summarization: adapting large language models can outperform human experts},
	shorttitle = {Clinical text summarization},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC10635391/},
	urldate = {2025-07-07},
	journal = {Research square},
	author = {Van Veen, Dave and Van Uden, Cara and Blankemeier, Louis and Delbrouck, Jean-Benoit and Aali, Asad and Bluethgen, Christian and Pareek, Anuj and Polacin, Malgorzata and Reis, Eduardo Pontes and Seehofnerova, Anna},
	year = {2023},
	pages = {rs--3},
}

@article{holm_local_2023,
	title = {Local clustering of relic neutrinos with kinetic field theory},
	volume = {844},
	issn = {0370-2693},
	url = {http://arxiv.org/abs/2305.13379},
	doi = {10.1016/j.physletb.2023.138073},
	abstract = {The density of relic neutrinos is expected to be enhanced due to clustering in our local neighbourhood at Earth. We introduce a novel analytical technique to calculate the neutrino overdensity, based on kinetic field theory. Kinetic field theory is a particle-based theory for cosmic structure formation and in this work we apply it for the first time to massive neutrinos. The gravitational interaction is expanded in a perturbation series and we take into account the first-order contribution to the local density of relic neutrinos. For neutrino masses that are consistent with cosmological neutrino mass bounds, our results are in excellent agreement with state-of-the-art calculations.},
	urldate = {2025-07-07},
	journal = {Physics Letters B},
	author = {Holm, Emil Brinch and Oldengott, Isabel M. and Zentarra, Stefan},
	month = sep,
	year = {2023},
	note = {arXiv:2305.13379 [hep-ph]},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, High Energy Physics - Phenomenology},
	pages = {138073},
}

@misc{hu_word_2021,
	title = {Word {Graph} {Guided} {Summarization} for {Radiology} {Findings}},
	url = {http://arxiv.org/abs/2112.09925},
	doi = {10.48550/arXiv.2112.09925},
	abstract = {Radiology reports play a critical role in communicating medical findings to physicians. In each report, the impression section summarizes essential radiology findings. In clinical practice, writing impression is highly demanded yet time-consuming and prone to errors for radiologists. Therefore, automatic impression generation has emerged as an attractive research direction to facilitate such clinical practice. Existing studies mainly focused on introducing salient word information to the general text summarization framework to guide the selection of the key content in radiology findings. However, for this task, a model needs not only capture the important words in findings but also accurately describe their relations so as to generate high-quality impressions. In this paper, we propose a novel method for automatic impression generation, where a word graph is constructed from the findings to record the critical words and their relations, then a Word Graph guided Summarization model (WGSum) is designed to generate impressions with the help of the word graph. Experimental results on two datasets, OpenI and MIMIC-CXR, confirm the validity and effectiveness of our proposed approach, where the state-of-the-art results are achieved on both datasets. Further experiments are also conducted to analyze the impact of different graph designs to the performance of our method.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Hu, Jinpeng and Li, Jianling and Chen, Zhihong and Shen, Yaling and Song, Yan and Wan, Xiang and Chang, Tsung-Hui},
	month = dec,
	year = {2021},
	note = {arXiv:2112.09925 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chen_fast_2018,
	title = {Fast {Abstractive} {Summarization} with {Reinforce}-{Selected} {Sentence} {Rewriting}},
	url = {http://arxiv.org/abs/1805.11080},
	doi = {10.48550/arXiv.1805.11080},
	abstract = {Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Chen, Yen-Chun and Bansal, Mohit},
	month = may,
	year = {2018},
	note = {arXiv:1805.11080 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{lu_clinicalbertsum_2020,
	title = {Clinicalbertsum: {Rct} summarization by using clinical bert embeddings},
	shorttitle = {Clinicalbertsum},
	journal = {Stanford CS224N Custom Project},
	author = {Lu, Mingyi and Jin, Xiaomeng and Wang, Z.},
	year = {2020},
}

@misc{sotudeh_attend_2020,
	title = {Attend to {Medical} {Ontologies}: {Content} {Selection} for {Clinical} {Abstractive} {Summarization}},
	shorttitle = {Attend to {Medical} {Ontologies}},
	url = {http://arxiv.org/abs/2005.00163},
	doi = {10.48550/arXiv.2005.00163},
	abstract = {Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of Rouge metrics (with improvements: 2.9\% RG-1, 2.5\% RG-2, 1.9\% RG-L), in the healthcare domain where any range of improvement impacts patients' welfare.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Sotudeh, Sajad and Goharian, Nazli and Filice, Ross W.},
	month = may,
	year = {2020},
	note = {arXiv:2005.00163 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{alsentzer_publicly_2019,
	title = {Publicly {Available} {Clinical} {BERT} {Embeddings}},
	url = {http://arxiv.org/abs/1904.03323},
	doi = {10.48550/arXiv.1904.03323},
	abstract = {Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Alsentzer, Emily and Murphy, John R. and Boag, Willie and Weng, Wei-Hung and Jin, Di and Naumann, Tristan and McDermott, Matthew B. A.},
	month = jun,
	year = {2019},
	note = {arXiv:1904.03323 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{luhn_automatic_1958,
	title = {The automatic creation of literature abstracts},
	volume = {2},
	url = {https://ieeexplore.ieee.org/abstract/document/5392672/},
	number = {2},
	urldate = {2025-07-06},
	journal = {IBM Journal of research and development},
	author = {Luhn, Hans Peter},
	year = {1958},
	note = {Publisher: Ibm},
	pages = {159--165},
}

@article{jiang_pre-pandemic_2023,
	title = {Pre-pandemic assessment: a decade of progress in electronic health record adoption among {U}.{S}. hospitals},
	volume = {1},
	issn = {2976-5390},
	shorttitle = {Pre-pandemic assessment},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10986221/},
	doi = {10.1093/haschl/qxad056},
	abstract = {As the COVID-19 pandemic loomed, the adoption of electronic health records (EHRs) in US hospitals became a pivotal concern. This study provides a pre-pandemic assessment, highlighting a decade of progress in EHR adoption from 2009 to 2019, with the last available survey conducted from January to June of 2020. It delves into the current EHR adoption rates, variations across different hospital categories, the influence of major vendors, and the challenges in implementing these systems. The study found that basic EHR adoption surged from 6.6\% to 81.2\%, while comprehensive systems increased from 3.6\% to 63.2\%. Despite this growth, the findings point to enduring disparities among hospitals, a concentrated market share by 6 vendors (90\%), and significant concerns regarding maintenance costs. These insights provide an invaluable snapshot of the state of EHR adoption at the brink of the pandemic, serving as a benchmark to assess hospitals’ readiness to utilize digital infrastructure in health care. The conclusions underscore the necessity for strategic policy interventions to encourage a competitive landscape and guarantee equitable access, ultimately strengthening the health care system's responsiveness to global health crises such as COVID-19.},
	number = {5},
	urldate = {2025-07-03},
	journal = {Health Affairs Scholar},
	author = {Jiang, John (Xuefeng) and Qi, Kangkang and Bai, Ge and Schulman, Kevin},
	month = oct,
	year = {2023},
	pmid = {38756982},
	pmcid = {PMC10986221},
	pages = {qxad056},
}

@misc{noauthor_allocation_nodate,
	title = {Allocation of {Physician} {Time} in {Ambulatory} {Practice}: {A} {Time} and {Motion} {Study} in 4 {Specialties} {\textbackslash}textbar {Annals} of {Internal} {Medicine}},
	url = {https://www.acpjournals.org/doi/10.7326/M16-0961},
	urldate = {2025-07-03},
}

@article{de_groot_nursing_2022,
	title = {Nursing documentation and its relationship with perceived nursing workload: a mixed-methods study among community nurses},
	volume = {21},
	issn = {1472-6955},
	shorttitle = {Nursing documentation and its relationship with perceived nursing workload},
	url = {https://doi.org/10.1186/s12912-022-00811-7},
	doi = {10.1186/s12912-022-00811-7},
	abstract = {The time that nurses spent on documentation can be substantial and burdensome. To date it was unknown if documentation activities are related to the workload that nurses perceive. A distinction between clinical documentation and organizational documentation seems relevant. This study aims to gain insight into community nurses’ views on a potential relationship between their clinical and organizational documentation activities and their perceived nursing workload.},
	number = {1},
	urldate = {2025-07-03},
	journal = {BMC Nursing},
	author = {De Groot, Kim and De Veer, Anke J. E. and Munster, Anne M. and Francke, Anneke L. and Paans, Wolter},
	month = jan,
	year = {2022},
	keywords = {Documentation burden, Electronic health record, Home care, Mixed-methods research, Nursing documentation, Nursing process, Nursing workload, User-friendliness},
	pages = {34},
}

@article{supriyono_survey_2024,
	title = {A survey of text summarization: {Techniques}, evaluation and challenges},
	volume = {7},
	issn = {2949-7191},
	shorttitle = {A survey of text summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719124000189},
	doi = {10.1016/j.nlp.2024.100070},
	abstract = {This paper explores the complex field of text summarization in Natural Language Processing (NLP), with particular attention to the development and importance of semantic understanding. Text summarization is a crucial component of natural language processing (NLP), which helps to translate large amounts of textual data into clear and understandable representations. As the story progresses, it demonstrates the dynamic transition from simple syntactic structures to sophisticated models with semantic comprehension. In order to effectively summarize, syntactic, semantic, and pragmatic concerns become crucial, highlighting the necessity of capturing not only grammar but also the context and underlying meaning. It examines the wide range of summarization models, from conventional extractive techniques to state-of-the-art tools like pre-trained models. Applications are found in many different fields, demonstrating how versatile summarizing techniques are. Semantic drift and domain-specific knowledge remain obstacles, despite progress. In the future, the study predicts developments like artificial intelligence integration and transfer learning, which motivates academics to investigate these prospects for advancement. The approach, which is based on the PRISMA framework, emphasizes a methodical and open literature review. The work attempts to further natural language processing (NLP) and text summarization by combining various research findings and suggesting future research directions in this dynamic subject.},
	urldate = {2025-07-03},
	journal = {Natural Language Processing Journal},
	author = {{Supriyono} and Wibawa, Aji Prasetya and {Suyono} and Kurniawan, Fachrul},
	month = jun,
	year = {2024},
	keywords = {Natural language processing, PRISMA, Semantic, Syntactic, Text summarization, Transfer learning},
	pages = {100070},
}

@article{maleki_varnosfaderani_role_2024,
	title = {The {Role} of {AI} in {Hospitals} and {Clinics}: {Transforming} {Healthcare} in the 21st {Century}},
	volume = {11},
	issn = {2306-5354},
	shorttitle = {The {Role} of {AI} in {Hospitals} and {Clinics}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11047988/},
	doi = {10.3390/bioengineering11040337},
	abstract = {As healthcare systems around the world face challenges such as escalating costs, limited access, and growing demand for personalized care, artificial intelligence (AI) is emerging as a key force for transformation. This review is motivated by the urgent need to harness AI’s potential to mitigate these issues and aims to critically assess AI’s integration in different healthcare domains. We explore how AI empowers clinical decision-making, optimizes hospital operation and management, refines medical image analysis, and revolutionizes patient care and monitoring through AI-powered wearables. Through several case studies, we review how AI has transformed specific healthcare domains and discuss the remaining challenges and possible solutions. Additionally, we will discuss methodologies for assessing AI healthcare solutions, ethical challenges of AI deployment, and the importance of data privacy and bias mitigation for responsible technology use. By presenting a critical assessment of AI’s transformative potential, this review equips researchers with a deeper understanding of AI’s current and future impact on healthcare. It encourages an interdisciplinary dialogue between researchers, clinicians, and technologists to navigate the complexities of AI implementation, fostering the development of AI-driven solutions that prioritize ethical standards, equity, and a patient-centered approach.},
	number = {4},
	urldate = {2025-07-03},
	journal = {Bioengineering},
	author = {Maleki Varnosfaderani, Shiva and Forouzanfar, Mohamad},
	month = mar,
	year = {2024},
	pmid = {38671759},
	pmcid = {PMC11047988},
	pages = {337},
}

@misc{noauthor_extractive_nodate,
	title = {Extractive {Summarization} - an overview {\textbackslash}textbar {ScienceDirect} {Topics}},
	url = {https://www.sciencedirect.com/topics/computer-science/extractive-summarization},
	urldate = {2025-07-04},
}

@article{shakil_abstractive_2024,
	title = {Abstractive {Text} {Summarization}: {State} of the {Art}, {Challenges}, and {Improvements}},
	volume = {603},
	issn = {09252312},
	shorttitle = {Abstractive {Text} {Summarization}},
	url = {http://arxiv.org/abs/2409.02413},
	doi = {10.1016/j.neucom.2024.128255},
	abstract = {Specifically focusing on the landscape of abstractive text summarization, as opposed to extractive techniques, this survey presents a comprehensive overview, delving into state-of-the-art techniques, prevailing challenges, and prospective research directions. We categorize the techniques into traditional sequence-to-sequence models, pre-trained large language models, reinforcement learning, hierarchical methods, and multi-modal summarization. Unlike prior works that did not examine complexities, scalability and comparisons of techniques in detail, this review takes a comprehensive approach encompassing state-of-the-art methods, challenges, solutions, comparisons, limitations and charts out future improvements - providing researchers an extensive overview to advance abstractive summarization research. We provide vital comparison tables across techniques categorized - offering insights into model complexity, scalability and appropriate applications. The paper highlights challenges such as inadequate meaning representation, factual consistency, controllable text summarization, cross-lingual summarization, and evaluation metrics, among others. Solutions leveraging knowledge incorporation and other innovative strategies are proposed to address these challenges. The paper concludes by highlighting emerging research areas like factual inconsistency, domain-specific, cross-lingual, multilingual, and long-document summarization, as well as handling noisy data. Our objective is to provide researchers and practitioners with a structured overview of the domain, enabling them to better understand the current landscape and identify potential areas for further research and improvement.},
	urldate = {2025-07-04},
	journal = {Neurocomputing},
	author = {Shakil, Hassan and Farooq, Ahmad and Kalita, Jugal},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {128255},
}

@article{almohaimeed_abstractive_2025,
	title = {Abstractive text summarization: {A} comprehensive survey of techniques, systems, and challenges},
	volume = {57},
	issn = {1574-0137},
	shorttitle = {Abstractive text summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013725000383},
	doi = {10.1016/j.cosrev.2025.100762},
	abstract = {ive text summarization addresses information overload by generating paraphrased content that mimics human expression, yet it faces significant computational and linguistic challenges. This paper presents a detailed functional taxonomy of abstractive summarization, structured along four dimensions: techniques (including structure-based, semantic, and deep learning approaches, including large language models), system architectures (ranging from single-model to multi-agent and human-in-the-loop interactive systems), evaluation methods (covering lexical, semantic, and human-centered assessments), and datasets. Our taxonomy explicitly distinguishes techniques from architectures to clarify how methodological strategies are operationalized in practice. We examine pressing multilingual challenges such as linguistic complexity, data scarcity, and performance disparities in cross-lingual transfer, particularly for low-resource languages. Additionally, we address persistent issues such as factual inaccuracies, content hallucinations, and biases in widely used evaluation metrics. The paper highlights emerging trends—including cross-lingual summarization, interactive summarization systems, and ethically grounded frameworks—as key directions for future research. This synthesis not only maps the current landscape but also outlines pathways to enhance the accuracy, reliability, and applicability of abstractive summarization in real-world settings.},
	urldate = {2025-07-04},
	journal = {Computer Science Review},
	author = {Almohaimeed, Norah and Azmi, Aqil M.},
	month = aug,
	year = {2025},
	keywords = {Abstractive summarization, Datasets, Evaluation methods, Summarization techniques, System architectures, Taxonomy},
	pages = {100762},
}

@incollection{balogh_diagnostic_2015,
	title = {The {Diagnostic} {Process}},
	url = {https://www.ncbi.nlm.nih.gov/books/NBK338593/},
	abstract = {This chapter provides an overview of diagnosis in health care, including the committee's conceptual model of the diagnostic process and a review of clinical reasoning. Diagnosis has important implications for patient care, research, and policy. Diagnosis has been described as both a process and a classification scheme, or a “pre-existing set of categories agreed upon by the medical profession to designate a specific condition” (Jutel, 2009).1 When a diagnosis is accurate and made in a timely manner, a patient has the best opportunity for a positive health outcome because clinical decision making will be tailored to a correct understanding of the patient's health problem (Holmboe and Durning, 2014). In addition, public policy decisions are often influenced by diagnostic information, such as setting payment policies, resource allocation decisions, and research priorities (Jutel, 2009; Rosenberg, 2002; WHO, 2012).},
	language = {en},
	urldate = {2025-07-04},
	booktitle = {Improving {Diagnosis} in {Health} {Care}},
	publisher = {National Academies Press (US)},
	author = {Balogh, Erin P. and Miller, Bryan T. and Ball, John R. and Care, Committee on Diagnostic Error in Health and Services, Board on Health Care and Medicine, Institute of and The National Academies of Sciences, Engineering},
	month = dec,
	year = {2015},
}

@article{ling_clinical_2015,
	title = {Clinical {Documents} {Clustering} {Based} on {Medication}/{Symptom} {Names} {Using} {Multi}-{View} {Nonnegative} {Matrix} {Factorization}},
	volume = {14},
	issn = {1558-2639},
	url = {https://ieeexplore.ieee.org/document/7111340},
	doi = {10.1109/TNB.2015.2422612},
	abstract = {Clinical documents are rich free-text data sources containing valuable medication and symptom information, which have a great potential to improve health care. In this paper, we build an integrating system for extracting medication names and symptom names from clinical notes. Then we apply nonnegative matrix factorization (NMF) and multi-view NMF to cluster clinical notes into meaningful clusters based on sample-feature matrices. Our experimental results show that multi-view NMF is a preferable method for clinical document clustering. Moreover, we find that using extracted medication/symptom names to cluster clinical documents outperforms just using words.},
	number = {5},
	urldate = {2025-07-04},
	journal = {IEEE Transactions on NanoBioscience},
	author = {Ling, Yuan and Pan, Xuelian and Li*, Guangrong and Hu, Xiaohua},
	month = jul,
	year = {2015},
	keywords = {Accuracy, Clinical document, Data mining, Design automation, Diseases, Hospitals, Informatics, Medical diagnostic imaging, clinical notes, document clustering, multi-view, nonnegative matrix factorization},
	pages = {500--504},
}

@article{ando_exploring_2022,
	title = {Exploring optimal granularity for extractive summarization of unstructured health records: {Analysis} of the largest multi-institutional archive of health records in {Japan}},
	volume = {1},
	issn = {2767-3170},
	shorttitle = {Exploring optimal granularity for extractive summarization of unstructured health records},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931252/},
	doi = {10.1371/journal.pdig.0000099},
	abstract = {Automated summarization of clinical texts can reduce the burden of medical professionals. “Discharge summaries” are one promising application of the summarization, because they can be generated from daily inpatient records. Our preliminary experiment suggests that 20–31\% of the descriptions in discharge summaries overlap with the content of the inpatient records. However, it remains unclear how the summaries should be generated from the unstructured source. To decompose the physician’s summarization process, this study aimed to identify the optimal granularity in summarization. We first defined three types of summarization units with different granularities to compare the performance of the discharge summary generation: whole sentences, clinical segments, and clauses. We defined clinical segments in this study, aiming to express the smallest medically meaningful concepts. To obtain the clinical segments, it was necessary to automatically split the texts in the first stage of the pipeline. Accordingly, we compared rule-based methods and a machine learning method, and the latter outperformed the formers with an F1 score of 0.846 in the splitting task. Next, we experimentally measured the accuracy of extractive summarization using the three types of units, based on the ROUGE-1 metric, on a multi-institutional national archive of health records in Japan. The measured accuracies of extractive summarization using whole sentences, clinical segments, and clauses were 31.91, 36.15, and 25.18, respectively. We found that the clinical segments yielded higher accuracy than sentences and clauses. This result indicates that summarization of inpatient records demands finer granularity than sentence-oriented processing. Although we used only Japanese health records, it can be interpreted as follows: physicians extract “concepts of medical significance” from patient records and recombine them in new contexts when summarizing chronological clinical records, rather than simply copying and pasting topic sentences. This observation suggests that a discharge summary is created by higher-order information processing over concepts on sub-sentence level, which may guide future research in this field., Medical practice includes significant paperwork, and therefore, automated processing of clinical texts can reduce medical professionals’ burden. Accordingly, we focused on hospitals’ discharge summaries from daily inpatient records stored in Electric Health Records. By applying summarization technologies, which are well-studied in Natural Language Processing, discharge summaries could be generated automatically from the source texts. However, automated summarization of daily inpatient records involves various technical topics and challenges, and the generation of discharge summaries is a complex process of mixing extractive and abstractive summarization. Thus, in this study, we explored optimal granularity for extractive summarization, attempting to decompose actual physicians’ processing. In the experiments, we used three types of summarization units with different granularities to compare performances of discharge summary generation: whole sentences, clinical segments, and clauses. We originally defined clinical segments, aiming to express the smallest medically meaningful concepts. The result indicated that sub-sentence processing, larger than clauses, improves the quality of the summaries. This finding can guide future development of medical documents’ automated summarization.},
	number = {9},
	urldate = {2025-07-04},
	journal = {PLOS Digital Health},
	author = {Ando, Kenichiro and Okumura, Takashi and Komachi, Mamoru and Horiguchi, Hiromasa and Matsumoto, Yuji},
	month = sep,
	year = {2022},
	pmid = {36812582},
	pmcid = {PMC9931252},
	pages = {e0000099},
}

@article{luo_pre-trained_2024,
	title = {Pre-trained language models in medicine: {A} survey},
	volume = {154},
	issn = {0933-3657},
	shorttitle = {Pre-trained language models in medicine},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365724001465},
	doi = {10.1016/j.artmed.2024.102904},
	abstract = {With the rapid progress in Natural Language Processing (NLP), Pre-trained Language Models (PLM) such as BERT, BioBERT, and ChatGPT have shown great potential in various medical NLP tasks. This paper surveys the cutting-edge achievements in applying PLMs to various medical NLP tasks. Specifically, we first brief PLMS and outline the research of PLMs in medicine. Next, we categorise and discuss the types of tasks in medical NLP, covering text summarisation, question-answering, machine translation, sentiment analysis, named entity recognition, information extraction, medical education, relation extraction, and text mining. For each type of task, we first provide an overview of the basic concepts, the main methodologies, the advantages of applying PLMs, the basic steps of applying PLMs application, the datasets for training and testing, and the metrics for task evaluation. Subsequently, a summary of recent important research findings is presented, analysing their motivations, strengths vs weaknesses, similarities vs differences, and discussing potential limitations. Also, we assess the quality and influence of the research reviewed in this paper by comparing the citation count of the papers reviewed and the reputation and impact of the conferences and journals where they are published. Through these indicators, we further identify the most concerned research topics currently. Finally, we look forward to future research directions, including enhancing models’ reliability, explainability, and fairness, to promote the application of PLMs in clinical practice. In addition, this survey also collect some download links of some model codes and the relevant datasets, which are valuable references for researchers applying NLP techniques in medicine and medical professionals seeking to enhance their expertise and healthcare service through AI technology.},
	urldate = {2025-07-04},
	journal = {Artificial Intelligence in Medicine},
	author = {Luo, Xudong and Deng, Zhiqi and Yang, Binxia and Luo, Michael Y.},
	month = aug,
	year = {2024},
	keywords = {BERT, GPT, Healthcare, Medical science, Natural language processing, Pre-trained language model},
	pages = {102904},
}

@article{van_veen_clinical_2023-1,
	title = {Clinical {Text} {Summarization}: {Adapting} {Large} {Language} {Models} {Can} {Outperform} {Human} {Experts}},
	issn = {2693-5015},
	shorttitle = {Clinical {Text} {Summarization}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10635391/},
	doi = {10.21203/rs.3.rs-3483777/v1},
	abstract = {Sifting through vast textual data and summarizing key information from electronic health records (EHR) imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy on a diverse range of clinical summarization tasks has not yet been rigorously demonstrated. In this work, we apply domain adaptation methods to eight LLMs, spanning six datasets and four distinct clinical summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not improve results. Further, in a clinical reader study with ten physicians, we show that summaries from our best-adapted LLMs are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis highlights challenges faced by both LLMs and human experts. Lastly, we correlate traditional quantitative NLP metrics with reader study scores to enhance our understanding of how these metrics align with physician preferences. Our research marks the first evidence of LLMs outperforming human experts in clinical text summarization across multiple tasks. This implies that integrating LLMs into clinical workflows could alleviate documentation burden, empowering clinicians to focus more on personalized patient care and the inherently human aspects of medicine.},
	urldate = {2025-07-04},
	journal = {Research Square},
	author = {Van Veen, Dave and Van Uden, Cara and Blankemeier, Louis and Delbrouck, Jean-Benoit and Aali, Asad and Bluethgen, Christian and Pareek, Anuj and Polacin, Malgorzata and Reis, Eduardo Pontes and Seehofnerová, Anna and Rohatgi, Nidhi and Hosamani, Poonam and Collins, William and Ahuja, Neera and Langlotz, Curtis P. and Hom, Jason and Gatidis, Sergios and Pauly, John and Chaudhari, Akshay S.},
	month = oct,
	year = {2023},
	pmid = {37961377},
	pmcid = {PMC10635391},
	pages = {rs.3.rs--3483777},
}

@incollection{rodziewicz_medical_2025,
	address = {Treasure Island (FL)},
	title = {Medical {Error} {Reduction} and {Prevention}},
	copyright = {Copyright © 2025, StatPearls Publishing LLC.},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK499956/},
	abstract = {Medical errors have more recently been recognized as a serious public health problem, reported as the third leading cause of death in the US. However, because medical errors are comprised of different types of failures (eg, diagnostic or medication errors) that can result in various outcomes (eg, near-miss, injury, or no harm), estimates of the incidence of medical errors vary widely in studies. One study reported that approximately 400,000 hospitalized patients experience some preventable harm each year, while another estimated that {\textbackslash}textgreater200,000 patient deaths annually were due to preventable medical errors. Moreover, the reported cost of medical errors is wide-ranging, with some experts estimating {\textbackslash}20 billion each year and others approximating healthcare costs of {\textbackslash}35.7 to \$45 billion annually for hospital-acquired infections alone. The definition of a medical error varies, making analysis via uniform objectives difficult. Furthermore, a lack of standardized terminology has hindered data assessment, synthesis, and evaluation. The Institute of Medicine (IOM) Committee on Quality of Health Care in the US, which performed the first large study on medical errors, defined a medical error as "the failure of a planned action to be completed as intended or the use of a wrong plan to achieve an aim." Another definition identifies medical errors as a failure in care that may or may not result in patient harm. Regardless of the definition, medical errors are associated with high morbidity, mortality, and economic burden. Moreover, they can negatively impact the patient, their family, involved clinicians and support staff, the healthcare facility, and the community. Healthcare professionals may experience profound psychological effects (eg, anger, guilt, inadequacy, depression, and suicidal ideation) due to actual or perceived errors, which the threat of impending legal action may compound. Clinicians can also equate errors with failure, a breach of public trust, and patient injury despite their mandate to do no harm, which may lead to decreased clinical confidence. Some experts believe the term error is excessively antagonistic and perpetuates a blame culture. Due to the negative connotation, limited use of the term is prudent when documenting patient records; some experts suggest the term not be used at all. However, adverse events secondary to medical errors occur; therefore, simply discontinuing the word's usage will not prevent or reduce these errors. Uncovering the cause of these errors, as well as providing viable solutions to avoid these errors from occurring, is challenging. Healthcare professionals should be familiar with the different types of medical errors to understand better the adverse events that may be caused. Common types of medical errors include surgical errors, diagnostic errors, medication errors, equipment failures, patient falls, hospital-acquired infections, and communication failures. By identifying the deficiencies, failures, and risk factors that lead to an adverse event, corrective measures can be developed to prevent similar errors. Encouraging individuals involved in every aspect of healthcare to report medical errors is essential to this process. Confidential reporting options are necessary to identify deficiencies or failures a system may contain. Changing workplace culture and developing protocols for addressing medical errors can encourage medical error reporting. Institutions that adopt a patient safety culture and implement corrective interventions can make healthcare safer for patients and healthcare workers. Working together, healthcare professionals can improve patient safety by identifying the contributing factors and events that result in medical errors, developing multifaceted prevention protocols, and implementing these strategies at various healthcare levels.},
	language = {eng},
	urldate = {2025-07-04},
	booktitle = {{StatPearls}},
	publisher = {StatPearls Publishing},
	author = {Rodziewicz, Thomas L. and Houseman, Benjamin and Vaqar, Sarosh and Hipskind, John E.},
	year = {2025},
	pmid = {29763131},
}

@article{yadav_data_2023,
	title = {Data {Privacy} in {Healthcare}: {In} the {Era} of {Artificial} {Intelligence}},
	volume = {14},
	issn = {2229-5178},
	shorttitle = {Data {Privacy} in {Healthcare}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10718098/},
	doi = {10.4103/idoj.idoj_543_23},
	abstract = {Data Privacy has increasingly become a matter of concern in the era of large public digital respositories of data. This is particularly true in healthcare where data can be misused if traced back to patients, and brings with itself a myriad of possibilities. Bring custodians of data, as well as being at the helm of disigning studies and products that can potentially benefit products, healthcare professionals often find themselves unsure about ethical and legal constraints that undelie data sharing. In this review we touch upon the concerns, leal frameworks as well as some common practices in these respects.},
	number = {6},
	urldate = {2025-07-04},
	journal = {Indian Dermatology Online Journal},
	author = {Yadav, Neel and Pandey, Saumya and Gupta, Amit and Dudani, Pankhuri and Gupta, Somesh and Rangarajan, Krithika},
	month = oct,
	year = {2023},
	pmid = {38099022},
	pmcid = {PMC10718098},
	pages = {788--792},
}

@article{khalid_privacy-preserving_2023,
	title = {Privacy-preserving artificial intelligence in healthcare: {Techniques} and applications},
	volume = {158},
	issn = {0010-4825},
	shorttitle = {Privacy-preserving artificial intelligence in healthcare},
	url = {https://www.sciencedirect.com/science/article/pii/S001048252300313X},
	doi = {10.1016/j.compbiomed.2023.106848},
	abstract = {There has been an increasing interest in translating artificial intelligence (AI) research into clinically-validated applications to improve the performance, capacity, and efficacy of healthcare services. Despite substantial research worldwide, very few AI-based applications have successfully made it to clinics. Key barriers to the widespread adoption of clinically validated AI applications include non-standardized medical records, limited availability of curated datasets, and stringent legal/ethical requirements to preserve patients’ privacy. Therefore, there is a pressing need to improvise new data-sharing methods in the age of AI that preserve patient privacy while developing AI-based healthcare applications. In the literature, significant attention has been devoted to developing privacy-preserving techniques and overcoming the issues hampering AI adoption in an actual clinical environment. To this end, this study summarizes the state-of-the-art approaches for preserving privacy in AI-based healthcare applications. Prominent privacy-preserving techniques such as Federated Learning and Hybrid Techniques are elaborated along with potential privacy attacks, security challenges, and future directions.},
	urldate = {2025-07-04},
	journal = {Computers in Biology and Medicine},
	author = {Khalid, Nazish and Qayyum, Adnan and Bilal, Muhammad and Al-Fuqaha, Ala and Qadir, Junaid},
	month = may,
	year = {2023},
	keywords = {Artificial intelligence (AI), Electronic health record (EHR), Privacy, Privacy preservation},
	pages = {106848},
}

@article{nerella_transformers_2024,
	title = {Transformers and large language models in healthcare: {A} review},
	volume = {154},
	issn = {0933-3657},
	shorttitle = {Transformers and large language models in healthcare},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365724001428},
	doi = {10.1016/j.artmed.2024.102900},
	abstract = {With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of healthcare data, including clinical NLP, medical imaging, structured Electronic Health Records (EHR), social media, bio-physiological signals, biomolecular sequences. Furthermore, which have also include the articles that used the transformer architecture for generating surgical instructions and predicting adverse outcomes after surgeries under the umbrella of critical care. Under diverse settings, these models have been used for clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. Finally, we also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.},
	urldate = {2025-07-04},
	journal = {Artificial Intelligence in Medicine},
	author = {Nerella, Subhash and Bandyopadhyay, Sabyasachi and Zhang, Jiaqing and Contreras, Miguel and Siegel, Scott and Bumin, Aysegul and Silva, Brandon and Sena, Jessica and Shickel, Benjamin and Bihorac, Azra and Khezeli, Kia and Rashidi, Parisa},
	month = aug,
	year = {2024},
	keywords = {Electronic Health Records, Healthcare, Large Language Models, Medical Imaging, Natural Language Processing, Transformers},
	pages = {102900},
}

@article{luo_pre-trained_2024-1,
	title = {Pre-trained language models in medicine: {A} survey},
	volume = {154},
	issn = {0933-3657},
	shorttitle = {Pre-trained language models in medicine},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365724001465},
	doi = {10.1016/j.artmed.2024.102904},
	abstract = {With the rapid progress in Natural Language Processing (NLP), Pre-trained Language Models (PLM) such as BERT, BioBERT, and ChatGPT have shown great potential in various medical NLP tasks. This paper surveys the cutting-edge achievements in applying PLMs to various medical NLP tasks. Specifically, we first brief PLMS and outline the research of PLMs in medicine. Next, we categorise and discuss the types of tasks in medical NLP, covering text summarisation, question-answering, machine translation, sentiment analysis, named entity recognition, information extraction, medical education, relation extraction, and text mining. For each type of task, we first provide an overview of the basic concepts, the main methodologies, the advantages of applying PLMs, the basic steps of applying PLMs application, the datasets for training and testing, and the metrics for task evaluation. Subsequently, a summary of recent important research findings is presented, analysing their motivations, strengths vs weaknesses, similarities vs differences, and discussing potential limitations. Also, we assess the quality and influence of the research reviewed in this paper by comparing the citation count of the papers reviewed and the reputation and impact of the conferences and journals where they are published. Through these indicators, we further identify the most concerned research topics currently. Finally, we look forward to future research directions, including enhancing models’ reliability, explainability, and fairness, to promote the application of PLMs in clinical practice. In addition, this survey also collect some download links of some model codes and the relevant datasets, which are valuable references for researchers applying NLP techniques in medicine and medical professionals seeking to enhance their expertise and healthcare service through AI technology.},
	urldate = {2025-07-04},
	journal = {Artificial Intelligence in Medicine},
	author = {Luo, Xudong and Deng, Zhiqi and Yang, Binxia and Luo, Michael Y.},
	month = aug,
	year = {2024},
	keywords = {BERT, GPT, Healthcare, Medical science, Natural language processing, Pre-trained language model},
	pages = {102904},
}

@inproceedings{liao_parameter-efficient_2023,
	address = {Toronto, Canada},
	title = {Parameter-{Efficient} {Fine}-{Tuning} without {Introducing} {New} {Latency}},
	url = {https://aclanthology.org/2023.acl-long.233/},
	doi = {10.18653/v1/2023.acl-long.233},
	abstract = {Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full fine-tuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03\% parameters of full fine-tuning.},
	urldate = {2025-07-04},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liao, Baohao and Meng, Yan and Monz, Christof},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {4242--4260},
}

@article{alves_benchmarking_2025,
	title = {A benchmarking framework and dataset for learning to defer in human-{AI} decision-making},
	volume = {12},
	copyright = {2025 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-025-04664-y},
	doi = {10.1038/s41597-025-04664-y},
	abstract = {Learning to Defer (L2D) algorithms improve human-AI collaboration by deferring decisions to human experts when they are likely to be more accurate than the AI model. These can be crucial in high-stakes tasks like fraud detection, where false negatives can cost victims their life savings. The primary challenge in training and evaluating these systems is the high cost of acquiring expert predictions, often leading to the use of simplistic simulated expert behavior in benchmarks. We introduce OpenL2D, a framework generating synthetic experts with adjustable decision-making processes and work capacity constraints for more realistic L2D testing. Applied to a public fraud detection dataset, OpenL2D creates the financial fraud alert review dataset (FiFAR), which contains predictions from 50 fraud analysts for 30 K instances. We show that FiFAR’s synthetic experts are similar to real experts in metrics such as consistency and inter-expert agreement. Our L2D benchmark reveals that performance rankings of L2D algorithms vary significantly based on the available experts, highlighting the need to consider diverse expert behavior in L2D benchmarking.},
	language = {en},
	number = {1},
	urldate = {2025-07-04},
	journal = {Scientific Data},
	author = {Alves, Jean V. and Leitão, Diogo and Jesus, Sérgio and Sampaio, Marco O. P. and Liébana, Javier and Saleiro, Pedro and Figueiredo, Mário A. T. and Bizarro, Pedro},
	month = apr,
	year = {2025},
	keywords = {Computer science, Scientific data},
	pages = {506},
}

@article{pratap_fine_2025,
	title = {The fine art of fine-tuning: {A} structured review of advanced {LLM} fine-tuning techniques},
	volume = {11},
	issn = {2949-7191},
	shorttitle = {The fine art of fine-tuning},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719125000202},
	doi = {10.1016/j.nlp.2025.100144},
	abstract = {Transformer-based models have consistently demonstrated superior accuracy compared to various traditional models across a range of downstream tasks. However, due to their large nature, training or fine-tuning them for specific tasks has heavy computational and memory demands. This causes the creation of specialized transformer-based models to be almost impossible in the generally present constrained scenarios. To tackle this issue and to make these large models more accessible, a plethora of techniques have been developed. In this study, we will be reviewing the types of techniques developed, their impacts and benefits concerning performance and resource usage along with the latest developments in the domain. We have broadly categorized these techniques into six key areas: Changes in Training Method, Changes in Adapter, Quantization, Parameter Selection, Mixture of Experts, and Application based methods. We collated the results of various techniques on common benchmarks and also evaluated their performance on different datasets and base models.},
	urldate = {2025-07-04},
	journal = {Natural Language Processing Journal},
	author = {Pratap, Samar and Aranha, Alston Richard and Kumar, Divyanshu and Malhotra, Gautam and Iyer, Anantharaman Palacode Narayana and S.s., Shylaja},
	month = jun,
	year = {2025},
	keywords = {Adapter, FFT, LLM, LoRA, MoE, PEFT, Quantization},
	pages = {100144},
}

@misc{noauthor_fine-tuning_nodate,
	title = {Fine-{Tuning} and {Deploying} {Large} {Language} {Models} {Over} {Edges}: {Issues} and {Approaches}},
	url = {https://arxiv.org/html/2408.10691v1},
	urldate = {2025-07-04},
}

@article{tu_overview_2024,
	title = {An overview of large {AI} models and their applications},
	volume = {2},
	issn = {2731-9008},
	url = {https://doi.org/10.1007/s44267-024-00065-8},
	doi = {10.1007/s44267-024-00065-8},
	abstract = {In recent years, large-scale artificial intelligence (AI) models have become a focal point in technology, attracting widespread attention and acclaim. Notable examples include Google’s BERT and OpenAI’s GPT, which have scaled their parameter sizes to hundreds of billions or even tens of trillions. This growth has been accompanied by a significant increase in the amount of training data, significantly improving the capabilities and performance of these models. Unlike previous reviews, this paper provides a comprehensive discussion of the algorithmic principles of large-scale AI models and their industrial applications from multiple perspectives. We first outline the evolutionary history of these models, highlighting milestone algorithms while exploring their underlying principles and core technologies. We then evaluate the challenges and limitations of large-scale AI models, including computational resource requirements, model parameter inflation, data privacy concerns, and specific issues related to multi-modal AI models, such as reliance on text-image pairs, inconsistencies in understanding and generation capabilities, and the lack of true “multi-modality”. Various industrial applications of these models are also presented. Finally, we discuss future trends, predicting further expansion of model scale and the development of cross-modal fusion. This study provides valuable insights to inform and inspire future future research and practice.},
	language = {en},
	number = {1},
	urldate = {2025-07-04},
	journal = {Visual Intelligence},
	author = {Tu, Xiaoguang and He, Zhi and Huang, Yi and Zhang, Zhi-Hao and Yang, Ming and Zhao, Jian},
	month = dec,
	year = {2024},
	keywords = {Artificial Intelligence, Artificial intelligence, Big Data, Computational Intelligence, GPT, Large AI models, Large language models, Logic in AI, Machine Learning, Symbolic AI},
	pages = {34},
}

@misc{noauthor_pre-training_nodate,
	title = {Pre-training {Everywhere}: {Parameter}-{Efficient} {Fine}-{Tuning} for {Medical} {Image} {Analysis} via {Target} {Parameter} {Pre}-training},
	url = {https://arxiv.org/html/2408.15011v1},
	urldate = {2025-07-04},
}

@misc{li_revisiting_2024,
	title = {Revisiting {Catastrophic} {Forgetting} in {Large} {Language} {Model} {Tuning}},
	url = {http://arxiv.org/abs/2406.04836},
	abstract = {Catastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning new data. It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated. This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF in the field of LLMs. Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape. Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF. Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF.},
	urldate = {2025-07-04},
	publisher = {arXiv},
	author = {Li, Hongyu and Ding, Liang and Fang, Meng and Tao, Dacheng},
	month = jun,
	year = {2024},
	doi = {10.48550/arXiv.2406.04836},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{abou_baker_parameter-efficient_2024-1,
	title = {Parameter-{Efficient} {Fine}-{Tuning} of {Large} {Pretrained} {Models} for {Instance} {Segmentation} {Tasks}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-4990},
	url = {https://www.mdpi.com/2504-4990/6/4/133},
	doi = {10.3390/make6040133},
	abstract = {Research and applications in artificial intelligence have recently shifted with the rise of large pretrained models, which deliver state-of-the-art results across numerous tasks. However, the substantial increase in parameters introduces a need for parameter-efficient training strategies. Despite significant advancements, limited research has explored parameter-efficient fine-tuning (PEFT) methods in the context of transformer-based models for instance segmentation. Addressing this gap, this study investigates the effectiveness of PEFT methods, specifically adapters and Low-Rank Adaptation (LoRA), applied to two models across four benchmark datasets. Integrating sequentially arranged adapter modules and applying LoRA to deformable attention—explored here for the first time—achieves competitive performance while fine-tuning only about 1–6\% of model parameters, a marked improvement over the 40–55\% required in traditional fine-tuning. Key findings indicate that using 2–3 adapters per transformer block offers an optimal balance of performance and efficiency. Furthermore, LoRA, exhibits strong parameter efficiency when applied to deformable attention, and in certain cases surpasses adapter configurations. These results show that the impact of PEFT techniques varies based on dataset complexity and model architecture, underscoring the importance of context-specific tuning. Overall, this work demonstrates the potential of PEFT to enable scalable, customizable, and computationally efficient transfer learning for instance segmentation tasks.},
	language = {en},
	number = {4},
	urldate = {2025-07-04},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Abou Baker, Nermeen and Rohrschneider, David and Handmann, Uwe},
	month = dec,
	year = {2024},
	keywords = {LoRA, MASK DINO, PEFT, SEEM, adapters, finetuning, instance segmentation},
	pages = {2783--2807},
}

@misc{noauthor_low-rank_nodate,
	title = {Low-{Rank} {Adaptation} for {Foundation} {Models}: {A} {Comprehensive} {Review}},
	url = {https://arxiv.org/html/2501.00365v1},
	urldate = {2025-07-04},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-07-04},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2106.09685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{abdelaziz_data_2024,
	title = {From {Data} to {Insights}: {A} {Survey} on {Biomedical} {Text} {Summarization} {Approaches} and {Challenges}},
	volume = {5},
	shorttitle = {From {Data} to {Insights}},
	url = {https://www.ijci.zu.edu.eg/index.php/ijci/article/view/85},
	urldate = {2025-06-24},
	journal = {International Journal of Computers and Informatics (Zagazig University)},
	author = {AbdelAziz, Nabil M. and Ali, Aliaa A. and Naguib, Soaad M. and Fayed, Lamiaa S.},
	year = {2024},
	pages = {28--43},
}

@misc{zhong_extractive_2020,
	title = {Extractive {Summarization} as {Text} {Matching}},
	url = {http://arxiv.org/abs/2004.08795},
	doi = {10.48550/arXiv.2004.08795},
	abstract = {This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github.com/maszhongming/MatchSum.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Zhong, Ming and Liu, Pengfei and Chen, Yiran and Wang, Danqing and Qiu, Xipeng and Huang, Xuanjing},
	month = apr,
	year = {2020},
	note = {arXiv:2004.08795 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{xiao_extractive_2019,
	title = {Extractive {Summarization} of {Long} {Documents} by {Combining} {Global} and {Local} {Context}},
	url = {http://arxiv.org/abs/1909.08089},
	doi = {10.48550/arXiv.1909.08089},
	abstract = {In this paper, we propose a novel neural single document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Xiao, Wen and Carenini, Giuseppe},
	month = sep,
	year = {2019},
	note = {arXiv:1909.08089 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ruan_histruct_2022,
	title = {{HiStruct}+: {Improving} {Extractive} {Text} {Summarization} with {Hierarchical} {Structure} {Information}},
	shorttitle = {{HiStruct}+},
	url = {http://arxiv.org/abs/2203.09629},
	doi = {10.48550/arXiv.2203.09629},
	abstract = {Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model's SOTA performance.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Ruan, Qian and Ostendorff, Malte and Rehm, Georg},
	month = mar,
	year = {2022},
	note = {arXiv:2203.09629 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{gupta_sumpubmed_2021,
	title = {{SumPubMed}: {Summarization} dataset of {PubMed} scientific articles},
	shorttitle = {{SumPubMed}},
	url = {https://aclanthology.org/2021.acl-srw.30/},
	urldate = {2025-06-24},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}: {Student} {Research} {Workshop}},
	author = {Gupta, Vivek and Bharti, Prerna and Nokhiz, Pegah and Karnick, Harish},
	year = {2021},
	pages = {292--303},
}

@misc{dong_discourse-aware_2021,
	title = {Discourse-{Aware} {Unsupervised} {Summarization} of {Long} {Scientific} {Documents}},
	url = {http://arxiv.org/abs/2005.00513},
	doi = {10.48550/arXiv.2005.00513},
	abstract = {We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Dong, Yue and Mircea, Andrei and Cheung, Jackie C. K.},
	month = jan,
	year = {2021},
	note = {arXiv:2005.00513 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{alsentzer_publicly_2019-1,
	title = {Publicly {Available} {Clinical} {BERT} {Embeddings}},
	url = {http://arxiv.org/abs/1904.03323},
	doi = {10.48550/arXiv.1904.03323},
	abstract = {Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Alsentzer, Emily and Murphy, John R. and Boag, Willie and Weng, Wei-Hung and Jin, Di and Naumann, Tristan and McDermott, Matthew B. A.},
	month = jun,
	year = {2019},
	note = {arXiv:1904.03323 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{abacha_overview_2021,
	title = {Overview of the {MEDIQA} 2021 shared task on summarization in the medical domain},
	url = {https://aclanthology.org/2021.bionlp-1.8/},
	urldate = {2025-06-24},
	booktitle = {Proceedings of the 20th {Workshop} on {Biomedical} {Language} {Processing}},
	author = {Abacha, Asma Ben and M’rabet, Yassine and Zhang, Yuhao and Shivade, Chaitanya and Langlotz, Curtis and Demner-Fushman, Dina},
	year = {2021},
	pages = {74--85},
}

@inproceedings{lin_class_2011,
	title = {A class of submodular functions for document summarization},
	url = {https://aclanthology.org/P11-1052.pdf},
	urldate = {2025-06-23},
	booktitle = {Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies},
	author = {Lin, Hui and Bilmes, Jeff},
	year = {2011},
	pages = {510--520},
}

@article{edmundson_new_1969,
	title = {New {Methods} in {Automatic} {Extracting}},
	volume = {16},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/321510.321519},
	doi = {10.1145/321510.321519},
	abstract = {This paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location).
            The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts.},
	language = {en},
	number = {2},
	urldate = {2025-06-23},
	journal = {Journal of the ACM},
	author = {Edmundson, H. P.},
	month = apr,
	year = {1969},
	pages = {264--285},
}

@article{dumais_latent_2004,
	title = {Latent semantic analysis.},
	volume = {38},
	url = {https://eric.ed.gov/?id=EJ678116},
	urldate = {2025-06-23},
	journal = {Annual Review of Information Science and Technology (ARIST)},
	author = {Dumais, Susan T.},
	year = {2004},
	note = {Publisher: ERIC},
	pages = {189--230},
}

@article{luhn_automatic_1958-1,
	title = {The automatic creation of literature abstracts},
	volume = {2},
	url = {https://ieeexplore.ieee.org/abstract/document/5392672/},
	number = {2},
	urldate = {2025-06-23},
	journal = {IBM Journal of research and development},
	author = {Luhn, Hans Peter},
	year = {1958},
	note = {Publisher: Ibm},
	pages = {159--165},
}

@inproceedings{mihalcea_textrank_2004,
	title = {Textrank: {Bringing} order into text},
	shorttitle = {Textrank},
	url = {https://aclanthology.org/W04-3252.pdf},
	urldate = {2025-06-23},
	booktitle = {Proceedings of the 2004 conference on empirical methods in natural language processing},
	author = {Mihalcea, Rada and Tarau, Paul},
	year = {2004},
	pages = {404--411},
}

@article{brin_pagerank_1998,
	title = {The {PageRank} citation ranking: bringing order to the web},
	volume = {98},
	shorttitle = {The {PageRank} citation ranking},
	url = {https://cir.nii.ac.jp/crid/1571417125844972032},
	urldate = {2025-06-23},
	journal = {Proceedings of ASIS, 1998},
	author = {Brin, Sergey},
	year = {1998},
	pages = {161--172},
}

@techreport{page_pagerank_1999,
	title = {The {PageRank} citation ranking: {Bringing} order to the web.},
	shorttitle = {The {PageRank} citation ranking},
	url = {http://ilpubs.stanford.edu:8090/422/?utm_campaign=Technical%20SEO%20Weekly&utm_medium=email&utm_source=Revue%20newsletter},
	urldate = {2025-06-23},
	institution = {Stanford infolab},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	year = {1999},
}

@article{erkan_lexrank_2004,
	title = {Lexrank: {Graph}-based lexical centrality as salience in text summarization},
	volume = {22},
	shorttitle = {Lexrank},
	url = {https://www.jair.org/index.php/jair/article/view/10396},
	urldate = {2025-06-23},
	journal = {Journal of artificial intelligence research},
	author = {Erkan, Günes and Radev, Dragomir R.},
	year = {2004},
	pages = {457--479},
}

@incollection{jain_pneumonia_2025,
	address = {Treasure Island (FL)},
	title = {Pneumonia {Pathology}},
	copyright = {Copyright © 2025, StatPearls Publishing LLC.},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK526116/},
	abstract = {Pneumonia has been defined as an infection of the lung parenchyma. Rather than looking at it as a single disease, health care professionals must remember that pneumonia is an umbrella term for a group of syndromes caused by a variety of organisms resulting in varied manifestations and sequelae. There have been many attempts to classify pneumonia based on the etiology, clinical setting in which the patent acquired the infection, and the pattern of involvement of lung parenchyma, among other classifications. This article reviews pneumonia based on the classification followed by the American Thoracic Society. Community-Acquired Pneumonia (CAP) Any pneumonia acquired outside of a hospital in a community setting. Hospital-Acquired Pneumonia (HAP) Any pneumonia acquired 48 hours after being admitted in an inpatient setting such as a hospital and not incubating at the time of admission is considered as HAP. This classification helps clear the confusion surrounding the terms healthcare-associated and hospital-acquired pneumonia. Now all pneumonia acquired in the setting of assisted-living facilities, rehabilitation facilities, and other healthcare facilities have been included under community-acquired pneumonia, and a hospital setting is necessary for classifying pneumonia as HAP. Ventilator Associated Pneumonia (VAP) Any pneumonia acquired 48 hours after endotracheal intubation is considered as VAP. These categories have helped establish the common organisms responsible for each type of pneumonia and helped to formulate treatment guidelines for the efficient management in both in-patient and out-patient setting. Depending on the pattern of involvement, pneumonia has historically also been studied as: Focal non-segmental or lobar pneumonia: involvement of a single lobe of the lung. . Multifocal bronchopneumonia or lobular pneumonia. Focal or diffuse interstitial pneumonia.},
	language = {eng},
	urldate = {2025-06-03},
	booktitle = {{StatPearls}},
	publisher = {StatPearls Publishing},
	author = {Jain, Vardhmaan and Vashisht, Rishik and Yilmaz, Gizem and Bhardwaj, Abhishek},
	year = {2025},
	pmid = {30252372},
}

@inproceedings{zhang_learning_2018,
	address = {Brussels, Belgium},
	title = {Learning to {Summarize} {Radiology} {Findings}},
	url = {http://aclweb.org/anthology/W18-5623},
	doi = {10.18653/v1/W18-5623},
	abstract = {The Impression section of a radiology report summarizes crucial radiology ﬁndings in natural language and plays a central role in communicating these ﬁndings to physicians. However, the process of generating impressions by summarizing ﬁndings is time-consuming for radiologists and prone to errors. We propose to automate the generation of radiology impressions with neural sequence-to-sequence learning. We further propose a customized neural model for this task which learns to encode the study background information and use this information to guide the decoding process. On a large dataset of radiology reports collected from actual hospital studies, our model outperforms existing non-neural and neural baselines under the ROUGE metrics. In a blind experiment, a board-certiﬁed radiologist indicated that 67\% of sampled system summaries are at least as good as the corresponding humanwritten summaries, suggesting signiﬁcant clinical validity. To our knowledge our work represents the ﬁrst attempt in this direction.},
	language = {en},
	urldate = {2025-05-09},
	booktitle = {Proceedings of the {Ninth} {International} {Workshop} on {Health} {Text} {Mining} and {Information} {Analysis}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yuhao and Ding, Daisy Yi and Qian, Tianpei and Manning, Christopher D. and Langlotz, Curtis P.},
	year = {2018},
	pages = {204--213},
}

@article{frisoni_graph-enhanced_2023,
	title = {Graph-{Enhanced} {Biomedical} {Abstractive} {Summarization} {Via} {Factual} {Evidence} {Extraction}},
	volume = {4},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-023-01867-1},
	doi = {10.1007/s42979-023-01867-1},
	abstract = {Infusing structured semantic representations into language models is a rising research trend underpinning many natural language processing tasks that require understanding and reasoning capabilities. Decoupling factual non-ambiguous concept units from the lexical surface holds great potential in abstractive summarization, especially in the biomedical domain, where fact selection and rephrasing are made more difficult by specialized jargon and hard factuality constraints. Nevertheless, current graph-augmented contributions rely on extractive binary relations, failing to model real-world n-ary and nested biomedical interactions mentioned in the text. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization empowered by event extraction, namely graph-based representations of relevant medical evidence derived from the source scientific document. By relying on dual text-graph encoders, we prove the promising role of explicit event structures, achieving better or comparable performance than previous state-of-the-art models on the CDSR dataset. We conduct extensive ablation studies, including a wide experimentation of graph representation learning techniques. Finally, we offer some hints to guide future research in the field.},
	language = {en},
	number = {5},
	urldate = {2025-05-09},
	journal = {SN Computer Science},
	author = {Frisoni, Giacomo and Italiani, Paolo and Moro, Gianluca and Bartolini, Ilaria and Boschetti, Marco Antonio and Carbonaro, Antonella},
	month = jun,
	year = {2023},
	keywords = {Abstractive document summarization, Biomedical text mining, Event extraction, Knowledge-driven natural language processing, Natural language understanding, Semantic parsing},
	pages = {500},
}

@inproceedings{alambo_entity-driven_2022,
	title = {Entity-{Driven} {Fact}-{Aware} {Abstractive} {Summarization} of {Biomedical} {Literature}},
	url = {https://ieeexplore.ieee.org/document/9956656/},
	doi = {10.1109/ICPR56361.2022.9956656},
	abstract = {As part of the large number of scientific articles being published every year, the publication rate of biomedical literature has been increasing. Consequently, there has been considerable effort to harness and summarize the massive amount of biomedical research articles. While transformer-based encoder-decoder models in a vanilla source document-to-summary setting have been extensively studied for abstractive summarization in different domains, their major limitations continue to be entity hallucination (a phenomenon where generated summaries constitute entities not related to or present in source article(s)) and factual inconsistency. This problem is exacerbated in a biomedical setting where named entities and their semantics (which can be captured through a knowledge base) constitute the essence of an article. The use of named entities and facts mined from background knowledge bases pertaining to the named entities to guide abstractive summarization has not been studied in biomedical article summarization literature. In this paper, we propose an entity-driven fact-aware framework for training end-to-end transformer-based encoder-decoder models for abstractive summarization of biomedical articles. We call the proposed approach, whose building block is a transformer-based model, EFAS, Entity-driven Fact-aware Abstractive Summarization. We conduct a set of experiments using five state-of-the-art transformer-based encoder-decoder models (two of which are specifically designed for long document summarization) and demonstrate that injecting knowledge into the training/inference phase of these models enables the models to achieve significantly better performance than the standard source document-to-summary setting in terms of entity-level factual accuracy, N-gram novelty, and semantic equivalence while performing comparably on ROUGE metrics. The proposed approach is evaluated on ICD-11-Summ-1000, a dataset we build for abstractive summarization of biomedical literature, and PubMed-50k, a segment of a large-scale benchmark dataset for abstractive summarization of biomedical literature.},
	urldate = {2025-05-09},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Alambo, Amanuel and Banerjee, Tanvi and Thirunarayan, Krishnaprasad and Raymer, Michael},
	month = aug,
	year = {2022},
	note = {ISSN: 2831-7475},
	keywords = {Abstractive Summarization, Benchmark testing, Biological system modeling, ICD-11, Knowledge Bases, Knowledge Retrieval, Knowledge based systems, Knowledge-enhanced Natural Language Generation, Measurement, Named Entity Recognition, Natural languages, Semantics, Training, Transformers},
	pages = {613--620},
}

@article{demner-fushman_preparing_2016,
	title = {Preparing a collection of radiology examinations for distribution and retrieval},
	volume = {23},
	issn = {1067-5027},
	url = {https://doi.org/10.1093/jamia/ocv080},
	doi = {10.1093/jamia/ocv080},
	abstract = {Objective Clinical documents made available for secondary use play an increasingly important role in discovery of clinical knowledge, development of research methods, and education. An important step in facilitating secondary use of clinical document collections is easy access to descriptions and samples that represent the content of the collections. This paper presents an approach to developing a collection of radiology examinations, including both the images and radiologist narrative reports, and making them publicly available in a searchable database. Materials and Methods The authors collected 3996 radiology reports from the Indiana Network for Patient Care and 8121 associated images from the hospitals’ picture archiving systems. The images and reports were de-identified automatically and then the automatic de-identification was manually verified. The authors coded the key findings of the reports and empirically assessed the benefits of manual coding on retrieval. Results The automatic de-identification of the narrative was aggressive and achieved 100\% precision at the cost of rendering a few findings uninterpretable. Automatic de-identification of images was not quite as perfect. Images for two of 3996 patients (0.05\%) showed protected health information. Manual encoding of findings improved retrieval precision. Conclusion Stringent de-identification methods can remove all identifiers from text radiology reports. DICOM de-identification of images does not remove all identifying information and needs special attention to images scanned from film. Adding manual coding to the radiologist narrative reports significantly improved relevancy of the retrieved clinical documents. The de-identified Indiana chest X-ray collection is available for searching and downloading from the National Library of Medicine ( http://openi.nlm.nih.gov/ ).},
	number = {2},
	urldate = {2025-05-09},
	journal = {Journal of the American Medical Informatics Association},
	author = {Demner-Fushman, Dina and Kohli, Marc D. and Rosenman, Marc B. and Shooshan, Sonya E. and Rodriguez, Laritza and Antani, Sameer and Thoma, George R. and McDonald, Clement J.},
	month = mar,
	year = {2016},
	pages = {304--310},
}

@article{demner-fushman_preparing_2016-1,
	title = {Preparing a collection of radiology examinations for distribution and retrieval},
	volume = {23},
	issn = {1067-5027},
	url = {https://doi.org/10.1093/jamia/ocv080},
	doi = {10.1093/jamia/ocv080},
	abstract = {Objective Clinical documents made available for secondary use play an increasingly important role in discovery of clinical knowledge, development of research methods, and education. An important step in facilitating secondary use of clinical document collections is easy access to descriptions and samples that represent the content of the collections. This paper presents an approach to developing a collection of radiology examinations, including both the images and radiologist narrative reports, and making them publicly available in a searchable database. Materials and Methods The authors collected 3996 radiology reports from the Indiana Network for Patient Care and 8121 associated images from the hospitals’ picture archiving systems. The images and reports were de-identified automatically and then the automatic de-identification was manually verified. The authors coded the key findings of the reports and empirically assessed the benefits of manual coding on retrieval. Results The automatic de-identification of the narrative was aggressive and achieved 100\% precision at the cost of rendering a few findings uninterpretable. Automatic de-identification of images was not quite as perfect. Images for two of 3996 patients (0.05\%) showed protected health information. Manual encoding of findings improved retrieval precision. Conclusion Stringent de-identification methods can remove all identifiers from text radiology reports. DICOM de-identification of images does not remove all identifying information and needs special attention to images scanned from film. Adding manual coding to the radiologist narrative reports significantly improved relevancy of the retrieved clinical documents. The de-identified Indiana chest X-ray collection is available for searching and downloading from the National Library of Medicine ( http://openi.nlm.nih.gov/ ).},
	number = {2},
	urldate = {2025-05-09},
	journal = {Journal of the American Medical Informatics Association},
	author = {Demner-Fushman, Dina and Kohli, Marc D. and Rosenman, Marc B. and Shooshan, Sonya E. and Rodriguez, Laritza and Antani, Sameer and Thoma, George R. and McDonald, Clement J.},
	month = mar,
	year = {2016},
	pages = {304--310},
}

@misc{noauthor_preparing_nodate,
	title = {Preparing a collection of radiology examinations for distribution and retrieval {\textbar} {Journal} of the {American} {Medical} {Informatics} {Association} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/jamia/article-abstract/23/2/304/2572395},
	urldate = {2025-05-09},
}

@inproceedings{suster_clicr_2018,
	address = {New Orleans, Louisiana},
	title = {{CliCR}: a {Dataset} of {Clinical} {Case} {Reports} for {Machine} {Reading} {Comprehension}},
	shorttitle = {{CliCR}},
	url = {https://aclanthology.org/N18-1140/},
	doi = {10.18653/v1/N18-1140},
	abstract = {We present a new dataset for machine comprehension in the medical domain. Our dataset uses clinical case reports with around 100,000 gap-filling queries about these cases. We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20\% F1) between the best human and machine readers. We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills. We find that inferences using domain knowledge and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines.},
	urldate = {2025-05-08},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Šuster, Simon and Daelemans, Walter},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {1551--1563},
}

@inproceedings{ben_abacha_overview_2019,
	address = {Florence, Italy},
	title = {Overview of the {MEDIQA} 2019 {Shared} {Task} on {Textual} {Inference}, {Question} {Entailment} and {Question} {Answering}},
	url = {https://www.aclweb.org/anthology/W19-5039},
	doi = {10.18653/v1/W19-5039},
	abstract = {This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain speciﬁc information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98\% in the NLI task, 74.9\% in the RQE task, and 78.3\% in the QA task. In this paper, we describe the tasks, the datasets, and the participants’ approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.},
	language = {en},
	urldate = {2025-05-08},
	booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Ben Abacha, Asma and Shivade, Chaitanya and Demner-Fushman, Dina},
	year = {2019},
	pages = {370--379},
}

@misc{lehman_clinical-t5_nodate,
	title = {Clinical-{T5}: {Large} {Language} {Models} {Built} {Using} {MIMIC} {Clinical} {Text}},
	shorttitle = {Clinical-{T5}},
	url = {https://physionet.org/content/clinical-t5/},
	doi = {10.13026/AW2E-HE88},
	abstract = {Recent advances in scaling large language models (LLMs) has resulted in
significant improvements over a number of natural language processing
benchmarks. There has been some work to pretrain these language models over
clinical text. These works demonstrate that training a language model using
masked language modeling (MLM) on clinical notes is an effective technique for
boosting performance on downstream tasks. All of these previous works use
decoder-only architectures. We train 4 different clinical T5 models on the
union of MIMIC-III and IV notes. Two of the models are initialized from
previous T5-models (T5-base and SciFive). We additionally train a T5-Base and
T5-Large model from scratch. These models should not be distributed to non-
credentialed users. Research has shown that these language models have the
potential to leak sensitive information. Due to this potential risk, we
release the model weights under PhysioNet credentialed access.},
	urldate = {2025-05-08},
	publisher = {PhysioNet},
	author = {Lehman, Eric and Johnson, Alistair},
}

@misc{lehman_clinical-t5_nodate-1,
	title = {Clinical-{T5}: {Large} {Language} {Models} {Built} {Using} {MIMIC} {Clinical} {Text}},
	shorttitle = {Clinical-{T5}},
	url = {https://physionet.org/content/clinical-t5/1.0.0/},
	doi = {10.13026/RJ8X-V335},
	abstract = {Recent advances in scaling large language models (LLMs) has resulted in
significant improvements over a number of natural language processing
benchmarks. There has been some work to pretrain these language models over
clinical text. These works demonstrate that training a language model using
masked language modeling (MLM) on clinical notes is an effective technique for
boosting performance on downstream tasks. All of these previous works use
decoder-only architectures. We train 4 different clinical T5 models on the
union of MIMIC-III and IV notes. Two of the models are initialized from
previous T5-models (T5-base and SciFive). We additionally train a T5-Base and
T5-Large model from scratch. These models should not be distributed to non-
credentialed users. Research has shown that these language models have the
potential to leak sensitive information. Due to this potential risk, we
release the model weights under PhysioNet credentialed access.},
	urldate = {2025-05-08},
	publisher = {PhysioNet},
	author = {Lehman, Eric and Johnson, Alistair},
}

@misc{wang_cord-19_2020,
	title = {{CORD}-19: {The} {COVID}-19 {Open} {Research} {Dataset}},
	shorttitle = {{CORD}-19},
	url = {http://arxiv.org/abs/2004.10706},
	doi = {10.48550/arXiv.2004.10706},
	abstract = {The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.},
	urldate = {2025-05-08},
	publisher = {arXiv},
	author = {Wang, Lucy Lu and Lo, Kyle and Chandrasekhar, Yoganand and Reas, Russell and Yang, Jiangjiang and Burdick, Doug and Eide, Darrin and Funk, Kathryn and Katsis, Yannis and Kinney, Rodney and Li, Yunyao and Liu, Ziyang and Merrill, William and Mooney, Paul and Murdick, Dewey and Rishi, Devvret and Sheehan, Jerry and Shen, Zhihong and Stilson, Brandon and Wade, Alex and Wang, Kuansan and Wang, Nancy Xin Ru and Wilhelm, Chris and Xie, Boya and Raymond, Douglas and Weld, Daniel S. and Etzioni, Oren and Kohlmeier, Sebastian},
	month = jul,
	year = {2020},
	note = {arXiv:2004.10706 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Digital Libraries},
}

@incollection{muller_bioasq_2015,
	address = {Cham},
	title = {{BioASQ}: {A} {Challenge} on {Large}-{Scale} {Biomedical} {Semantic} {Indexing} and {Question} {Answering}},
	volume = {9059},
	isbn = {978-3-319-24470-9 978-3-319-24471-6},
	shorttitle = {{BioASQ}},
	url = {http://link.springer.com/10.1007/978-3-319-24471-6_3},
	abstract = {This article provides an overview of BIOASQ, a new competition on biomedical semantic indexing and question answering (QA). BIOASQ aims to push towards systems that will allow biomedical workers to express their information needs in natural language and that will return concise and user-understandable answers by combining information from multiple sources of different kinds, including biomedical articles, databases, and ontologies. BIOASQ encourages participants to adopt semantic indexing as a means to combine multiple information sources and to facilitate the matching of questions to answers. It also adopts a broad semantic indexing and QA architecture that subsumes current relevant approaches, even though no current system instantiates all of its components. Hence, the architecture can also be seen as our view of how relevant work from ﬁelds such as information retrieval, hierarchical classiﬁcation, question answering, ontologies, and linked data can be combined, extended, and applied to biomedical question answering. BIOASQ will develop publicly available benchmarks and it will adopt and possibly reﬁne existing evaluation measures. The evaluation infrastructure of the competition will remain publicly available beyond the end of BIOASQ.},
	language = {en},
	urldate = {2025-05-08},
	booktitle = {Multimodal {Retrieval} in the {Medical} {Domain}},
	publisher = {Springer International Publishing},
	author = {Balikas, Georgios and Krithara, Anastasia and Partalas, Ioannis and Paliouras, George},
	editor = {Müller, Henning and Jimenez Del Toro, Oscar Alfonso and Hanbury, Allan and Langs, Georg and Foncubierta Rodriguez, Antonio},
	year = {2015},
	doi = {10.1007/978-3-319-24471-6_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {26--39},
}

@article{uzuner_2010_2011,
	title = {2010 i2b2/{VA} challenge on concepts, assertions, and relations in clinical text},
	volume = {18},
	issn = {1527-974X, 1067-5027},
	url = {https://academic.oup.com/jamia/article/18/5/552/830538},
	doi = {10.1136/amiajnl-2011-000203},
	abstract = {Abstract
            The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification.
            These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.},
	language = {en},
	number = {5},
	urldate = {2025-05-08},
	journal = {Journal of the American Medical Informatics Association},
	author = {Uzuner, Özlem and South, Brett R and Shen, Shuying and DuVall, Scott L},
	month = sep,
	year = {2011},
	pages = {552--556},
}

@article{johnson_mimic-iii_2016,
	title = {{MIMIC}-{III}, a freely accessible critical care database},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201635},
	doi = {10.1038/sdata.2016.35},
	abstract = {MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
	language = {en},
	number = {1},
	urldate = {2025-05-08},
	journal = {Scientific Data},
	author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
	month = may,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Health care, Medical research, Outcomes research, Prognosis},
	pages = {160035},
}

@misc{cohan_discourse-aware_2018,
	title = {A {Discourse}-{Aware} {Attention} {Model} for {Abstractive} {Summarization} of {Long} {Documents}},
	url = {http://arxiv.org/abs/1804.05685},
	doi = {10.48550/arXiv.1804.05685},
	abstract = {Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.},
	urldate = {2025-05-08},
	publisher = {arXiv},
	author = {Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
	month = may,
	year = {2018},
	note = {arXiv:1804.05685 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ghosh_clipsyntel_2023,
	title = {{CLIPSyntel}: {CLIP} and {LLM} {Synergy} for {Multimodal} {Question} {Summarization} in {Healthcare}},
	shorttitle = {{CLIPSyntel}},
	url = {http://arxiv.org/abs/2312.11541},
	doi = {10.48550/arXiv.2312.11541},
	abstract = {In the era of modern healthcare, swiftly generating medical question summaries is crucial for informed and timely patient care. Despite the increasing complexity and volume of medical data, existing studies have focused solely on text-based summarization, neglecting the integration of visual information. Recognizing the untapped potential of combining textual queries with visual representations of medical conditions, we introduce the Multimodal Medical Question Summarization (MMQS) Dataset. This dataset, a major contribution to our work, pairs medical queries with visual aids, facilitating a richer and more nuanced understanding of patient needs. We also propose a framework, utilizing the power of Contrastive Language Image Pretraining(CLIP) and Large Language Models(LLMs), consisting of four modules that identify medical disorders, generate relevant context, filter medical concepts, and craft visually aware summaries. Our comprehensive framework harnesses the power of CLIP, a multimodal foundation model, and various general-purpose LLMs, comprising four main modules: the medical disorder identification module, the relevant context generation module, the context filtration module for distilling relevant medical concepts and knowledge, and finally, a general-purpose LLM to generate visually aware medical question summaries. Leveraging our MMQS dataset, we showcase how visual cues from images enhance the generation of medically nuanced summaries. This multimodal approach not only enhances the decision-making process in healthcare but also fosters a more nuanced understanding of patient queries, laying the groundwork for future research in personalized and responsive medical care},
	urldate = {2025-05-08},
	publisher = {arXiv},
	author = {Ghosh, Akash and Acharya, Arkadeep and Jain, Raghav and Saha, Sriparna and Chadha, Aman and Sinha, Setu},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11541 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhu_leveraging_2023,
	title = {Leveraging {Summary} {Guidance} on {Medical} {Report} {Summarization}},
	url = {http://arxiv.org/abs/2302.04001},
	doi = {10.48550/arXiv.2302.04001},
	abstract = {This study presents three deidentified large medical text datasets, named DISCHARGE, ECHO and RADIOLOGY, which contain 50K, 16K and 378K pairs of report and summary that are derived from MIMIC-III, respectively. We implement convincing baselines of automated abstractive summarization on the proposed datasets with pre-trained encoder-decoder language models, including BERT2BERT, T5-large and BART. Further, based on the BART model, we leverage the sampled summaries from the train set as prior knowledge guidance, for encoding additional contextual representations of the guidance with the encoder and enhancing the decoding representations in the decoder. The experimental results confirm the improvement of ROUGE scores and BERTScore made by the proposed method, outperforming the larger model T5-large.},
	urldate = {2025-05-08},
	publisher = {arXiv},
	author = {Zhu, Yunqi and Yang, Xuebing and Wu, Yuanyuan and Zhang, Wensheng},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04001 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{otmakhova_m3_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {M3: {Multi}-level dataset for {Multi}-document summarisation of {Medical} studies},
	shorttitle = {M3},
	url = {https://aclanthology.org/2022.findings-emnlp.286/},
	doi = {10.18653/v1/2022.findings-emnlp.286},
	abstract = {We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies), a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity: documents, sentences, and propositions. The dataset also includes several levels of annotation, including biomedical entities, direction, and strength of relations between them, and the discourse relationships between the input documents (“contradiction” or “agreement”). We showcase usage scenarios of the dataset by testing 10 generic and domain-specific summarisation models in a zero-shot setting, and introduce a probing task based on counterfactuals to test if models are aware of the direction and strength of the conclusions generated from input studies.},
	urldate = {2025-05-08},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Otmakhova, Yulia and Verspoor, Karin and Baldwin, Timothy and Jimeno Yepes, Antonio and Lau, Jey Han},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {3887--3901},
}

@inproceedings{gupta_sumpubmed_2021-1,
	address = {Online},
	title = {{SumPubMed}: {Summarization} {Dataset} of {PubMed} {Scientific} {Articles}},
	shorttitle = {{SumPubMed}},
	url = {https://aclanthology.org/2021.acl-srw.30/},
	doi = {10.18653/v1/2021.acl-srw.30},
	abstract = {Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SumPubMed , using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SumPubMed . SumPubMed is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SumPubMed . Thus, SumPubMed opens new avenues for the future improvement of models as well as the development of new evaluation metrics.},
	urldate = {2025-05-08},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}: {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Gupta, Vivek and Bharti, Prerna and Nokhiz, Pegah and Karnick, Harish},
	editor = {Kabbara, Jad and Lin, Haitao and Paullada, Amandalynne and Vamvas, Jannis},
	month = aug,
	year = {2021},
	pages = {292--303},
}

@inproceedings{ben_abacha_summarization_2019,
	address = {Florence, Italy},
	title = {On the {Summarization} of {Consumer} {Health} {Questions}},
	url = {https://aclanthology.org/P19-1215/},
	doi = {10.18653/v1/P19-1215},
	abstract = {Question understanding is one of the main challenges in question answering. In real world applications, users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question, leading to substantially more false positives in answer retrieval. In this paper, we study neural abstractive models for medical question summarization. We introduce the MeQSum corpus of 1,000 summarized consumer health questions. We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. In particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a ROUGE-1 score of 44.16\%. We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization.},
	urldate = {2025-05-08},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ben Abacha, Asma and Demner-Fushman, Dina},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {2228--2234},
}

@inproceedings{deyoung_ms2_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{MSˆ2}: {Multi}-{Document} {Summarization} of {Medical} {Studies}},
	shorttitle = {{MSˆ2}},
	url = {https://aclanthology.org/2021.emnlp-main.594/},
	doi = {10.18653/v1/2021.emnlp-main.594},
	abstract = {To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MSˆ2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system`s generated summaries. Data and models are available at https://github.com/allenai/ms2.},
	urldate = {2025-05-08},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {DeYoung, Jay and Beltagy, Iz and van Zuylen, Madeleine and Kuehl, Bailey and Wang, Lucy Lu},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {7494--7513},
}

@misc{johnson_mimic-cxr_nodate,
	title = {{MIMIC}-{CXR} {Database}},
	url = {https://physionet.org/content/mimic-cxr/2.1.0/},
	doi = {10.13026/4JQJ-JW95},
	abstract = {The MIMIC Chest X-ray (MIMIC-CXR) Database v2.0.0 is a large publicly
available dataset of chest radiographs in DICOM format with free-text
radiology reports. The dataset contains 377,110 images corresponding to
227,835 radiographic studies performed at the Beth Israel Deaconess Medical
Center in Boston, MA. The dataset is de-identified to satisfy the US Health
Insurance Portability and Accountability Act of 1996 (HIPAA) Safe Harbor
requirements. Protected health information (PHI) has been removed. The dataset
is intended to support a wide body of research in medicine including image
understanding, natural language processing, and decision support.},
	urldate = {2025-05-08},
	publisher = {PhysioNet},
	author = {Johnson, Alistair and Pollard, Tom and Mark, Roger and Berkowitz, Seth and Horng, Steven},
}

@misc{johnson_mimic-cxr_nodate-1,
	title = {{MIMIC}-{CXR} {Database}},
	url = {https://physionet.org/content/mimic-cxr/2.1.0/},
	doi = {10.13026/4JQJ-JW95},
	abstract = {The MIMIC Chest X-ray (MIMIC-CXR) Database v2.0.0 is a large publicly
available dataset of chest radiographs in DICOM format with free-text
radiology reports. The dataset contains 377,110 images corresponding to
227,835 radiographic studies performed at the Beth Israel Deaconess Medical
Center in Boston, MA. The dataset is de-identified to satisfy the US Health
Insurance Portability and Accountability Act of 1996 (HIPAA) Safe Harbor
requirements. Protected health information (PHI) has been removed. The dataset
is intended to support a wide body of research in medicine including image
understanding, natural language processing, and decision support.},
	urldate = {2025-05-08},
	publisher = {PhysioNet},
	author = {Johnson, Alistair and Pollard, Tom and Mark, Roger and Berkowitz, Seth and Horng, Steven},
}

@article{wang_large_2024,
	title = {Large language models in medical and healthcare fields: applications, advances, and challenges},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Large language models in medical and healthcare fields},
	url = {https://doi.org/10.1007/s10462-024-10921-0},
	doi = {10.1007/s10462-024-10921-0},
	abstract = {Large language models (LLMs) are increasingly recognized for their advanced language capabilities, offering significant assistance in diverse areas like medical communication, patient data optimization, and surgical planning. Our survey meticulously searched for papers with keywords such as “medical,” “clinical,” “healthcare,” and “LLMs” across various databases, including ACM and Google Scholar. It sought to delve into the latest trends and applications of LLMs in healthcare, analyzing 175 relevant publications to support both practitioners and researchers in the field. We have compiled 56 experimental datasets, various evaluation methods and reviewed cutting-edge LLMs across tasks. Our comprehensive analysis of LLMs in healthcare applications, including medical question-answering, dialogue summarization, electronic health record generation, scientific research, medical education, medical product safety monitoring, clinical health reasoning, and clinical decision support. Furthermore, we have identified the challenges, including data security, inaccurate information, fairness and bias, plagiarism, copyrights, and accountability, and the potential solutions, namely de-identification framework, references,counterfactually fair prompting,opening and ending control codes, and establishing normative standards,to address these open issues,respectively. The findings of this survey exert a profound impact on spurring innovation in practical applications and addressing inherent challenges within the academic and medical communities.},
	language = {en},
	number = {11},
	urldate = {2025-05-04},
	journal = {Artificial Intelligence Review},
	author = {Wang, Dandan and Zhang, Shiqing},
	month = sep,
	year = {2024},
	keywords = {Artificial Intelligence, Clinical health reasoning, Electronic health record generation, Fairness and bias, Healthcare applications, Large language models, Medical question-answering},
	pages = {299},
}

@misc{li_chatdoctor_2023,
	title = {{ChatDoctor}: {A} {Medical} {Chat} {Model} {Fine}-{Tuned} on a {Large} {Language} {Model} {Meta}-{AI} ({LLaMA}) {Using} {Medical} {Domain} {Knowledge}},
	shorttitle = {{ChatDoctor}},
	url = {http://arxiv.org/abs/2303.14070},
	doi = {10.48550/arXiv.2303.14070},
	abstract = {The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.},
	urldate = {2025-05-04},
	publisher = {arXiv},
	author = {Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
	month = jun,
	year = {2023},
	note = {arXiv:2303.14070 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{luan_multi-task_2018,
	title = {Multi-{Task} {Identification} of {Entities}, {Relations}, and {Coreference} for {Scientific} {Knowledge} {Graph} {Construction}},
	url = {http://arxiv.org/abs/1808.09602},
	doi = {10.48550/arXiv.1808.09602},
	abstract = {We introduce a multi-task setup of identifying and classifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called Scientific Information Extractor (SciIE) for with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.},
	urldate = {2025-05-03},
	publisher = {arXiv},
	author = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
	month = aug,
	year = {2018},
	note = {arXiv:1808.09602 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_multi-task_nodate,
	title = {Multi-{Task} {Identification} of {Entities}, {Relations}, and {Coreferencefor} {Scientific} {Knowledge} {Graph} {Construction}},
	url = {https://nlp.cs.washington.edu/sciIE/},
	urldate = {2025-05-03},
}

@article{zhang_applications_2022,
	title = {Applications of {Explainable} {Artificial} {Intelligence} in {Diagnosis} and {Surgery}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/12/2/237},
	doi = {10.3390/diagnostics12020237},
	abstract = {In recent years, artificial intelligence (AI) has shown great promise in medicine. However, explainability issues make AI applications in clinical usages difficult. Some research has been conducted into explainable artificial intelligence (XAI) to overcome the limitation of the black-box nature of AI methods. Compared with AI techniques such as deep learning, XAI can provide both decision-making and explanations of the model. In this review, we conducted a survey of the recent trends in medical diagnosis and surgical applications using XAI. We have searched articles published between 2019 and 2021 from PubMed, IEEE Xplore, Association for Computing Machinery, and Google Scholar. We included articles which met the selection criteria in the review and then extracted and analyzed relevant information from the studies. Additionally, we provide an experimental showcase on breast cancer diagnosis, and illustrate how XAI can be applied in medical XAI applications. Finally, we summarize the XAI methods utilized in the medical XAI applications, the challenges that the researchers have met, and discuss the future research directions. The survey result indicates that medical XAI is a promising research direction, and this study aims to serve as a reference to medical experts and AI scientists when designing medical XAI applications.},
	language = {en},
	number = {2},
	urldate = {2025-05-02},
	journal = {Diagnostics},
	author = {Zhang, Yiming and Weng, Ying and Lund, Jonathan},
	month = feb,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, deep learning, diagnosis, explainable artificial intelligence (XAI), machine learning, surgery},
	pages = {237},
}

@misc{aziz_explainable_2024,
	title = {Explainable {AI} in {Healthcare}: {Systematic} {Review} of {Clinical} {Decision} {Support} {Systems}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Explainable {AI} in {Healthcare}},
	url = {https://www.medrxiv.org/content/10.1101/2024.08.10.24311735v1},
	doi = {10.1101/2024.08.10.24311735},
	abstract = {This systematic review examines the evolution and current landscape of eXplainable Artificial Intelligence (XAI) in Clinical Decision Support Systems (CDSS), highlighting significant advancements and identifying persistent challenges. Utilising the PRISMA protocol, we searched major indexed databases such as Scopus, Web of Science, PubMed, and the Cochrane Library, to analyse publications from January 2000 to April 2024. This timeframe captures the progressive integration of XAI in CDSS, offering a historical and technological overview. The review covers the datasets, application areas, machine learning models, explainable AI methods, and evaluation strategies for multiple XAI methods.
Analysing 68 articles, we uncover valuable insights into the strengths and limitations of current XAI approaches, revealing significant research gaps and providing actionable recommendations. We emphasise the need for more public datasets, advanced data treatment methods, comprehensive evaluations of XAI methods, and interdisciplinary collaboration. Our findings stress the importance of balancing model performance with explainability and enhancing the usability of XAI tools for medical practitioners. This research provides a valuable resource for healthcare professionals, researchers, and policymakers seeking to develop and evaluate effective, ethical decision-support systems in clinical settings.},
	language = {en},
	urldate = {2025-05-02},
	publisher = {medRxiv},
	author = {Aziz, Noor A. and Manzoor, Awais and Qureshi, Muhammad Deedahwar Mazhar and Qureshi, M. Atif and Rashwan, Wael},
	month = aug,
	year = {2024},
	note = {Pages: 2024.08.10.24311735},
}

@article{frasca_explainable_2024,
	title = {Explainable and interpretable artificial intelligence in medicine: a systematic bibliometric review},
	volume = {4},
	issn = {2731-0809},
	shorttitle = {Explainable and interpretable artificial intelligence in medicine},
	url = {https://doi.org/10.1007/s44163-024-00114-7},
	doi = {10.1007/s44163-024-00114-7},
	abstract = {This review aims to explore the growing impact of machine learning and deep learning algorithms in the medical field, with a specific focus on the critical issues of explainability and interpretability associated with black-box algorithms. While machine learning algorithms are increasingly employed for medical analysis and diagnosis, their complexity underscores the importance of understanding how these algorithms explain and interpret data to take informed decisions. This review comprehensively analyzes challenges and solutions presented in the literature, offering an overview of the most recent techniques utilized in this field. It also provides precise definitions of interpretability and explainability, aiming to clarify the distinctions between these concepts and their implications for the decision-making process. Our analysis, based on 448 articles and addressing seven research questions, reveals an exponential growth in this field over the last decade. The psychological dimensions of public perception underscore the necessity for effective communication regarding the capabilities and limitations of artificial intelligence. Researchers are actively developing techniques to enhance interpretability, employing visualization methods and reducing model complexity. However, the persistent challenge lies in finding the delicate balance between achieving high performance and maintaining interpretability. Acknowledging the growing significance of artificial intelligence in aiding medical diagnosis and therapy, and the creation of interpretable artificial intelligence models is considered essential. In this dynamic context, an unwavering commitment to transparency, ethical considerations, and interdisciplinary collaboration is imperative to ensure the responsible use of artificial intelligence. This collective commitment is vital for establishing enduring trust between clinicians and patients, addressing emerging challenges, and facilitating the informed adoption of these advanced technologies in medicine.},
	language = {en},
	number = {1},
	urldate = {2025-05-02},
	journal = {Discover Artificial Intelligence},
	author = {Frasca, Maria and La Torre, Davide and Pravettoni, Gabriella and Cutica, Ilaria},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence},
	pages = {15},
}

@article{sadeghi_review_2024,
	title = {A review of {Explainable} {Artificial} {Intelligence} in healthcare},
	volume = {118},
	issn = {0045-7906},
	url = {https://www.sciencedirect.com/science/article/pii/S0045790624002982},
	doi = {10.1016/j.compeleceng.2024.109370},
	abstract = {Explainable Artificial Intelligence (XAI) encompasses the strategies and methodologies used in constructing AI systems that enable end-users to comprehend and interpret the outputs and predictions made by AI models. The increasing deployment of opaque AI applications in high-stakes fields, particularly healthcare, has amplified the need for clarity and explainability. This stems from the potential high-impact consequences of erroneous AI predictions in such critical sectors. The effective integration of AI models in healthcare hinges on the capacity of these models to be both explainable and interpretable. Gaining the trust of healthcare professionals necessitates AI applications to be transparent about their decision-making processes and underlying logic. Our paper conducts a systematic review of the various facets and challenges of XAI within the healthcare realm. It aims to dissect a range of XAI methodologies and their applications in healthcare, categorizing them into six distinct groups: feature-oriented methods, global methods, concept models, surrogate models, local pixel-based methods, and human-centric approaches. Specifically, this study focuses on the significance of XAI in addressing healthcare-related challenges, underscoring its vital role in safety-critical scenarios. Our objective is to provide an exhaustive exploration of XAI's applications in healthcare, alongside an analysis of relevant experimental outcomes, thereby fostering a holistic understanding of XAI's role and potential in this critical domain.},
	urldate = {2025-05-02},
	journal = {Computers and Electrical Engineering},
	author = {Sadeghi, Zahra and Alizadehsani, Roohallah and Cifci, Mehmet Akif and Kausar, Samina and Rehman, Rizwan and Mahanta, Priyakshi and Bora, Pranjal Kumar and Almasri, Ammar and Alkhawaldeh, Rami S. and Hussain, Sadiq and Alatas, Bilal and Shoeibi, Afshin and Moosaei, Hossein and Hladík, Milan and Nahavandi, Saeid and Pardalos, Panos M.},
	month = aug,
	year = {2024},
	keywords = {Explainable AI, Healthcare, Interpretability, Transparent AI},
	pages = {109370},
}

@article{ali_enlightening_2023,
	title = {The enlightening role of explainable artificial intelligence in medical \& healthcare domains: {A} systematic literature review},
	volume = {166},
	issn = {0010-4825},
	shorttitle = {The enlightening role of explainable artificial intelligence in medical \& healthcare domains},
	url = {https://www.sciencedirect.com/science/article/pii/S001048252301020X},
	doi = {10.1016/j.compbiomed.2023.107555},
	abstract = {In domains such as medical and healthcare, the interpretability and explainability of machine learning and artificial intelligence systems are crucial for building trust in their results. Errors caused by these systems, such as incorrect diagnoses or treatments, can have severe and even life-threatening consequences for patients. To address this issue, Explainable Artificial Intelligence (XAI) has emerged as a popular area of research, focused on understanding the black-box nature of complex and hard-to-interpret machine learning models. While humans can increase the accuracy of these models through technical expertise, understanding how these models actually function during training can be difficult or even impossible. XAI algorithms such as Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) can provide explanations for these models, improving trust in their predictions by providing feature importance and increasing confidence in the systems. Many articles have been published that propose solutions to medical problems by using machine learning models alongside XAI algorithms to provide interpretability and explainability. In our study, we identified 454 articles published from 2018–2022 and analyzed 93 of them to explore the use of these techniques in the medical domain.},
	urldate = {2025-05-02},
	journal = {Computers in Biology and Medicine},
	author = {Ali, Subhan and Akhlaq, Filza and Imran, Ali Shariq and Kastrati, Zenun and Daudpota, Sher Muhammad and Moosa, Muhammad},
	month = nov,
	year = {2023},
	keywords = {Artificial intelligence, Deep learning, Explainable, Healthcare, Machine learning, Medical},
	pages = {107555},
}

@misc{noauthor_applications_nodate,
	title = {Applications of {Explainable} {Artificial} {Intelligence} in {Diagnosis} and {Surgery}},
	url = {https://www.mdpi.com/2075-4418/12/2/237},
	urldate = {2025-05-02},
}

@article{omar_large_2024,
	title = {Large language models in medicine: {A} review of current clinical trials across healthcare applications},
	volume = {3},
	issn = {2767-3170},
	shorttitle = {Large language models in medicine},
	url = {https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000662},
	doi = {10.1371/journal.pdig.0000662},
	abstract = {This review analyzes current clinical trials investigating large language models’ (LLMs) applications in healthcare. We identified 27 trials (5 published and 22 ongoing) across 4 main clinical applications: patient care, data handling, decision support, and research assistance. Our analysis reveals diverse LLM uses, from clinical documentation to medical decision-making. Published trials show promise but highlight accuracy concerns. Ongoing studies explore novel applications like patient education and informed consent. Most trials occur in the United States of America and China. We discuss the challenges of evaluating rapidly evolving LLMs through clinical trials and identify gaps in current research. This review aims to inform future studies and guide the integration of LLMs into clinical practice.},
	language = {en},
	number = {11},
	urldate = {2025-05-02},
	journal = {PLOS Digital Health},
	author = {Omar, Mahmud and Nadkarni, Girish N. and Klang, Eyal and Glicksberg, Benjamin S.},
	month = nov,
	year = {2024},
	note = {Publisher: Public Library of Science},
	keywords = {Cancer treatment, Clinical trials, Decision making, Diagnostic medicine, Language, Medicine and health sciences, Patients, Randomized controlled trials},
	pages = {e0000662},
}

@article{wang_large_2024-1,
	title = {Large language models in medical and healthcare fields: applications, advances, and challenges},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Large language models in medical and healthcare fields},
	url = {https://doi.org/10.1007/s10462-024-10921-0},
	doi = {10.1007/s10462-024-10921-0},
	abstract = {Large language models (LLMs) are increasingly recognized for their advanced language capabilities, offering significant assistance in diverse areas like medical communication, patient data optimization, and surgical planning. Our survey meticulously searched for papers with keywords such as “medical,” “clinical,” “healthcare,” and “LLMs” across various databases, including ACM and Google Scholar. It sought to delve into the latest trends and applications of LLMs in healthcare, analyzing 175 relevant publications to support both practitioners and researchers in the field. We have compiled 56 experimental datasets, various evaluation methods and reviewed cutting-edge LLMs across tasks. Our comprehensive analysis of LLMs in healthcare applications, including medical question-answering, dialogue summarization, electronic health record generation, scientific research, medical education, medical product safety monitoring, clinical health reasoning, and clinical decision support. Furthermore, we have identified the challenges, including data security, inaccurate information, fairness and bias, plagiarism, copyrights, and accountability, and the potential solutions, namely de-identification framework, references,counterfactually fair prompting,opening and ending control codes, and establishing normative standards,to address these open issues,respectively. The findings of this survey exert a profound impact on spurring innovation in practical applications and addressing inherent challenges within the academic and medical communities.},
	language = {en},
	number = {11},
	urldate = {2025-05-02},
	journal = {Artificial Intelligence Review},
	author = {Wang, Dandan and Zhang, Shiqing},
	month = sep,
	year = {2024},
	keywords = {Artificial Intelligence, Clinical health reasoning, Electronic health record generation, Fairness and bias, Healthcare applications, Large language models, Medical question-answering},
	pages = {299},
}

@article{nazi_large_2024,
	title = {Large {Language} {Models} in {Healthcare} and {Medical} {Domain}: {A} {Review}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-9709},
	shorttitle = {Large {Language} {Models} in {Healthcare} and {Medical} {Domain}},
	url = {https://www.mdpi.com/2227-9709/11/3/57},
	doi = {10.3390/informatics11030057},
	abstract = {The deployment of large language models (LLMs) within the healthcare sector has sparked both enthusiasm and apprehension. These models exhibit the remarkable ability to provide proficient responses to free-text queries, demonstrating a nuanced understanding of professional medical knowledge. This comprehensive survey delves into the functionalities of existing LLMs designed for healthcare applications and elucidates the trajectory of their development, starting with traditional Pretrained Language Models (PLMs) and then moving to the present state of LLMs in the healthcare sector. First, we explore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare applications, particularly focusing on clinical language understanding tasks. These tasks encompass a wide spectrum, ranging from named entity recognition and relation extraction to natural language inference, multimodal medical applications, document classification, and question-answering. Additionally, we conduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain, while also assessing the utilization of various open-source LLMs and highlighting their significance in healthcare applications. Furthermore, we present the essential performance metrics employed to evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations. Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector by offering a holistic perspective on their potential benefits and shortcomings. This review provides a comprehensive exploration of the current landscape of LLMs in healthcare, addressing their role in transforming medical applications and the areas that warrant further research and development.},
	language = {en},
	number = {3},
	urldate = {2025-05-02},
	journal = {Informatics},
	author = {Nazi, Zabir Al and Peng, Wei},
	month = sep,
	year = {2024},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ChatGPT, generative AI, healthcare, large language model, machine learning applications, medical AI, medicine, natural language generation, natural language processing},
	pages = {57},
}

@misc{xie_survey_2023,
	title = {A {Survey} for {Biomedical} {Text} {Summarization}: {From} {Pre}-trained to {Large} {Language} {Models}},
	shorttitle = {A {Survey} for {Biomedical} {Text} {Summarization}},
	url = {http://arxiv.org/abs/2304.08763},
	doi = {10.48550/arXiv.2304.08763},
	abstract = {The exponential growth of biomedical texts such as biomedical literature and electronic health records (EHRs), poses a significant challenge for clinicians and researchers to access clinical information efficiently. To tackle this challenge, biomedical text summarization (BTS) has been proposed as a solution to support clinical information retrieval and management. BTS aims at generating concise summaries that distill key information from single or multiple biomedical documents. In recent years, the rapid advancement of fundamental natural language processing (NLP) techniques, from pre-trained language models (PLMs) to large language models (LLMs), has greatly facilitated the progress of BTS. This growth has led to numerous proposed summarization methods, datasets, and evaluation metrics, raising the need for a comprehensive and up-to-date survey for BTS. In this paper, we present a systematic review of recent advancements in BTS, leveraging cutting-edge NLP techniques from PLMs to LLMs, to help understand the latest progress, challenges, and future directions. We begin by introducing the foundational concepts of BTS, PLMs and LLMs, followed by an in-depth review of available datasets, recent approaches, and evaluation metrics in BTS. We finally discuss existing challenges and promising future directions in the era of LLMs. To facilitate the research community, we line up open resources including available datasets, recent approaches, codes, evaluation metrics, and the leaderboard in a public project: https://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master. We believe that this survey will be a useful resource to researchers, allowing them to quickly track recent advancements and provide guidelines for future BTS research within the research community.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Xie, Qianqian and Luo, Zheheng and Wang, Benyou and Ananiadou, Sophia},
	month = jul,
	year = {2023},
	note = {arXiv:2304.08763 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{pawar_survey_2023,
	title = {Survey on the {Biomedical} {Text} {Summarization} {Techniques} with an {Emphasis} on {Databases}, {Techniques}, {Semantic} {Approaches}, {Classification} {Techniques}, and {Similarity} {Measures}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/15/5/4216},
	doi = {10.3390/su15054216},
	abstract = {Biomedical text summarization (BTS) is proving to be an emerging area of work and research with the need for sustainable healthcare applications such as evidence-based medicine practice (EBM) and telemedicine which help effectively support healthcare needs of the society. However, with the rapid growth in the biomedical literature and the diversities in its structure and resources, it is becoming challenging to carry out effective text summarization for better insights. The goal of this work is to conduct a comprehensive systematic literature review of significant and high-impact literary work in BTS with a deep understanding of its major artifacts such as databases, semantic similarity measures, and semantic enrichment approaches. In the systematic literature review conducted, we applied search filters to find high-impact literature in the biomedical text summarization domain from IEEE, SCOPUS, Elsevier, EBSCO, and PubMed databases. The systematic literature review (SLR) yielded 81 works; those were analyzed for qualitative study. The in-depth study of the literature shows the relevance and efficacy of the deep learning (DL) approach, context-aware feature extraction techniques, and their relevance in BTS. Biomedical question answering (BQA) system is one of the most popular applications of text summarizations for building self-sufficient healthcare systems and are pointing to future research directions. The review culminates in realization of a proposed framework for the BQA system MEDIQA with design of better heuristics for content screening, document screening, and relevance ranking. The presented framework provides an evidence-based biomedical question answering model and text summarizer that can lead to real-time evidence-based clinical support system to healthcare practitioners.},
	language = {en},
	number = {5},
	urldate = {2025-05-02},
	journal = {Sustainability},
	author = {Pawar, Dipti and Phansalkar, Shraddha and Sharma, Abhishek and Sahu, Gouri Kumar and Ang, Chun Kit and Lim, Wei Hong},
	month = jan,
	year = {2023},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {biomedical question answering, databases, semantic enrichment, text similarity, text summarization},
	pages = {4216},
}

@misc{noauthor_survey_nodate,
	title = {Survey on the {Biomedical} {Text} {Summarization} {Techniques} with an {Emphasis} on {Databases}, {Techniques}, {Semantic} {Approaches}, {Classification} {Techniques}, and {Similarity} {Measures}},
	url = {https://www.mdpi.com/2071-1050/15/5/4216},
	urldate = {2025-05-02},
}

@article{rohil_exploratory_2022,
	title = {An exploratory study of automatic text summarization in biomedical and healthcare domain},
	volume = {2},
	issn = {2772-4425},
	url = {https://www.sciencedirect.com/science/article/pii/S2772442522000223},
	doi = {10.1016/j.health.2022.100058},
	abstract = {In the last two decades, the uses of automatic text summarization have been realized in a wide range of applications in various fields cutting across a number of verticals. Amongst these, one of the most inquired is the domain of healthcare and medicine. Many of the studies have revealed that the use of automatic text summarization in the biomedical and healthcare domain helps researchers and medical professionals save their time and access more information in considerably short spans of time. This article reports some of the recent studies that enumerate the benefits and limitations of the uses of automatic text summarization in the biomedical and healthcare domain. In addition, the paper also explores certain new possible applications of automatic text summarization in the biomedical and healthcare domain. Furthermore, it discusses the trends and vision towards future opportunities for possible research in automatic text summarization in the context of medical and healthcare domain.},
	urldate = {2025-05-02},
	journal = {Healthcare Analytics},
	author = {Rohil, Mukesh Kumar and Magotra, Varun},
	month = nov,
	year = {2022},
	keywords = {Artificial Intelligence, Automatic Text Summarization, Electronic Health Records, Medical Subject Headings, Natural Language Processing, Unified Medical Language System},
	pages = {100058},
}

@article{chaves_automatic_2022,
	title = {Automatic {Text} {Summarization} of {Biomedical} {Text} {Data}: {A} {Systematic} {Review}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Automatic {Text} {Summarization} of {Biomedical} {Text} {Data}},
	url = {https://www.mdpi.com/2078-2489/13/8/393},
	doi = {10.3390/info13080393},
	abstract = {In recent years, the evolution of technology has led to an increase in text data obtained from many sources. In the biomedical domain, text information has also evidenced this accelerated growth, and automatic text summarization systems play an essential role in optimizing physicians’ time resources and identifying relevant information. In this paper, we present a systematic review in recent research of text summarization for biomedical textual data, focusing mainly on the methods employed, type of input data text, areas of application, and evaluation metrics used to assess systems. The survey was limited to the period between 1st January 2014 and 15th March 2022. The data collected was obtained from WoS, IEEE, and ACM digital libraries, while the search strategies were developed with the help of experts in NLP techniques and previous systematic reviews. The four phases of a systematic review by PRISMA methodology were conducted, and five summarization factors were determined to assess the studies included: Input, Purpose, Output, Method, and Evaluation metric. Results showed that 3.5\% of 801 studies met the inclusion criteria. Moreover, Single-document, Biomedical Literature, Generic, and Extractive summarization proved to be the most common approaches employed, while techniques based on Machine Learning were performed in 16 studies and Rouge (Recall-Oriented Understudy for Gisting Evaluation) was reported as the evaluation metric in 26 studies. This review found that in recent years, more transformer-based methodologies for summarization purposes have been implemented compared to a previous survey. Additionally, there are still some challenges in text summarization in different domains, especially in the biomedical field in terms of demand for further research.},
	language = {en},
	number = {8},
	urldate = {2025-05-02},
	journal = {Information},
	author = {Chaves, Andrea and Kesiku, Cyrille and Garcia-Zapirain, Begonya},
	month = aug,
	year = {2022},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {intrinsic evaluation, language processing, medical documents, text summarization},
	pages = {393},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic {Text} {Summarization} of {Biomedical} {Text} {Data}: {A} {Systematic} {Review}},
	url = {https://www.mdpi.com/2078-2489/13/8/393},
	urldate = {2025-05-02},
}

@article{wang_systematic_2021,
	title = {A systematic review of automatic text summarization for biomedical literature and {EHRs}},
	volume = {28},
	issn = {1527-974X},
	doi = {10.1093/jamia/ocab143},
	abstract = {OBJECTIVE: Biomedical text summarization helps biomedical information seekers avoid information overload by reducing the length of a document while preserving the contents' essence. Our systematic review investigates the most recent biomedical text summarization researches on biomedical literature and electronic health records by analyzing their techniques, areas of application, and evaluation methods. We identify gaps and propose potential directions for future research.
MATERIALS AND METHODS: This review followed the PRISMA methodology and replicated the approaches adopted by the previous systematic review published on the same topic. We searched 4 databases (PubMed, ACM Digital Library, Scopus, and Web of Science) from January 1, 2013 to April 8, 2021. Two reviewers independently screened title, abstract, and full-text for all retrieved articles. The conflicts were resolved by the third reviewer. The data extraction of the included articles was in 5 dimensions: input, purpose, output, method, and evaluation.
RESULTS: Fifty-eight out of 7235 retrieved articles met the inclusion criteria. Thirty-nine systems used single-document biomedical research literature as their input, 17 systems were explicitly designed for clinical support, 47 systems generated extractive summaries, and 53 systems adopted hybrid methods combining computational linguistics, machine learning, and statistical approaches. As for the assessment, 51 studies conducted an intrinsic evaluation using predefined metrics.
DISCUSSION AND CONCLUSION: This study found that current biomedical text summarization systems have achieved good performance using hybrid methods. Studies on electronic health records summarization have been increasing compared to a previous survey. However, the majority of the works still focus on summarizing literature.},
	language = {eng},
	number = {10},
	journal = {Journal of the American Medical Informatics Association: JAMIA},
	author = {Wang, Mengqian and Wang, Manhua and Yu, Fei and Yang, Yue and Walker, Jennifer and Mostafa, Javed},
	month = sep,
	year = {2021},
	pmid = {34338801},
	pmcid = {PMC8449627},
	keywords = {Biomedical Research, Electronic Health Records, Machine Learning, Publications, automatic text summarization, biomedical and health sciences literature, computational linguistics, electronic health records, machine learning},
	pages = {2287--2297},
}

@article{mishra_text_2014,
	title = {Text summarization in the biomedical domain: a systematic review of recent research},
	volume = {52},
	issn = {1532-0480},
	shorttitle = {Text summarization in the biomedical domain},
	doi = {10.1016/j.jbi.2014.06.009},
	abstract = {OBJECTIVE: The amount of information for clinicians and clinical researchers is growing exponentially. Text summarization reduces information as an attempt to enable users to find and understand relevant source texts more quickly and effortlessly. In recent years, substantial research has been conducted to develop and evaluate various summarization techniques in the biomedical domain. The goal of this study was to systematically review recent published research on summarization of textual documents in the biomedical domain.
MATERIALS AND METHODS: MEDLINE (2000 to October 2013), IEEE Digital Library, and the ACM digital library were searched. Investigators independently screened and abstracted studies that examined text summarization techniques in the biomedical domain. Information is derived from selected articles on five dimensions: input, purpose, output, method and evaluation.
RESULTS: Of 10,786 studies retrieved, 34 (0.3\%) met the inclusion criteria. Natural language processing (17; 50\%) and a hybrid technique comprising of statistical, Natural language processing and machine learning (15; 44\%) were the most common summarization approaches. Most studies (28; 82\%) conducted an intrinsic evaluation.
DISCUSSION: This is the first systematic review of text summarization in the biomedical domain. The study identified research gaps and provides recommendations for guiding future research on biomedical text summarization.
CONCLUSION: Recent research has focused on a hybrid technique comprising statistical, language processing and machine learning techniques. Further research is needed on the application and evaluation of text summarization in real research or patient care settings.},
	language = {eng},
	journal = {Journal of Biomedical Informatics},
	author = {Mishra, Rashmi and Bian, Jiantao and Fiszman, Marcelo and Weir, Charlene R. and Jonnalagadda, Siddhartha and Mostafa, Javed and Del Fiol, Guilherme},
	month = dec,
	year = {2014},
	pmid = {25016293},
	pmcid = {PMC4261035},
	keywords = {Abstracting and Indexing, Artificial Intelligence, Biomedical domain, Humans, Information Storage and Retrieval, Intrinsic evaluation, Language processing, MEDLINE, Machine learning, Natural Language Processing, Text summarization},
	pages = {457--467},
}

@article{cai_covidsum_2022,
	title = {{COVIDSum}: {A} linguistically enriched {SciBERT}-based summarization model for {COVID}-19 scientific papers},
	volume = {127},
	issn = {1532-0480},
	shorttitle = {{COVIDSum}},
	doi = {10.1016/j.jbi.2022.103999},
	abstract = {The coronavirus disease (COVID-19) has claimed the lives of over 350,000 people and infected more than 173 million people worldwide, it triggers researchers from diverse fields are accelerating their research to help diagnostics, therapies, and vaccines. Researchers also publish their recent research progress through scientific papers. However, manually writing the abstract of a paper is time-consuming, and it increases the writing burden of the researchers. Abstractive summarization technique which automatically provides researchers reliable draft abstracts, can alleviate this problem. In this work, we propose a linguistically enriched SciBERT-based summarization model for COVID-19 scientific papers, named COVIDSum. Specifically, we first extract salient sentences from source papers and construct word co-occurrence graphs. Then, we adopt a SciBERT-based sequence encoder and a Graph Attention Networks-based graph encoder to encode sentences and word co-occurrence graphs, respectively. Finally, we fuse the above two encodings and generate an abstractive summary of each scientific paper. When evaluated on the publicly available COVID-19 open research dataset, the performance of our proposed model achieves significant improvement compared with other document summarization models.},
	language = {eng},
	journal = {Journal of Biomedical Informatics},
	author = {Cai, Xiaoyan and Liu, Sen and Yang, Libin and Lu, Yan and Zhao, Jintao and Shen, Dinggang and Liu, Tianming},
	month = mar,
	year = {2022},
	pmid = {35104642},
	pmcid = {PMC8800935},
	keywords = {Abstractive summarization, COVID-19, COVID-19 scientific papers, Humans, Language, Linguistically enriched pre-trained language model, Publishing, SARS-CoV-2, SciBERT},
	pages = {103999},
}

@misc{pal_medmcqa_2022,
	title = {{MedMCQA} : {A} {Large}-scale {Multi}-{Subject} {Multi}-{Choice} {Dataset} for {Medical} domain {Question} {Answering}},
	shorttitle = {{MedMCQA}},
	url = {http://arxiv.org/abs/2203.14371},
	doi = {10.48550/arXiv.2203.14371},
	abstract = {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS {\textbackslash}\& NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects {\textbackslash}\& topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.14371 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{muller_covid-twitter-bert_2020,
	title = {{COVID}-{Twitter}-{BERT}: {A} {Natural} {Language} {Processing} {Model} to {Analyse} {COVID}-19 {Content} on {Twitter}},
	shorttitle = {{COVID}-{Twitter}-{BERT}},
	url = {http://arxiv.org/abs/2005.07503},
	doi = {10.48550/arXiv.2005.07503},
	abstract = {In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10-30\% marginal improvement compared to its base model, BERT-Large, on five different classification datasets. The largest improvements are on the target domain. Pretrained transformer models, such as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural language processing tasks, including classification, question-answering and chatbots. CT-BERT is optimised to be used on COVID-19 content, in particular social media posts from Twitter.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Müller, Martin and Salathé, Marcel and Kummervold, Per E.},
	month = may,
	year = {2020},
	note = {arXiv:2005.07503 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{taylor_galactica_2022,
	title = {Galactica: {A} {Large} {Language} {Model} for {Science}},
	shorttitle = {Galactica},
	url = {http://arxiv.org/abs/2211.09085},
	doi = {10.48550/arXiv.2211.09085},
	abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09085 [cs]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
}

@inproceedings{beltagy_scibert_2019,
	address = {Hong Kong, China},
	title = {{SciBERT}: {A} {Pretrained} {Language} {Model} for {Scientific} {Text}},
	shorttitle = {{SciBERT}},
	url = {https://aclanthology.org/D19-1371/},
	doi = {10.18653/v1/D19-1371},
	abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	urldate = {2025-05-02},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {3615--3620},
}

@misc{li_clinical-longformer_2022,
	title = {Clinical-{Longformer} and {Clinical}-{BigBird}: {Transformers} for long clinical sequences},
	shorttitle = {Clinical-{Longformer} and {Clinical}-{BigBird}},
	url = {http://arxiv.org/abs/2201.11838},
	doi = {10.48550/arXiv.2201.11838},
	abstract = {Transformers-based models, such as BERT, have dramatically improved the performance for various natural language processing tasks. The clinical knowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art results when performed on clinical named entity recognition and natural language inference tasks. One of the core limitations of these transformers is the substantial memory consumption due to their full self-attention mechanism. To overcome this, long sequence transformer models, e.g. Longformer and BigBird, were proposed with the idea of sparse attention mechanism to reduce the memory usage from quadratic to the sequence length to a linear scale. These models extended the maximum input sequence length from 512 to 4096, which enhanced the ability of modeling long-term dependency and consequently achieved optimal results in a variety of tasks. Inspired by the success of these long sequence transformer models, we introduce two domain enriched language models, namely Clinical-Longformer and Clinical-BigBird, which are pre-trained from large-scale clinical corpora. We evaluate both pre-trained models using 10 baseline tasks including named entity recognition, question answering, and document classification tasks. The results demonstrate that Clinical-Longformer and Clinical-BigBird consistently and significantly outperform ClinicalBERT as well as other short-sequence transformers in all downstream tasks. We have made our source code available at [https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models available for public download at: [https://huggingface.co/yikuan8/Clinical-Longformer].},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Li, Yikuan and Wehbe, Ramsey M. and Ahmad, Faraz S. and Wang, Hanyin and Luo, Yuan},
	month = apr,
	year = {2022},
	note = {arXiv:2201.11838 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{shin_biomegatron_2020,
	title = {{BioMegatron}: {Larger} {Biomedical} {Domain} {Language} {Model}},
	shorttitle = {{BioMegatron}},
	url = {http://arxiv.org/abs/2010.06060},
	doi = {10.48550/arXiv.2010.06060},
	abstract = {There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of named entity recognition, relation extraction, and question answering. Model checkpoints and code are available at [https://ngc.nvidia.com] and [https://github.com/NVIDIA/NeMo].},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Shin, Hoo-Chang and Zhang, Yang and Bakhturina, Evelina and Puri, Raul and Patwary, Mostofa and Shoeybi, Mohammad and Mani, Raghav},
	month = oct,
	year = {2020},
	note = {arXiv:2010.06060 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yasunaga_scisummnet_2019,
	title = {{ScisummNet}: {A} {Large} {Annotated} {Corpus} and {Content}-{Impact} {Models} for {Scientific} {Paper} {Summarization} with {Citation} {Networks}},
	shorttitle = {{ScisummNet}},
	url = {http://arxiv.org/abs/1909.01716},
	doi = {10.48550/arXiv.1909.01716},
	abstract = {Scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article's impacts on research community. This paper provides novel solutions to these two challenges. We 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors' original highlights (abstract) and the article's actual impacts on the community (citations), to create comprehensive, hybrid summaries. We conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. Our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Yasunaga, Michihiro and Kasai, Jungo and Zhang, Rui and Fabbri, Alexander R. and Li, Irene and Friedman, Dan and Radev, Dragomir R.},
	month = sep,
	year = {2019},
	note = {arXiv:1909.01716 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@inproceedings{jin_pubmedqa_2019,
	address = {Hong Kong, China},
	title = {{PubMedQA}: {A} {Dataset} for {Biomedical} {Research} {Question} {Answering}},
	shorttitle = {{PubMedQA}},
	url = {https://www.aclweb.org/anthology/D19-1259},
	doi = {10.18653/v1/D19-1259},
	abstract = {We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial ﬁbrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artiﬁcially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the ﬁrst QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase ﬁne-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1\% accuracy, compared to single human performance of 78.0\% accuracy and majority-baseline of 55.2\% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
	year = {2019},
	pages = {2567--2577},
}

@inproceedings{wang_wonder_2024,
	address = {Mexico City, Mexico},
	title = {Wonder at {Chemotimelines} 2024: {MedTimeline}: {An} {End}-to-{End} {NLP} {System} for {Timeline} {Extraction} from {Clinical} {Narratives}},
	shorttitle = {Wonder at {Chemotimelines} 2024},
	url = {https://aclanthology.org/2024.clinicalnlp-1.48/},
	doi = {10.18653/v1/2024.clinicalnlp-1.48},
	abstract = {Extracting timeline information from clinical narratives is critical for cancer research and practice using electronic health records (EHRs). In this study, we apply MedTimeline, our end-to-end hybrid NLP system combining large language model, deep learning with knowledge engineering, to the ChemoTimeLine challenge subtasks. Our experiment results in 0.83, 0.90, 0.84, and 0.53, 0.63, 0.39, respectively, for subtask1 and subtask2 in breast, melanoma and ovarian cancer.},
	urldate = {2025-05-02},
	booktitle = {Proceedings of the 6th {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Liwei and Lu, Qiuhao and Li, Rui and Fu, Sunyang and Liu, Hongfang},
	editor = {Naumann, Tristan and Ben Abacha, Asma and Bethard, Steven and Roberts, Kirk and Bitterman, Danielle},
	month = jun,
	year = {2024},
	pages = {483--487},
}

@misc{liu_self-alignment_2021,
	title = {Self-{Alignment} {Pretraining} for {Biomedical} {Entity} {Representations}},
	url = {http://arxiv.org/abs/2010.11784},
	doi = {10.48550/arXiv.2010.11784},
	abstract = {Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Liu, Fangyu and Shareghi, Ehsan and Meng, Zaiqiao and Basaldella, Marco and Collier, Nigel},
	month = apr,
	year = {2021},
	note = {arXiv:2010.11784 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	doi = {10.48550/arXiv.2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv:2004.05150 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yan_zzxslpradbert_2025,
	title = {zzxslp/{RadBERT}},
	url = {https://github.com/zzxslp/RadBERT},
	abstract = {Code and models for Paper RadBERT: Adapting transformer-based language models to radiology},
	urldate = {2025-05-02},
	author = {Yan, An},
	month = feb,
	year = {2025},
	note = {original-date: 2022-08-18T21:13:40Z},
	keywords = {bert-models, healthcare-application, language-model, nlp},
}

@misc{yang_gatortron_2022,
	title = {{GatorTron}: {A} {Large} {Language} {Model} for {Clinical} {Natural} {Language} {Processing}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{GatorTron}},
	url = {https://www.medrxiv.org/content/10.1101/2022.02.27.22271257v2},
	doi = {10.1101/2022.02.27.22271257},
	abstract = {Objective To develop a large pretrained clinical language model from scratch using transformer architecture; systematically examine how transformer models of different sizes could help 5 clinical natural language processing (NLP) tasks at different linguistic levels.
Methods We created a large corpus with {\textgreater}90 billion words from clinical narratives ({\textgreater}82 billion words), scientific literature (6 billion words), and general English text (2.5 billion words). We developed GatorTron models from scratch using the BERT architecture of different sizes including 345 million, 3.9 billion, and 8.9 billion parameters, compared GatorTron with three existing transformer models in the clinical and biomedical domain on 5 different clinical NLP tasks including clinical concept extraction, relation extraction, semantic textual similarity, natural language inference, and medical question answering, to examine how large transformer models could help clinical NLP at different linguistic levels.
Results and Conclusion GatorTron scaled up transformer-based clinical language models to a size of 8.9 billion parameters and achieved state-of-the-art performance on 5 clinical NLP tasks of different linguistic levels targeting various healthcare information documented in unstructured electronic health records (EHRs). The proposed GatorTron models performed remarkably better in much complex clinical NLP tasks such as natural language inference (9.6\% and 7.5\% improvements) and question answering (9.5\% and 7.77\% improvements) compared with existing smaller clinical transformer models (i.e., BioBERT and ClinicalBERT), demonstrating the potential of large transformer-based clinical models for advanced medical artificial intelligent (AI) applications such as question answering.},
	language = {en},
	urldate = {2025-05-02},
	publisher = {medRxiv},
	author = {Yang, Xi and PourNejatian, Nima and Shin, Hoo Chang and Smith, Kaleb E. and Parisien, Christopher and Compas, Colin and Martin, Cheryl and Flores, Mona G. and Zhang, Ying and Magoc, Tanja and Harle, Christopher A. and Lipori, Gloria and Mitchell, Duane A. and Hogan, William R. and Shenkman, Elizabeth A. and Bian, Jiang and Wu, Yonghui},
	month = mar,
	year = {2022},
	note = {Pages: 2022.02.27.22271257},
}

@article{luo_biogpt_2022,
	title = {{BioGPT}: generative pre-trained transformer for biomedical text generation and mining},
	volume = {23},
	issn = {1477-4054},
	shorttitle = {{BioGPT}},
	url = {https://doi.org/10.1093/bib/bbac409},
	doi = {10.1093/bib/bbac409},
	abstract = {Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.},
	number = {6},
	urldate = {2025-05-02},
	journal = {Briefings in Bioinformatics},
	author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
	month = nov,
	year = {2022},
	pages = {bbac409},
}

@misc{noauthor_biogpt_nodate,
	title = {{BioGPT}: generative pre-trained transformer for biomedical text generation and mining {\textbar} {Briefings} in {Bioinformatics} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/bib/article/23/6/bbac409/6713511},
	urldate = {2025-05-02},
}

@article{noauthor_biogpt_2024,
	title = {{BioGPT}: generative pre-trained transformer for biomedical text generation and mining {\textbar} {Request} {PDF}},
	shorttitle = {{BioGPT}},
	url = {https://www.researchgate.net/publication/363889118_BioGPT_generative_pre-trained_transformer_for_biomedical_text_generation_and_mining},
	doi = {10.1093/bib/bbac409},
	abstract = {Request PDF {\textbar} BioGPT: generative pre-trained transformer for biomedical text generation and mining {\textbar} Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-05-02},
	journal = {ResearchGate},
	month = dec,
	year = {2024},
}

@misc{alsentzer_publicly_2019,
	title = {Publicly {Available} {Clinical} {BERT} {Embeddings}},
	url = {http://arxiv.org/abs/1904.03323},
	doi = {10.48550/arXiv.1904.03323},
	abstract = {Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Alsentzer, Emily and Murphy, John R. and Boag, Willie and Weng, Wei-Hung and Jin, Di and Naumann, Tristan and McDermott, Matthew B. A.},
	month = jun,
	year = {2019},
	note = {arXiv:1904.03323 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{peng_transfer_2019,
	address = {Florence, Italy},
	title = {Transfer {Learning} in {Biomedical} {Natural} {Language} {Processing}: {An} {Evaluation} of {BERT} and {ELMo} on {Ten} {Benchmarking} {Datasets}},
	shorttitle = {Transfer {Learning} in {Biomedical} {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/W19-5006},
	doi = {10.18653/v1/W19-5006},
	abstract = {Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE\_Benchmark.},
	language = {en},
	urldate = {2025-05-02},
	booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Peng, Yifan and Yan, Shankai and Lu, Zhiyong},
	year = {2019},
	pages = {58--65},
}

@article{lee_biobert_2020,
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
	volume = {36},
	issn = {1367-4811},
	shorttitle = {{BioBERT}},
	doi = {10.1093/bioinformatics/btz682},
	abstract = {MOTIVATION: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.
RESULTS: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.
AVAILABILITY AND IMPLEMENTATION: We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
	language = {eng},
	number = {4},
	journal = {Bioinformatics (Oxford, England)},
	author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	month = feb,
	year = {2020},
	pmid = {31501885},
	pmcid = {PMC7703786},
	keywords = {Data Mining, Language, Natural Language Processing, Software},
	pages = {1234--1240},
}

@misc{noauthor_biobert_nodate,
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining {\textbar} {Bioinformatics} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/bioinformatics/article/36/4/1234/5566506},
	urldate = {2025-05-02},
}

@article{gu_domain-specific_2022,
	title = {Domain-{Specific} {Language} {Model} {Pretraining} for {Biomedical} {Natural} {Language} {Processing}},
	volume = {3},
	issn = {2691-1957, 2637-8051},
	url = {http://arxiv.org/abs/2007.15779},
	doi = {10.1145/3458754},
	abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
	number = {1},
	urldate = {2025-05-02},
	journal = {ACM Transactions on Computing for Healthcare},
	author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	month = jan,
	year = {2022},
	note = {arXiv:2007.15779 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {1--23},
}

@misc{noauthor_large_nodate,
	title = {Large language models ({LLMs}): survey, technical frameworks, and future challenges {\textbar} {Artificial} {Intelligence} {Review}},
	url = {https://link.springer.com/article/10.1007/s10462-024-10888-y},
	urldate = {2025-05-02},
}

@misc{naveed_comprehensive_2024,
	title = {A {Comprehensive} {Overview} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.06435},
	doi = {10.48550/arXiv.2307.06435},
	abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
	month = oct,
	year = {2024},
	note = {arXiv:2307.06435 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{antar_diabetes_2023,
	title = {Diabetes mellitus: {Classification}, mediators, and complications; {A} gate to identify potential targets for the development of new effective treatments},
	volume = {168},
	issn = {07533322},
	shorttitle = {Diabetes mellitus},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0753332223015329},
	doi = {10.1016/j.biopha.2023.115734},
	language = {en},
	urldate = {2025-05-02},
	journal = {Biomedicine \& Pharmacotherapy},
	author = {Antar, Samar A. and Ashour, Nada A. and Sharaky, Marwa and Khattab, Muhammad and Ashour, Naira A. and Zaid, Roaa T. and Roh, Eun Joo and Elkamhawy, Ahmed and Al-Karmalawy, Ahmed A.},
	month = dec,
	year = {2023},
	pages = {115734},
}

@article{antar_diabetes_2023-1,
	title = {Diabetes mellitus: {Classification}, mediators, and complications; {A} gate to identify potential targets for the development of new effective treatments},
	volume = {168},
	issn = {0753-3322},
	shorttitle = {Diabetes mellitus},
	url = {https://www.sciencedirect.com/science/article/pii/S0753332223015329},
	doi = {10.1016/j.biopha.2023.115734},
	abstract = {Nowadays, diabetes mellitus has emerged as a significant global public health concern with a remarkable increase in its prevalence. This review article focuses on the definition of diabetes mellitus and its classification into different types, including type 1 diabetes (idiopathic and fulminant), type 2 diabetes, gestational diabetes, hybrid forms, slowly evolving immune-mediated diabetes, ketosis-prone type 2 diabetes, and other special types. Diagnostic criteria for diabetes mellitus are also discussed. The role of inflammation in both type 1 and type 2 diabetes is explored, along with the mediators and potential anti-inflammatory treatments. Furthermore, the involvement of various organs in diabetes mellitus is highlighted, such as the role of adipose tissue and obesity, gut microbiota, and pancreatic β-cells. The manifestation of pancreatic Langerhans β-cell islet inflammation, oxidative stress, and impaired insulin production and secretion are addressed. Additionally, the impact of diabetes mellitus on liver cirrhosis, acute kidney injury, immune system complications, and other diabetic complications like retinopathy and neuropathy is examined. Therefore, further research is required to enhance diagnosis, prevent chronic complications, and identify potential therapeutic targets for the management of diabetes mellitus and its associated dysfunctions.},
	urldate = {2025-05-02},
	journal = {Biomedicine \& Pharmacotherapy},
	author = {Antar, Samar A. and Ashour, Nada A. and Sharaky, Marwa and Khattab, Muhammad and Ashour, Naira A. and Zaid, Roaa T. and Roh, Eun Joo and Elkamhawy, Ahmed and Al-Karmalawy, Ahmed A.},
	month = dec,
	year = {2023},
	keywords = {Complications, Diabetes, Diagnosis, Etiology, Genetics, Inflammation, Treatment},
	pages = {115734},
}
