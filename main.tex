\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb, bm, mathtools}
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage{caption}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[a4paper, margin=1in]{geometry}
%\usepackage{authblk}
\usepackage[colorlinks=true, citecolor=blue, linkcolor=red, urlcolor=blue]{hyperref}
\usepackage{newunicodechar}
\newunicodechar{⋆}{$\star$}

\usepackage{xcolor}  % For color
%\usepackage{cite}  % For IEEE style citations
%\bibliographystyle{ieeetr}
%\let\oldcite\parencite  % Save the original \parencite command
\renewcommand{\parencite}[1]{\textcolor{blue}{\oldcite{#1}}}  % Redefine \parencite to include color

% Biblatex with APA
\usepackage{xcolor}  % For color
\usepackage[backend=biber, style=apa, sorting=nyt]{biblatex}
\DeclareLanguageMapping{english}{english-apa}
\addbibresource{references.bib}
\addbibresource{Rabuil.bib}
% ---- Make all citations blue automatically ----
\AtBeginDocument{%
  \DeclareCiteCommand{\parencite}[\mkbibparens]
    {\textcolor{blue}{\usebibmacro{cite:init}\usebibmacro{prenote}}}
    {\textcolor{blue}{\usebibmacro{citeindex}\usebibmacro{cite}}}
    {\multicitedelim}
    {\usebibmacro{postnote}}%
}
% Force all citations to be parenthetical
%\let\parencite\parencite

\captionsetup[table]{labelformat=simple, labelsep=colon, textfont=normalfont, labelfont=bf}
\captionsetup[figure]{labelformat=simple, labelsep=colon, textfont=normalfont, labelfont=bf}

\title{\textbf{PEFT: Parameter-Efficient Clinical Note Summarization: Evaluating Fine-Tuning Strategies Across Transformers and Large Language Models}}



%\author[1]{Aleka Melese Ayalew*}
%\author[1] {Md Rabiul Hasan}
%\author[1]{Tapio Seppänen}
%\affil[1]{Center for Machine Vision and Signal Processing, University of Oulu, 90014, %Finland\\
%\textit{*Corresponding Author Email: \href{mailto:aleka.melese@oulu.fi}{Aleka.Melese@oulu.fi}}}
\date{} % Removes the default date
\begin{document}
\maketitle
%\textit{\textbf{Institutional Email Addresses:} Aleka.Ayalew@oulu.fi (Aleka Melese Ayalew %),MdRabiul.Hasan@oulu.fi (Md Rabiul Hasan), Tapio.Seppanen@oulu.fi (Tapio Seppänen), and 
% Mourad.Oussalah@oulu.fi (Mourad
%Oussalah)}
%\vspace{1.5cm}
\begin {abstract}

Analyzing and summarizing critical information from electronic health records places a significant burden on clinicians. While transformer models and Large Language Models (LLMs) are effective for this task, the computational cost and strict performance requirements in healthcare present major challenges. This work addresses the challenge of computational cost and strict performance requirements in healthcare by evaluating parameter-efficient fine-tuning (PEFT) techniques in the context of abstractive summarization.
\\
\\
We conducted a comprehensive evaluation on the MIMIC-IV-Ext-BHC v1.2.0 dataset, containing 270,033 clinical notes, comparing four transformer models (T5, PEGASUS-XSUM, BART, and FLAN-T5) and two LLMs (Mistral-7B and LLaMA-3-8B) using zero-shot, full fine-tuning, and LoRA fine-tuning. Additionally, we integrated LexRank, clinical NLP functionalities provided by cTAKES, and domain-specific semantic similarity techniques into graph-based extractive summarization. \textcolor{red}{Our experiments revealed that LoRA achieved higher scores than full fine-tuning on key metrics while reducing the computational burden.} Specifically, our LLaMA-3B model, fine-tuned with LoRA, achieved a ROUGE-1 score of 0.7022, a ROUGE-2 of 0.5312, a ROUGE-L of 0.6718, a METEOR of 0.6787, a BERTScore precision of 0.9124, a BERTScore recall of 0.9238, and a BERTScore F1 of 0.9180 on the test set. \textcolor{red}{This performance corresponds to a 2.6$\times$ higher ROUGE-1 score compared to the leading extractive baseline, though direct comparison is limited by fundamental differences between extractive and abstractive summarization approaches, and was accomplished with approximately 99\% fewer trainable parameters compared to full fine-tuning.} Our research demonstrates that LoRA is a highly effective and scalable solution for deploying sophisticated language models in resource-constrained medical environments, providing a robust framework to improve healthcare professionals' access to information and speed up clinical decision-making.\\


\textbf{Keywords:} Clinical Note Summarization, LLMs, LoRA, Parameter-Efficient Fine-tuning, Transformer Models, cTAKES

\end{abstract}

%\tableofcontents 
%\newpage

\section{Introduction}
The use of electronic health records (EHRs) has rapidly evolved over the past decade, transforming how health data is managed globally. In high-income countries, EHR adoption surpasses 95\% in most hospitals, while lower-resource regions are actively improving their digital health infrastructure \parencite{hijazi_exploring_2025}. Each patient interaction generates a variety of free-text documents, including diagnostic notes, progress reports, radiological impressions, consultation findings, operation records, pathology results, and discharge summaries. This widespread implementation of EHRs has led to an increased clinical documentation burden, with physicians often spending up to two hours on documentation for every hour of patient care, and nurses dedicating as much as 60\% of their time to record-keeping. This situation contributes to a high perceived workload and clinician burnout \parencite{kobayashi_factors_2025}. Clinicians and healthcare administrators frequently find themselves overwhelmed by the sheer volume of data, which complicates the extraction of vital insights, the integration of recent medical advancements, and the thorough evaluation of patient histories. These challenges can result in delayed diagnoses, ineffective treatment options, and slow communication of medical information, ultimately impacting patient outcomes and professional satisfaction.\\

Automatic summarization offers a practical solution to the challenges posed by extensive clinical documentation. Manual summarization is prone to bias, errors, and high costs \parencite{supriyono_survey_2024}. By efficiently extracting essential information from lengthy records, automated summarization can support timely, evidence-based decision-making, reduce the workload of healthcare professionals, and facilitate research by transforming complex data into clear and actionable insights \parencite{maleki_varnosfaderani_role_2024}.\\


Recent advancements in transformer-based language models and large language models have significantly enhanced their ability to summarize text and perform a variety of natural language processing tasks. Traditional transformer models such as BART (Bidirectional and Auto-Regressive Transformers), T5 (Text-to-Text Transfer Transformer), Pegasus-Xsum (Pre-training with Extracted Gap-Sentences for Abstractive Summarization), and Flan-T5 (Fine-tuned Language Net-T5) have demonstrated strong performance in general-domain summarization tasks. Furthermore, large language models like Mistral-7B and LLaMA-3-8B have improved our capacity to understand and reason through complex medical scenarios, offering innovative approaches to interpreting clinical content.\\

However, the medical field has its own set of problems that make it different from other text processing applications. Clinical summarization techniques generally fall into two categories. \textit{Extractive summarization} selects the most relevant sentences or passages from source texts without significantly altering the text. This preserves the original terminology and evidential basis, but it can introduce readability and flow issues.  For instance, \parencite{ando_exploring_2022} demonstrated that extractive methods often suffer from coherence problems when concatenated sentences fail to form a natural narrative, limiting their usefulness in complex clinical documentation. By contrast, \textit{abstractive summarization} seeks to generate new sentences that capture the essential meaning of the original text. While this allows greater flexibility, it highlights that abstractive methods pose interpretability challenges due to their reliance on opaque neural architectures \parencite{shakil_abstractive_2024}. At the same time, \parencite{almohaimeed_abstractive_2025} emphasized their clinical utility, showing that abstractive approaches can synthesize information from multiple sources and provide patient-friendly explanations. Taken together, these perspectives suggest that while extractive methods preserve accuracy and traceability, abstractive methods offer greater adaptability and clinical value, albeit at the cost of transparency. Clinical note (text) summarization presents several domain-specific challenges that hinder the straightforward use of general-purpose language models. Pre-trained language models (PLMs), which are trained in general corpora, often struggle to handle the highly specialized language and structure of medical text \parencite{luo_pre-trained_2024}.\\


By connecting the burden of documentation to the complexity of clinical areas, our study is designed to evaluate how extractive and abstractive summarization, combined with parameter-efficient fine-tuning techniques, can effectively address these challenges. Some of these challenges are described as follows.  \textbf{(i)} Medical texts employ a significant amount of technical language, abbreviations, and specialized terms that may not be well-represented in general pre-training corpora, such as proprietary drug names and diagnostic codes like ICD-10 \parencite{luo_pre-trained_2024}. Clinical notes often have medication names, diagnostic codes, anatomical terms, and procedure descriptions that are hard to understand without special training \parencite{yang_tcm-gpt_2024}. \textbf{(ii)} Clinical documents contain dense medical information with complex relationships between symptoms, diagnoses, treatments, and outcomes \parencite{he_pretraining-based_2025}. \textbf{(iii)} Clinical applications need to be very accurate, unlike general text summarization, because they could affect patient safety and care quality \parencite{van_veen_clinical_2023}. Medical summaries that are mistaken and leave things out could cause a wrong diagnosis, bad treatment choices, or worse outcomes for the patient \parencite{rodziewicz_medical_2025}. \textbf{(iv)} Clinical applications follow strict privacy laws like the Health Insurance Portability and Accountability Act (HIPAA), which makes it hard to obtain vast clinical datasets for training and testing models \parencite{khalid_privacy-preserving_2023} \parencite{yadav_data_2023}. \textbf{(v)} Finally, limited computational resources pose a real challenge for many healthcare organizations. Community hospitals often lack the resources needed for full fine-tuning of transformer models with hundreds of millions of parameters \parencite{nerella_transformers_2024}. Together, these challenges motivate our study, which systematically evaluates both extractive and abstractive methods, incorporating parameter-efficient approaches to deliver accurate and computationally feasible summarization solutions for clinical contexts.\\

This study aims to systematically evaluate parameter-efficient fine-tuning (PEFT) approaches across both traditional transformers and large language models in the clinical summarization domain. By doing so, it addresses the lack of comparative, resource-conscious, and clinically relevant evaluations identified in prior research.  Some of the issues highlighted in the prior studies include: \textbf{(i)} A majority of investigations examine only a small number of models or fine-tuning approaches. This limitation restricts the ability to make well-informed decisions regarding the most suitable model for practical application based on the available data \parencite{alves_benchmarking_2025}. \textbf{(ii)} Insufficient Exploration of Parameter-Efficient Methods: There is a lack of rigorous examination of parameter-efficient techniques, such as LoRA or adapters, in the clinical setting. Research often defaults to full fine-tuning methods that require substantial computational resources \parencite{liao_parameter-efficient_2023}, but their systematic evaluation across both traditional transformers and large language models in clinical contexts remains limited. \textbf{(iii)} There is no previous work that used PEFT techniques on transformer models. Therefore, from this work, we verified that transformer models with PEFT yielded promising results, which is beneficial for further research. \textbf{(iv)} When models are only tested on a small number of datasets and metrics, like using only one automatic evaluation metric, the results are not valid outside of the study and can't be reliably generalized. \textbf{(v)} Previous research often evaluates only a limited selection of models, ranging from specialized transformers to general-purpose large language models, which does not provide comprehensive clinical insights  \parencite{van_yperen_lats_2025}. 

\subsection{The Promise of Parameter-Efficient Fine-Tuning}
Natural Language Processing (NLP) has recently seen a huge shift due to LLMs. Among their many state-of-the-art accomplishments are text generation, language translation, question answering, and code combination. Performance improvements have been noticeable after scaling these models, which can include billions or even trillions of parameters. Nevertheless, it is increasingly impractical for several uses due to numerous issues with memory restrictions, computing efficiency, and flexibility brought about by this rapid growth in size. Traditional fine-tuning techniques involve adjusting the entire network or a subset of it to transfer knowledge to downstream tasks \parencite{pratap_fine_2025}. However, this procedure frequently necessitates duplicating and updating the weights of the model for every task, which ends up requiring a significant amount of computing and memory resources. Furthermore, the use of such methods can result in catastrophic forgetting, which is a situation in which the model has lost previously learned knowledge when it is fine-tuned for new tasks. Large models that have been pre-trained have a difficult time maintaining their capacity to forecast jobs that are not distributed, even though they have a solid starting point. In addition to this, the process is made even more complicated by the possibility of overfitting target datasets \parencite{tu_overview_2024}. \\

Using a carefully planned multi-pronged experimental method, our research directly addresses the problems with traditional fine-tuning and domain-specific clinical note summarization. First, we look at four popular transformer architectures like BART, T5, PEGASUS-XSUM, FLAN-T5, Mistral-7B, and LLaMA-3-8B across three fine-tuning regimes: zero-shot, full parameter fine-tuning, and parameter-efficient LoRA adaptation. This design yields a model-strategy evaluation matrix. This results in a matrix of 20 experimental conditions, \textcolor{red}{enabling cross-model comparisons across different training strategies.} Second, we place particular emphasis on rigorous benchmarking of parameter-efficient methods by quantifying LoRA’s trainable parameter count, memory footprint, and wall-time relative to accuracy, thereby addressing the prior neglect of efficient approaches in the literature. Our evaluation protocol is multi-dimensional: in addition to standard ROUGE-1/2/L scores, we employ METEOR and BERTScore (F1, Recall, and Precision) to comprehensively assess lexical overlap, semantic fidelity, and paraphrastic quality, while stratified bootstrap confidence intervals furnish statistical robustness. Therefore, this study addresses those gaps in the prior research by conducting a systematic, multi-model comparison of zero-shot, full fine-tuning, and LoRA strategies on a dedicated clinical dataset. This study fills in the gaps by doing a full evaluation of different ways to fine-tune clinical text summaries. This study makes many important contributions to the fields of medical informatics and natural language processing: 
\\

\textbf{Methodological:} 
\begin{itemize}
    \item We conduct a systematic and exhaustive comparison of zero-shot, full fine-tuning, and LoRA strategies across a diverse set of state-of-the-art transformer models and large language models with a ``thinker-based user interface'' for abstractive medical text summarization.
    \item We propose and compare the clinical text summarization effectiveness of graph-based extractive summarization and traditional transformers (BART, T5, PEGASUS-XSUM, FLAN-T5) to that of modern large language models (Mistral-7B, LLaMA-3-8B) using different training methods with prompt engineering. This includes an initial assessment of graph-based extractive summarization as a baseline, offering a holistic perspective.
\end{itemize}
\textbf{Empirical:} 
\begin{itemize}
    \item \textcolor{red}{The proposed method achieves higher evaluation scores in abstractive medical text summaries, particularly with LLMs. LoRA achieves comparable or higher scores than complete fine-tuning while using fewer computing resources, facilitating deployment in clinical contexts.}
    \item We validate the effectiveness and robustness of our approach. At the same time, we provide detailed model-specific analysis, identifying which transformer architectures and LLMs are particularly well-suited for efficient adaptation using LoRA in the medical domain.
\end{itemize}
\textbf{Practical:} 
\begin{itemize}
    \item \textcolor{red}{The proposed fine-tuned models achieved higher scores than previously reported approaches on abstractive clinical note summarization, with consistent results on both validation (ROUGE-1: 0.6970, ROUGE-L: 0.6659, BERTScore F1: 0.9163) and test sets (ROUGE-1: 0.7022, ROUGE-L: 0.6718, BERTScore F1: 0.9180), while reducing computational resources.}
\end{itemize}



The rest of the paper is organized as follows: Section 2 presents some related previous works. It also defines summarization, fine-tuning techniques, and the role of LLMs in healthcare. The methods, dataset, data preprocessing, and prompt strategies are defined in Section 3. Section 4 defined evaluation techniques. Implementation details, including model configuration, experimental design, and fine-tuning strategies, are described in Section 5. Experimental results for all experiments and the GUI are defined in Section 6. The discussion, limitations, and future research directions are described in Section 7. Finally, we conclude this study in Section 8. 

\section{Related Work}
\subsection{Extractive Summarization}

Extractive summarization is commonly utilized on MIMIC-III and MIMIC-IV clinical datasets due to their abundant structured and unstructured notes. On MIMIC-III, \parencite{wang_wonder_2024} developed an extractive summarization with semantic enhancement through a topic-injection-based BERT model. The study measured semantic similarity between two pieces of text. \parencite{saeed_sumex_2024} employs a hybrid framework for semantic textual similarity and explanation generation.  Recent research work by  \parencite{holm_local_2023} has developed FactReranker, a fact-based reranking method. Domain-specific information ensures fact consistency in these tactics. Extractive models are simple and fast, but these tests reveal that high-quality clinical summaries require more abstractive, semantically aware production processes. In summary, extractive summarization approaches remain attractive for their simplicity and efficiency, but these techniques fail to capture the coherence and interpretability required for clinical use. This limitation explains why subsequent work has shifted attention toward abstractive methods, particularly zero-shot prompting and fine-tuned neural models, which can generate more fluent summaries.
 

\subsection{Abstractive Summarization}
\subsubsection{Zero-Shot Methods}
Zero-shot summarization has received attention because it avoids the cost of task-specific training, but its effectiveness in clinical domains remains uneven.
For discharge summaries \parencite{van_veen_clinical_2023} reported only modest lexical overlap (ROUGE-1: 0.27) using instruction-tuned FLAN-T5 and BART. This finding underscores the difficulty of applying general instruction-tuned models to long, heterogeneous narratives that contain multiple clinical sections. In contrast, \parencite{ganzinger_automated_2025} achieved state-of-the-art results on radiology impressions by leveraging in-context prompts with clinically similar exemplars. The promising results in this narrower setting reflect the shorter output length and more templated style of radiology reports, which reduce the burden of domain adaptation. A similar domain specificity is evident in the work of \parencite{hu_zero-shot_2024}, who demonstrated competitive zero-shot information extraction from radiology reports using ChatGPT, albeit with performance highly dependent on prompt design and with occasional factual inaccuracies.
These studies suggest that zero-shot approaches can work effectively in tasks that are limited and structurally homogeneous, but their usefulness decreases for lengthier, multi-section clinical papers where factual consistency and coherence are crucial. This contrast motivates our decision to move beyond zero-shot prompting and to evaluate parameter-efficient fine-tuning strategies so that they can better adapt large models to the linguistic and structural complexity of discharge summaries.

\subsubsection{Full Parameter Fine-Tuning}
Full parameter fine-tuning remains the conventional approach for adapting large models to biomedical text, and it consistently improves summary quality. However, it also introduces significant computational demands that limit its practicality in many healthcare settings. \parencite{lv_full_2024} attempted to reduce memory costs through the Low Memory Optimization (LOMO) algorithm, while \parencite{liu_hift_2024} proposed a hierarchical strategy that updates only subsets of parameters at each training step. Both methods highlight that even when memory usage is reduced, full fine-tuning still requires substantial resources. \parencite{qin_federated_2024} further illustrated this point by exploring federated fine-tuning for billion-parameter models, a direction that reduces data-sharing risks but amplifies communication overhead.\\
\\
These studies confirm not only the effectiveness of full fine-tuning, which is already well established, but also highlight the persistent challenges regarding its efficiency. Even improved optimizers or federated protocols do not fundamentally resolve the cost of updating all parameters. For clinical summarization, where hospitals and research units often operate under strict computational and privacy constraints, such approaches are difficult to scale. This observation strengthens the rationale for evaluating parameter-efficient alternatives such as LoRA, which promise comparable or superior performance at a fraction of the resource cost. Large language model-based approaches demonstrate significant promise in the field of healthcare; however, they require further adaptation to become practical for routine implementation in healthcare settings.

\subsubsection{Large Language Models in Healthcare}
LLMs in healthcare are being utilized for many different purposes, some of them including clinical note summarization, question-answering, dialog summarization, and automating different issues  \parencite{luo_chatgpt_2025} that have been trained or tested on MIMIC datasets. Medical experts judged their model as being as accurate as a human at retrieving clinically significant concepts. \parencite{huang_survey_2025} have shown that customized LLMs may capture domain-specific semantics with great precision. The investigation is limited, though, because it just looks at discharge summaries and doesn't provide a full range of benchmarking for real-world deployment circumstances. In another work, \parencite{wu_epfl-make_2024} introduced MEDISCHARGE, an instruction-tuned LLM pipeline that uses the Meditron-7B model as a starting point. Their method got around the problem of short inputs by dynamically choosing important parts of MIMIC-IV clinical notes to make Brief Hospital Course and Discharge Instruction summaries. The model did better than baseline approaches in BLEU, ROUGE, and BERTScore tests, showing that it can make short, relevant summaries. Even while the framework works well, its intricacy and focus on discharge situations make it less useful for a wider range of clinical narratives. These studies reveal that LLMs have a lot of potential for summarizing MIMIC-derived clinical narratives, but there are still problems with generalizability, factuality, and standardization. To move forward with real-world adoption, we still need evaluation methodologies that are aligned with the clinic and adaptations that are relevant to the domain.\\

Overall, the literature shows a clear pattern: extractive methods often lack coherence, zero-shot prompting struggles with longer notes, and full fine-tuning, though effective, is costly to run. Large language model–based approaches are promising but not yet practical for routine use without adaptation. Parameter-efficient fine-tuning offers a middle ground, maintaining or improving performance while sharply lowering training and inference costs. In this study, we compare LoRA with both full fine-tuning and zero-shot approaches across several transformer models to fill this gap and show a more scalable route for clinical text summarization.

\subsubsection{Parameter-Efficient Fine-Tuning}

Recent advances in large pretrained language models have revolutionized natural language processing, but fine-tuning these massive models on domain-specific tasks such as clinical text summarization remains computationally expensive and resource-intensive \parencite{maazallahi_advancing_2025}. This challenge is especially pronounced in clinical settings, where computational resources may be limited, and data privacy concerns restrict large-scale model retraining. \parencite{jiang_res-tuning_2023} and \parencite{ruan_tte_2025} have shown in their studies that PEFT approaches have become a useful way to get around these problems by cutting down on the number of trainable parameters by a lot while keeping or even improving task performance. Later, \parencite{german-morales_transfer_2025} proposed Low-Rank Adaptation, which is a well-known PEFT approach that uses low-rank matrices to approximate the weight updates of the model's transformer layers. By freezing the original model weights and learning only the low-rank decomposition matrices, LoRA significantly reduces the fine-tuning parameter count, often by more than 95\% without degrading performance. Adapter tuning proposed by \parencite{wang_parameter-efficient_2025} is another type of PEFT that adds small bottleneck feed-forward networks (adapters) to transformer layers and trains them while keeping the main model parameters the same.


\subsection{Problems Encountered in the Previous Studies}

Transformer models and LLMs have demonstrated substantial promise in general healthcare applications. However, their usefulness in clinical text summaries has received scant attention.  Mainly, we categorized them into three groups of problems encountered in the previous studies as described below: 
\begin{enumerate}[label=\Alph*)]
    \item \textbf{Model Coverage:} Much of the previous and current research examined individual model families or focused on a single fine-tuning method, providing minimal insight into the comparative strengths and trade-offs between designs. Similarly, most previous studies focused on a single model family (example: BART-based models) or one fine-tuning method. This left a lack of systematic cross-architecture comparisons that evaluate the trade-offs between different traditional transformers and LLMs.
    \item \textbf{Parameter-Efficient Fine-Tuning:} Parameter-efficient techniques such as LoRA are understudied in clinical settings, and when they are researched, evaluations frequently omit crucial parameters such as memory utilization, training time, or deployment cost.
    \item \textbf{Evaluation Limitations:} Extractive approaches, while computationally light, struggle with abstractive gold summaries written by physicians, resulting in low ROUGE scores and poor semantic alignment. Existing study evaluation techniques are likewise mainly based on surface-level lexical overlaps, frequently neglecting deeper semantic fidelity and clinical factual consistency.
\end{enumerate}


\subsection{Research Objectives and Problem Definition}
To address these gaps, our work undertakes a comprehensive, cross-architecture evaluation across transformer and LLM models with the help of prompt engineering and applies three training strategies, incorporating lexical, semantic, and clinical metrics, including cTAKES-extracted entities. In doing so, we combine a clinically informed extractive pipeline with LoRA-based abstractive generation to bridge performance with practicality. By quantifying both summary quality and computational efficiency, our study shows a clinically useful and cost-effective methodology that improves the quality of medical text summarization by looking at both the quality of the summary and the cost of the computation. The research aims to demonstrate that LoRA is not only useful but also a superior strategy for abstractive medical text summarization, particularly for LLMs. This shows that high-quality summaries can be achieved with significantly fewer computational resources. 
This study aims to address several key gaps in the existing literature on clinical text summarization. Our primary objective is to systematically evaluate the performance and computational efficiency of modern language models and fine-tuning strategies. We aim to identify an optimal, robust solution for resource-constrained healthcare environments, thereby establishing a new performance benchmark for clinical text summarization. To accomplish this study, we formulate the following research questions: 

\begin{itemize}

\item \textbf{RQ1: Comparative Performance of Fine-Tuning Strategies in Transformers vs. LLMs:} How do different fine-tuning strategies (zero-shot inference, full parameter tuning, and LoRA) influence the summarization performance of conventional transformer models compared to large language models on clinical text?
 \item \textbf{RQ2: Extractive vs. Abstractive Baselines:} How do neural abstractive summarization approaches (transformers and LLMs) with fine-tuning compare against classical graph-based extractive summarization methods (LexRank, TextRank, LSA, Luhn) in terms of coherence, factual accuracy, and clinical usability?
 \item \textbf{RQ3: Viability of LoRA for Resource-Constrained Environments:} To what extent does LoRA parameter-efficient fine-tuning improve practical viability by reducing trainable parameters, training time, and GPU memory requirements, while maintaining or enhancing summarization quality in resource-constrained healthcare environments? 

\item \textbf{RQ4: Zero-Shot Summarization Efficacy:} How effective is zero-shot summarization using structured prompts as a baseline for clinical text, and to what degree does subsequent fine-tuning (LoRA or full) improve performance in terms of factual accuracy and coherent medical summaries that minimize hallucination?
\end{itemize}

\section{Methods}
\subsection{Proposed System Architecture}
This study uses the MIMIC-IV-Ext-BHC v1.2.0 dataset displayed in Figure \ref{fig:methodology} to create a two-stage clinical summarizing system that combines extractive and abstractive approaches. First, a full preparation pipeline was used on discharge summaries to get rid of protected health information (PHI),  make the text more uniform, expand clinical appointment abbreviations, and use chunking algorithms that are cognizant of tokenizers. cTAKES was employed to identify key clinical entities such as diseases, medications, and procedures, with entity-rich sentences prioritized for ranking using LexRank, TextRank, LSA, and Luhn.   \\

For abstractive summarization, we evaluated zero-shot inference, full parameter fine-tuning, and parameter-efficient LoRA fine-tuning across four transformer models (BART, T5, PEGASUS-XSUM, and FLAN-T5) and two LLMs (Mistral-7B and LLaMA-3-8B). Structured model-specific prompting techniques guided zero-shot and fine-tuned generation. Performance was assessed using lexical (ROUGE, METEOR) and semantic (BERTScore) metrics. The best-performing model from both paradigms was integrated into a GUI that supports entity extraction and summarization. This end-to-end methodology balances clinical fidelity, efficiency, and scalability for real-world deployment in resource-constrained healthcare environments. Mathematically, abstractive summarization can be framed as a \textit{conditional sequence generation} task. 
The source clinical note is represented as a sequence of tokens:
\begin{equation}
X = (x_1, x_2, \ldots, x_n)
\label{eq:source_tokens}
\end{equation}
and the goal is to produce a summary, also as a sequence of tokens:
\begin{equation}
Y = (y_1, y_2, \ldots, y_m)
\label{eq:summary_tokens}
\end{equation}
The model generates this summary \textbf{one token at a time}, predicting each new token $y_t$ based on:
\begin{enumerate}
    \item The \textbf{entire source document} $X$
    \item All previously generated tokens in the summary, denoted as $y_{<t}$
\end{enumerate}
The objective is to find the summary $Y^*$ that \textbf{maximizes the conditional probability} of the summary given the source document:
\begin{equation}
Y^* = \arg\max_Y P(Y \mid X; \theta)
\label{eq:objective}
\end{equation}
Using the \textbf{chain rule of probability}, this joint probability can be decomposed into a product of step-by-step probabilities:
\begin{equation}
P(Y \mid X; \theta) = \prod_{t=1}^m P(y_t \mid X, y_{<t}; \theta)
\label{eq:chain_rule}
\end{equation}
where: $P(y_t \mid X, y_{<t}; \theta)$ is the probability of generating token $y_t$ next, given the document and all tokens generated so far, and $\theta$ represents the model's learned parameters (weights). 
 The model starts with no summary tokens and predicts the first word $y_1$ based only on $X$. It then predicts $y_2$ using both $X$ and $y_1$, then $y_3$ using $X$, $y_1$, and $y_2$, and so on, until the clinical summary is complete. This formulation enables the model to produce \textit{novel phrasing} and \textit{reorganized structure} instead of simply copying texts from the clinical source.
\begin{figure}
    \centering
    \includegraphics[width=1.1\linewidth]{methodology diagram.jpeg}
    \caption{Overview of the Proposed Architecture Clinical Text Summarization }
    \label{fig:methodology}
\end{figure}
\subsection{Dataset}

The experiments conducted in this study utilize the MIMIC-IV-Ext-BHC v1.2.0 clinical note dataset \parencite {aali_mimic-iv-ext-bhc_nodate}, a comprehensive and publicly available collection of deidentified free-text clinical notes curated by the MIT Laboratory for Computational Physiology, with a revision date of February 3, 2025. This dataset encompasses a wide range of documents, including discharge summaries, radiology reports, nursing notes, physician notes, and other forms of clinical documentation. All records within the MIMIC-IV-Note dataset have undergone an extensive de-identification process to remove any protected health information, such as patient names, dates, locations, and contact details.\\

This study primarily focuses on the discharge summaries subset, which consists of 270,033 records, input token length (2267 $\pm 914$), and target token length (564 $\pm 410$). Each note is paired with a corresponding Brief Hospital Course (BHC) summary, serving as the ground-truth target for our abstractive summarization task. These notes have an average token length of 2,267. For all experiments, the dataset was split into training, validation, and test sets to ensure robust model generalization and evaluation \parencite{ayalew_explainable_2025}. These documents summarize a patient's hospital course, diagnoses, treatments, and discharge instructions. Due to their rich semantic content and relatively structured format, discharge summaries serve as an ideal source for clinical text summarization tasks. Each note in the dataset is deidentified using a hybrid rule-based and neural Named Entity Recognition (NER) system. All Protected Health Information (PHI) is replaced with standardized placeholders, and notes are linked via anonymized identifiers:
\begin{itemize}
    \item \texttt{note\_id}: Unique identifier for each note.
    \item \texttt{input:} preprocessed discharge note text. The text provides a detailed narrative of the patient's hospitalization, including sections like medical history, treatments, and discharge instructions.
    \item \texttt{target}: A concise summary of the patient's hospital course, including major events, treatments, and outcomes during the hospital stay.
    \item \texttt{input\_tokens}: Contains the input token length.
    \item \texttt{target\_tokens}: Contains the target token length.
\end{itemize}

All data usage complies with the PhysioNet Credentialed Health Data License. Access was granted upon completion of CITI certification and a signed data use agreement \parencite{chicco_venus_2025}.  While the size of our dataset may appear modest compared to general-domain corpora, the MIMIC-IV datasets are a substantial and widely accepted benchmark for clinical NLP research due to their high quality and domain-specific complexity. The dataset is well prepared for doing medical informatics, training LLMs is domain-adapted for clinical summarization tasks, and is efficient for healthcare delivery. It’s also a robust, realistic, appropriate, and highly scalable foundation for our research, especially for clinical note summarization. As a result, this dataset is a new version and well-suited for doing this study. 
\subsection{Data Preprocessing}

Given the unstructured and noisy nature of clinical notes, a comprehensive preprocessing pipeline was implemented using \texttt{pandas}, \texttt{nltk}, and other libraries (see Algorithm \ref{alg:data_preprocessing1}). The following key steps were applied to prepare the discharge summaries for both extractive and abstractive summarization experiments:

\begin{enumerate}
 \item \textbf{Filtering and Text Normalization:} Notes with missing or empty \texttt{input} or \texttt{target} fields were filtered out. Basic text normalization included converting to lowercase, removing excess whitespace and newline characters, and ensuring punctuation regularity. Notes with missing or empty input or target fields were removed to ensure only complete examples were retained. Formally, given the original dataset:
\begin{equation}
\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N
\end{equation}
Where $x_i$ is the input note and $y_i$ is the target summary, filtering removes incomplete pairs:
\begin{equation}
\mathcal{D}' = \{(x_i, y_i) \in \mathcal{D} \mid x_i \neq \varnothing \ \land \ y_i \neq \varnothing \}
\end{equation}

This guarantees that training data always contains meaningful input–output pairs, preventing models from learning spurious patterns. After filtering, we standardized the remaining text by converting all characters to lowercase, collapsing multiple whitespaces (newlines), and ensuring consistent punctuation. This is represented by the normalization function:
\begin{equation}
\mathcal{N}(t) = \mathrm{punct\_norm} \left( \mathrm{ws\_norm} \left( \mathrm{lower}(t) \right) \right)
\end{equation}
Where $\text{lower}(t)$ converts to lowercase, $\text{ws\_norm}(t)$ collapses whitespace and line breaks, and $\text{punct\_norm}(t)$ enforces regular punctuation. Applying normalization gives:
\begin{equation}
\mathcal{D}'' = \{(\mathcal{N}(x_i), \mathcal{N}(y_i)) \mid (x_i, y_i) \in \mathcal{D}' \}
\end{equation}
This reduces vocabulary fragmentation (for example, “Hypertension” and “hypertension” become the same token) and ensures tokenizer stability across models. 


\textbf{Practical Impact.} These preprocessing steps ensure that the dataset is clean, consistent, and clinically meaningful. Filtering improves reliability by avoiding incomplete cases, while normalization helps models focus on medical content rather than formatting inconsistencies. Together, these steps improve summarization accuracy and make the system safer for clinical deployment, as models trained on consistent data are less likely to misinterpret critical medical terms.
\item\textbf{{Handling Special Characters and Medical Abbreviation Expansion:}}
Clinical notes often contain a high density of abbreviations, symbols, and irregular formatting. To make the dataset clearer and more consistent, we implemented a specialized preprocessing pipeline to handle these issues. We first defined a curated \textbf{medical abbreviation lexicon}:  
\begin{equation}
\mathcal{A} = \{(a_k, e_k)\}_{k=1}^K
\end{equation}

Where $a_k$ is an abbreviation (for example, ``SOB'') and $e_k$ its expanded form (``shortness of breath'').  This lexicon acts as a lookup table that ensures ambiguous abbreviations are consistently expanded into their full medical meaning when appropriate. Next, we defined the \textbf{abbreviation expansion function}:  
\begin{equation}
\mathcal{E}(t) = \text{replace}_{a_k \in \mathcal{A}}(a_k \rightarrow e_k, \ t)
\end{equation}

This function replaces $a_k$ in text $t$ with its expanded form $e_k$, while keeping conventional abbreviations unchanged if they are well recognized.  For example, ``HTN'' is expanded to ``hypertension,'' but abbreviations like ``MRI'' remain unchanged since they are widely understood by clinicians and tokenizers. This reduces ambiguity and improves the interpretability of generated summaries. We also introduced the \textbf{special character normalization function}:  

\begin{equation}
\mathcal{S}(t) = \mathrm{char\_norm}(t)
\end{equation}

Where it $\mathrm{char\_norm}(\cdot)$ removes or standardizes non-standard symbols, irregular formatting, and extraneous special characters. This step cleans up noisy characters (for example, stray ``@@@'' or inconsistent ``+/-'' usage) and enforces consistent formatting, making the dataset cleaner and easier for models to process. Finally, the preprocessed dataset after abbreviation expansion and special character handling is:  

\begin{equation}
\mathcal{D}''' = \{(\mathcal{S}(\mathcal{E}(x_i)), \ \mathcal{S}(\mathcal{E}(y_i))) \mid (x_i, y_i) \in \mathcal{D}'' \}
\end{equation}

This means that every input note and summary pair is standardized by expanding abbreviations and normalizing special characters before model training.


\textbf{Practical Impact:}  
These steps ensure consistent handling of medical abbreviations, reduce textual noise, and improve clarity. This not only helps models better capture clinical meaning but also increases the safety and interpretability of the generated summaries for healthcare professionals.


 \item \textbf{Chunking Strategy:} To address the input length limitations of transformer models, a sentence-level, tokenizer-aware chunking strategy was applied to MIMIC-IV clinical notes, but only when the tokenized input exceeded \texttt{1024} tokens. In such cases, the \texttt{chunk\_text} function segmented the document into smaller overlapping chunks, each containing at most \texttt{1024} tokens (\texttt{MAX\_INPUT\_TOKENS}), with a \texttt{100}-token overlap (\texttt{CHUNK\_OVERLAP}) to preserve contextual continuity between chunks. This overlap is especially essential in medical notes, where clinically relevant material can span several phrases. By conditionally chunking only longer notes, the technique minimizes unnecessary fragmentation while maintaining narrative coherence. This selective preprocessing ensures conformity with model input constraints while preserving key semantic structure, thereby enabling more accurate and context-aware summarization across diverse clinical note lengths. More specifically, given the preprocessed dataset $t$ from the above steps, we can define the tokenization function as \mathcal{T}(t) and the token length as:
\begin{equation}
L(t) = |\mathcal{T}(t)|
\end{equation}

Where $\mathcal{T}(t)$ is the sequence of tokens for text $t$, $L(t)$ is its length in terms of the number of tokens in text $t$.$L(t($ is important because transformer models can only handle inputs up to a fixed size only. Given the maximum token limit $T_{\max}$ and the overlap size $O$, the chunking function is defined as follows:
\begin{equation}
\mathcal{C}(t) =
\begin{cases}
[t], & \text{if } L(t) \leq T_{\max} \\
\{ \mathcal{T}(t)[1:T_{\max}], \ \mathcal{T}(t)[T_{\max}-O+1:2T_{\max}-O], \ \dots \}, & \text{if } L(t) > T_{\max}
\end{cases}
\end{equation}

If a note is short enough, it is kept as a single sequence. Otherwise, it is split into overlapping token windows of length $T_{\max}$ with an overlap of $O$ tokens. This ensures that clinical information spanning across segment boundaries is preserved, enabling more coherent and accurate summarization. The final chunked dataset is:
\begin{equation}
\mathcal{D}^{\mathrm{chunk}} =
\big\{ (\mathcal{C}(x_i), \ \mathcal{C}(y_i)) \ \big|\ (x_i, y_i) \in \mathcal{D}''' \big\}
\end{equation}

\textbf{Practical Impact:}  

This strategy ensures that long clinical documents are processed without exceeding model limits, while overlap between chunks preserves clinical context across boundaries. As a result, the summarization system remains accurate and context-aware, regardless of input length.
   
\item \textbf{Truncation/Padding Strategies:} Transformer models have a maximum input sequence length, and clinical notes often exceed this limit (1024 tokens). To handle this, we applied truncation for longer sequences and padding for shorter ones. Truncation was carefully designed to retain the most clinically relevant information, usually found at the beginning of notes, such as patient demographics and admission details. Padding ensures uniform sequence lengths, preventing alignment issues during model training. Target summaries were similarly normalized to match input lengths, facilitating consistent model outputs. Assuming we have the chunked dataset from the above steps, we define the truncation function as follows:
\begin{equation}
\mathrm{trunc}(t) =
\begin{cases}
\mathcal{T}(t)[1:T_{\max}], & \text{if } L(t) > T_{\max} \\
\mathcal{T}(t), & \text{otherwise}
\end{cases}
\end{equation}
If a note exceeds $T_{\max}$ tokens, only the first $T_{\max}$ tokens are retained, preserving the most essential patient information. Otherwise, the text remains unchanged.

Define the padding function:
\begin{equation}
\mathrm{pad}(t) =
\begin{cases}
\mathcal{T}(t) \ \Vert \ \mathrm{[PAD]}^{\,T_{\max} - L(t)}, & \text{if } L(t) < T_{\max} \\
\mathcal{T}(t), & \text{otherwise}
\end{cases}
\end{equation}
Where $\mathrm{[PAD]}$ is the padding token, and $\Vert$ denotes sequence concatenation.  
If a note has fewer $T_{\max}$ tokens, padding tokens are appended until it reaches the maximum length. This ensures all sequences are uniform, avoiding model errors and maintaining consistent input formatting.

The final length-normalized dataset is:
\begin{equation}
\mathcal{D}^{\mathrm{final}} =
\big\{ (\mathrm{pad}(\mathrm{trunc}(x_i)), \ \mathrm{pad}(\mathrm{trunc}(y_i))) \ \big|\ (x_i, y_i) \in \mathcal{D}^{\mathrm{chunk}} \big\}
\end{equation}

\textbf{Practical impact:} This preprocessing guarantees that all model inputs and outputs meet transformer length requirements while prioritizing clinically essential content, improving the reliability and accuracy of summarization.

\item \textbf{Section Header and Tag Normalization:} To reduce noise and guide the summarization process, only semantically rich sections of the note were retained. Therefore, to improve structural consistency and facilitate section-aware downstream processing, clinical notes frequently embedded with irregular or system-generated tags were adjusted using a rule-based heuristic method. The tag replacement dictionary encompasses more than 22 distinct medical section headers commonly found in MIMIC-IV. This step standardizes section boundaries, normalizes irregular tags, and removes formatting artifacts, improving readability and ensuring compatibility with section-aware summarization workflows. 
\begin{algorithm}[H]
\caption{Data Preprocessing for Clinical Notes}
\label{alg:data_preprocessing1}
\begin{algorithmic}[1]
\Require Raw Clinical Dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$, Token Limit $\tau$
\Ensure Cleaned Dataset $\tilde{\mathcal{D}} = \{(\tilde{x}_i, y_i)\}_{i=1}^N$
\For{each $(x_i, y_i) \in \mathcal{D}$}
    \State Remove PHI fields using regex placeholders
    \State Expand medical abbreviations using domain lexicon
    \State Normalize tags, headers, and remove formatting artifacts
    \If{$\text{TokenCount}(x_i) > \tau$}
        \State Chunk $x_i$ into overlapping segments of length $\tau$
    \EndIf
    \State Tokenize $x_i$ into sentences using \texttt{nltk.sent\_tokenize}
    \State Standardize section headers via dictionary mapping
    \State $\tilde{x}_i \gets x_i$ \Comment{Store preprocessed note}
\EndFor
\State \Return $\tilde{\mathcal{D}}$
\end{algorithmic}
\end{algorithm}


\subsection{Prompting Engineering Strategies}
To optimize the language models' generating capabilities for clinical text summarization, we designed model-specific prompting techniques that consider the distinctive qualities of each architecture, as well as its pre-training goals. Each prompt was meticulously crafted with task-specific instructions that are explicit and describe the clinical summary aim. Additionally, organized output criteria that are customized to healthcare documentation standards were incorporated into each prompt. We employ a zero-shot design to provide the language model with structured input-output examples, establishing a consistent format for clinical summarization. These observed improvements indicate that providing the model with representative clinical examples helps constrain its output format, enhancing factual coherence and reducing hallucinations. 

\section{Evaluation Metrics} 
\subsection{ROUGE Scores}
We employ ROUGE-N and ROUGE-L metrics to quantify n-gram and longest common subsequence overlap between the generated summary \( G \) and the reference summary \( R \).

\begin{itemize}
    \item \textbf{ROUGE-N (ROUGE-1 and ROUGE-2):} ROUGE-1: Measures the intersection (overlap) of unigrams (individual words) between the produced and reference summaries. It primarily evaluates the existence of significant keywords. ROUGE-2: Assesses the intersection of bigrams (word pairs). 
    \begin{equation}
    \text{ROUGE-N} = \frac{\sum_{S \in \{R\}} \sum_{\text{gram}_n \in S} \min \left( \text{Count}_{G}(\text{gram}_n), \text{Count}_{R}(\text{gram}_n) \right)}{\sum_{S \in \{R\}} \sum_{\text{gram}_n \in S} \text{Count}_{R}(\text{gram}_n)}
    \label{eq:rouge-n}
    \end{equation}
    
    \item \textbf{ROUGE-L:} Used to evaluate the longest common subsequence of the produced and benchmark summaries. It measures structural similarity at the sentence level and indicates how effectively the produced summary maintains the primary concepts in their original sequence.
    \begin{equation}
    \text{ROUGE-L} = \frac{LCS(G, R)}{\text{length}(R)}
    \label{eq:rouge-l}
    \end{equation}
\end{itemize}

\subsection{METEOR Score}
METEOR is a measure created for assessing machine translation, although it is also significantly relevant to summarization. In contrast to ROUGE, METEOR evaluates not just precise word matches but also matches derived from stemmed words, synonyms (utilizing WordNet), and paraphrases. METEOR assesses exact and fuzzy matching through stemming, synonymy, and word order. It combines unigram precision \( P \) and recall \( R \), penalized by fragmentation \( Pen \):
\begin{equation}
F_{mean} = \frac{10 \cdot P \cdot R}{R + 9 \cdot P}
\label{eq:fmean}
\end{equation}
\begin{equation}
\text{METEOR} = F_{mean} \cdot (1 - Pen)
\label{eq:meteor}
\end{equation}
\subsection{BERTScore}
The BERTScore is a more contemporary and semantically aware evaluation metric that uses contextual embeddings from pre-trained BERT models. It compares the semantic similarity of tokens in the candidate summary and the reference summary. Therefore, to measure semantic similarity, we utilize BERTScore using contextual embeddings from a transformer model:
\begin{equation}
\text{Precision} = \frac{1}{|G|} \sum_{g \in G} \max_{r \in R} \cos(g, r)
\label{eq:bert-p}
\end{equation}
\begin{equation}
\text{Recall} = \frac{1}{|R|} \sum_{r \in R} \max_{g \in G} \cos(r, g)
\label{eq:bert-r}
\end{equation}
\begin{equation}
\text{F1} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\label{eq:bert-f1}
\end{equation}
\section{Implementation Details}
All experiments were conducted using the Python programming language with essential libraries such as PyTorch and the Hugging Face Transformers library, with the PEFT library facilitating LoRA implementation. The training was performed on NVIDIA A100 and NVIDIA  A1000 GPUs, employing memory management techniques such as gradient checkpointing to reduce peak memory usage. For fine-tuning experiments, the dataset was partitioned into training, validation, and test sets with a standard ratio (70\% training, 15\% validation, 15\% test) \parencite{ayalew_smart_2025}. The validation set was used for hyperparameter tuning and early stopping to prevent overfitting, while the test set was held out and used only for final performance evaluation to ensure an unbiased assessment of generalization capabilities \parencite{ayalew_smart_2024}. In this proposed work, we employed the "large" version of each transformer model (with a maximum sequence length of 1024 tokens) to maintain a consistent capacity across the different architectures. The model was trained for 5 epochs using a learning rate of $2 \times 10^{-5}$, a per-device batch size of 4, and 8 gradient accumulation steps. For the generation process, we utilized 3 beams, applying a length penalty of 1.0 and a repetition penalty of 1.1. Additionally, a no-repeat n-gram size of 3 was enforced to avoid redundancy. The generation process was further refined with early stopping set to True, an overlap of 100, and a dynamic batch size ranging from 6 to 12.

\section{Experimental Results}
We conducted four categories of experiments. The first experiment focused on extractive summarization utilizing cTAKES. The second experiment, detailed in Algorithm \ref{alg:abstractive}, explored abstractive summarization using both transformer and LLM models without employing fine-tuning techniques. The third experiment involved full parameter fine-tuning exclusively for transformer models, as we opted not to perform full parameter fine-tuning for LLMs due to limitations in computational resources. Finally, the fourth experiment examined parameter-efficient fine-tuning via LoRA for both transformer and LLM models.

\begin{algorithm}[H]
\caption{Abstractive Summarization with Zero-Shot, Full, and LoRA Fine-Tuning}
\label{alg:abstractive}
\begin{algorithmic}[1]
\Require Cleaned Dataset $\tilde{\mathcal{D}} = \{(\tilde{x}_i, y_i)\}_{i=1}^N$, Transformers $\mathcal{M}_T$, LoRA Models $\mathcal{M}_L$
\Ensure Abstractive Summaries $S_Z$, $S_F$, $S_L$
\State \textbf{Zero-Shot Inference}
\For{each $T_j \in \mathcal{M}_T$}
    \For{each $\tilde{x}_i$}
        \State $s_{ij}^{(Z)} \gets T_j(\texttt{prompt}(\tilde{x}_i))$
    \EndFor
\EndFor
\State $S_Z \gets \{s_{ij}^{(Z)}\}$

\State \textbf{Full Fine-Tuning}
\For{each $T_j \in \mathcal{M}_T$}
    \State Initialize model parameters $\theta$
    \For{epoch $= 1$ to $E$}
        \For{each batch $(x_b, y_b)$}
            \State $\mathcal{L}_{CE} \gets$ CrossEntropy$(T_j(x_b), y_b)$
            \State $\theta \gets \theta - \eta \nabla_\theta \mathcal{L}_{CE}$
        \EndFor
    \EndFor
    \State $S_F \gets T_j(\tilde{\mathcal{D}})$
\EndFor

\State \textbf{LoRA Fine-Tuning}
\For{each $L_j \in \mathcal{M}_L$}
    \State Freeze $\theta$; initialize low-rank adapters $(A, B)$
    \For{epoch $= 1$ to $E$}
        \For{each batch $(x_b, y_b)$}
            \State $\Delta W = B A$ \Comment{LoRA update}
            \State $\mathcal{L}_{LoRA} \gets$ CrossEntropy$((\theta + \Delta W)(x_b), y_b)$
            \State Update $A, B$ using $\nabla \mathcal{L}_{LoRA}$
        \EndFor
    \EndFor
    \State $S_L \gets L_j(\tilde{\mathcal{D}})$
\EndFor
\State \Return $S_Z$, $S_F$, $S_L$
\end{algorithmic}
\end{algorithm}



\subsection{Experiments: 1. Graph-based extractive summarizer algorithms performance using cTAKES} 

In the first phase, we evaluated four classical graph-based summarization algorithms, such as LexRank, TextRank, Latent Semantic Analysis (LSA), and Luhn, on the MIMIC-IV discharge summaries. Each algorithm constructs a sentence similarity graph using TF-IDF cosine similarity and ranks sentences accordingly. To enrich clinical relevance, we used cTAKES to extract named entities such as \textit{ Disease/Disorder, Medication, Anatomical Site, Procedure, and Sign/Symptom} as shown in Figure \ref{fig:cTAKES Named Entities}. We combine graph-based centrality, clinical named entity recognition, and semantic similarity. Sentences containing these entities were prioritized for summarization, enhancing the informativeness of the extractive input. \textcolor{black}{To make the similarity graph more clinically aware, we went beyond simple TF–IDF cosine similarity and added two sources of biomedical semantic information. First, we utilized one of the domain-specific semantic similarity techniques, BioSentVec, a sentence-level embedding model trained on PubMed and clinical notes, which reflects how medical terms and phrases relate to each other in real clinical writing \parencite{dhawan_healthcare_2024}. This helps the system recognize that two sentences may carry the same clinical meaning even if they use different wording (for example, “SOB” vs. “shortness of breath”). Second, we integrate the cTAKES-identified UMLS domain-specific concepts in each sentence into their CUI2Vec embeddings \parencite{giancani_quality_2023}. Because CUI2Vec is built from electronic medical records and claims data, it captures common medical co-occurrence patterns such as which diseases typically appear together, which drugs are used for which conditions, or which findings are associated with particular diagnoses. By combining lexical TF-IDF overlaps, BioSentVec sentence similarity, and cui2vec concept similarity, the graph reflects domain-specific relationships that matter in clinical text rather than generic language patterns.}

\textcolor{black}{Even with these additions, the overall extractive performance remains modest. This is expected because MIMIC-IV discharge and radiology notes are highly compressed, clinically dense, and often shift topics quickly. The gold summaries are also fully abstractive and written by clinicians, meaning they reorganize, paraphrase, and condense the narrative in ways an extractive method cannot match. As a result, even a clinically enriched extractive model cannot reach the coherence and semantic alignment that the abstractive models achieved after fine-tuning.} Despite this, the overall performance remained modest. For instance, LexRank, the best-performing method, achieved ROUGE-1 of 0.273 (see Table \ref{tab:summary_performance1}). These relatively low scores can be attributed to the abstractive nature of the gold summaries, which were made by medical professionals and often paraphrased, synthesized, or reordered the source content. As extractive methods rely on surface-level sentence overlap, they struggled to match these abstract summaries word for word, particularly in semantic alignment and flow. 
\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\linewidth]{cTAKES Named Entities.jpeg}
    \caption{cTAKES-based Clinical Named Entity Extraction}
    \label{fig:cTAKES Named Entities}
\end{figure}


\begin{longtable}{|p{1.6cm}|p{1.4cm}|p{1.4cm}|p{1.5cm}|p{1.5cm}|p{1.7cm}|p{1.7cm}|p{1.5cm}|}
\caption{Summary performance metrics for different extractive models.\label{tab:summary_performance1}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endfirsthead
\multicolumn{8}{c}{{\tablename\ \thetable{} ....Continued from previous page}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERTScore F1} \\
\hline
\endhead
\hline \multicolumn{8}{|r|}{{....Continued on next page}} \\ \hline
\endfoot
\hline
\endlastfoot
\textbf{LexRank}   & 0.273 & 0.066 & 0.248 & 0.131 & 0.143 & 0.158 & 0.151 \\
\hline
\textbf{TextRank} & 0.250 & 0.061 & 0.240 & 0.126 & 0.138 & 0.135 & 0.134 \\
\hline
\textbf{LSA}      & 0.246 & 0.044 & 0.219 & 0.122 & 0.132 & 0.131 & 0.132 \\
\hline
\textbf{Luhn}     & 0.209 & 0.035 & 0.197 & 0.107 & 0.110 & 0.102 & 0.106 \\

\end{longtable}





\subsection{Experiments: 2. Without Fine-Tuning with Zero-Shot Inference}

\subsubsection{PEGASUS-XSUM AND FLAN-T5 Performance}
In our preliminary experiments, which were conducted without any modifications to the models, FLAN-T5 and PEGASUS-XSUM revealed notably different performance characteristics. As demonstrated in Figure \ref{fig:FLAN_lora_training}, FLAN-T5 emerged as a standout performer, achieving exceptional results on traditional lexical overlap metrics. In contrast, PEGASUS-XSUM, as depicted in Figure \ref{fig:PEGASUS_lora_loss}, consistently fell short of expectations, recording lower performance metrics relative to its counterparts. This disparity is further illustrated in Table \ref{tab:summary_performance}, where PEGASUS-XSUM underperformed across all evaluation metrics compared to the other model variants.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan.png}
        \caption{FLAN Without Fine-tuning}
        \label{fig:FLAN_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus.png}
        \caption{PEGASUS-XSUM Without Fine-tuning}
        \label{fig:PEGASUS_lora_loss}
    \end{minipage}
\end{figure}
\subsubsection{BART and T5 Results}
A zero-shot evaluation of BART and T5 revealed that BART achieved the highest overall performance among the four models assessed. This finding suggests that BART excels in summarizing medical texts without requiring fine-tuning. Specifically, BART outperformed both PEGASUS-XSUM and FLAN-T5 according to traditional lexical overlap metrics. Further evidence of BART's superiority is reflected in the statistical results, which indicate that it scores higher in assessment measures compared to T5, as illustrated in Figures \ref{fig:bart_lora_training} and \ref{fig:T5_lora_loss}.
%\vspace{-0.8cm}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart.png}
        \caption{BART Without Fine-tuning}
        \label{fig:bart_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5.png}
        \caption{T5 Without Fine-tuning}
        \label{fig:T5_lora_loss}
    \end{minipage}
\end{figure}

\subsubsection{Mistral-7B and LLaMA-3-8B }
As seen from Figure \ref{fig:Mistral} and Figure \ref{fig:Llama}, the Mistral-7B and LLaMA-3-8B results are better than the traditional transformer model results.  The evaluation outcomes of Mistral-7B and LLaMA-3-8B through zero-shot inference show significant advancements compared to conventional transformer models like PEGASUS-XSUM, T5, FLAN-T5, and BART. Both models, without fine-tuning, attained improved scores on most metrics, which suggests their superior generalization abilities and comprehension of clinical text.  
\vspace{-4}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.43\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Misteral.png}
        \caption{Mistral-7B Without Fine-tuning}
        \label{fig:Mistral}
    \end{minipage}
    \hfill
    \begin{minipage}{0.43\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Llama.png}
        \caption{LLaMA-3-8B Without Fine-tuning}
        \label{fig:Llama}
    \end{minipage}
\end{figure}


\small
\begin{longtable}{|p{1.6cm}|p{1.4cm}|p{1.4cm}|p{1.5cm}|p{1.5cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|}
\caption{Summary performance metrics for different models without fine-tuning, including ROUGE scores, METEOR, and BERTScore metrics.\label{tab:summary_performance}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERTScore F1} \\
\hline
\endfirsthead
\multicolumn{8}{c}{{\tablename\ \thetable{} ...Continued from previous page}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERTScore F1} \\
\hline
\endhead
\hline \multicolumn{8}{|r|}{{....Continued on next page}} \\ \hline
\endfoot
\hline
\endlastfoot

\textbf{PEGASUS \linebreak -XSUM} & 0.22 & 0.05 & 0.11 & 0.16 & 0.77 & 0.81 & 0.796 \\
\hline
\textbf{FLAN-T5} & 0.25 & 0.04 & 0.12 & 0.15 & 0.80 & 0.80 & 0.80 \\
\hline
\textbf{T5} & 0.25 & 0.05 & 0.12 & 0.15 & 0.81 & 0.81 & 0.81 \\
\hline
\textbf{BART} & 0.27 & 0.06 & 0.14 & 0.17 & 0.811 & 0.814 & 0.817 \\
\hline
\textbf{Mistral-7B} & 0.3621 & 0.1088 & 0.1804 & 0.2270 & 0.8154 & 0.8141 & 0.8273 \\
\hline
\textbf{LLaMA-3-8B} & 0.3855 & 0.0981 & 0.1666 & 0.2671 & 0.8232 & 0.8177 & 0.8235 \\
\hline

\end{longtable}

\subsection{Experiments: 3. Full Parameter Fine-Tuning}
From previous experiments, we have not obtained a better result using zero-shot or an experiment without using fine-tuning. The zero-shot evaluation revealed the limited effectiveness of models in clinical text summarization without domain-specific adaptation, achieving modest ROUGE scores that indicated the necessity for task-specific training. Therefore, we do the following experiments using full parameter fine-tuning techniques.

\subsubsection{PEGASUS-XSUM Using Full Parameter Fine-Tuning}
Based on the complete evaluation results in Table \ref{tab:fullparameter_performance} and the related performance visuals, PEGASUS-XSUM significantly outperformed the previous zero-shot baseline after adjusting all its parameters. Figure \ref{fig:Pegasus2_lora_training} shows that the PEGASUS-XSUM model did much better on all evaluation metrics after full parameter fine-tuning. Figure 6 shows that the training dynamics demonstrated stable and consistent convergence throughout the fine-tuning process. The losses for both training and validation went down steadily over the five epochs. As shown in Figure \ref{fig:Pegasus2_lora_loss}, the training loss went down from about 3.2 to 2.55, and the validation loss went down from 2.85 to 2.49, which shows that the model learned well without overfitting. The fact that both loss curves went down at the same time and were very close to each other showed that the model could generalize well to new clinical texts. 
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with full training.png}
        \caption{PEGASUS-XSUM with full parameter Training}
        \label{fig:Pegasus2_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with full loss.png}
        \caption{PEGASUS-XSUM with full parameter Loss}
        \label{fig:Pegasus2_lora_loss}
    \end{minipage}
\end{figure}
\subsubsection{T5 Using Full Parameter Fine-Tuning}
T5 performed well after comprehensive parameter fine-tuning, generating competitive results across all assessment criteria and displaying consistent learning dynamics throughout the training procedure. As shown in Figure \ref{fig:T52_full_training}, T5 achieved ROUGE-1 scores of 0.4991 (validation) and 0.4969 (test), demonstrating significant gains over its zero-shot baseline, but somewhat lower than FLAN-T5 and BART. Figure \ref{fig:T52_full_loss}  shows outstanding training dynamics, with both training and validation losses converging smoothly. The parallel decrease of loss curves without considerable divergence implies consistent learning and strong generalizability to previously encountered clinical literature.  
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with full training.png}
        \caption{T5 with full parameter Training}
        \label{fig:T52_full_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with full loss.png}
        \caption{T5 with full parameter Loss}
        \label{fig:T52_full_loss}
    \end{minipage}
\end{figure}
\subsubsection{FLAN-T5 Using Full Parameter Fine-Tuning}
Following the meticulous optimization of all its parameters, FLAN-T5 emerged as a standout performer, securing better scores across all evaluation metrics. As illustrated in Figure \ref{fig:Flan_full_training}, FLAN-T5 not only achieved superior ROUGE-1 scores but also outperformed both the zero-shot and fully parameter fine-tuned results of its counterparts, T5 and PEGASUS-XSUM. Furthermore, Figure \ref{fig:Flan_full_loss} provides a compelling illustration of the model’s training dynamics. The loss curves, which descend in a harmonious parallel without showing signs of divergence, indicate that the model is not only learning effectively but also possesses a strong capacity for generalization.
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with full training.png}
        \caption{FLAN with full parameter Training}
        \label{fig:Flan_full_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with full loss.png}
        \caption{FLAN with full parameter Loss}
        \label{fig:Flan_full_loss}
    \end{minipage}
\end{figure}
\subsubsection{BART Using Full Parameter Fine-Tuning}
\textcolor{red}{After fine-tuning all its parameters, BART achieves the highest scores among transformer models compared to other full-parameter fine-tuned models.} It achieved the highest scores across several important evaluation measures and was the best at summarizing clinical text. Table \ref{tab:fullparameter_performance} and Figure \ref{fig:Bart2_full_training} show that BART had the best ROUGE-1 scores. Most impressively, BART got the best METEOR scores and BERTScore, which were much higher than any other model and showed better semantic understanding and content retention. The training dynamics in Figure \ref{fig:Bart2_full_loss} illustrate excellent training dynamics, with both training and validation losses converging smoothly. Despite this, BART has less convergence in training and validation losses compared to other models. This illustration shows stable learning without overfitting. BART's denoising autoencoder pre-training architecture is particularly well-suited for clinical text processing, as it can achieve the most comprehensive performance across semantic and lexical evaluation dimensions. This capability makes it the premier choice for full parameter fine-tuning in medical text summarization applications.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with full training.png}
        \caption{BART with full parameter Training}
        \label{fig:Bart2_full_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with full loss.png}
        \caption{BART with full parameter Loss}
        \label{fig:Bart2_full_loss}
    \end{minipage}
\end{figure}
In general, and based on our experimental results, domain-specific full parameter fine-tuning demonstrates substantial advantages for clinical text summarization, transforming general-purpose transformer models into highly effective medical text processing systems. The comprehensive adaptation of all model parameters enables deep learning of clinical terminology, medical reasoning patterns, and domain-specific linguistic structures that are essential for accurate healthcare documentation summarization. This method helps models gain a deeper understanding of complicated medical relationships, the order in which patients are treated, and the hierarchical nature of clinical information. This leads to summaries that keep important medical context and clinical accuracy. Table \ref{tab:fullparameter_performance} shows that the experimental results show huge performance gains for all of the transformer architectures that were tested when compared to their zero-shot baselines in terms of all evaluation metrics.


\small
\begin{longtable} [htbp]{|p{1.57cm}|p{1.3cm}|p{0.8cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.45cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}|}
\caption{Performance metrics for different models across validation and test stages (full parameter training ), including loss, ROUGE scores, METEOR, and BERTScore metrics.\label{tab:fullparameter_performance}} \\
\hline
\textbf{Model} & \textbf{Stage} & \textbf{Loss} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERT- \linebreak Score Precision} & \textbf{BERT- \linebreak Score Recall} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endfirsthead
\multicolumn{10}{c}{{\tablename\ \thetable{} ....Continued from previous page}} \\
\hline

\endhead
 \hline
\endfoot
\hline
\endlastfoot

\multirow{\textbf{PEGASUS \linebreak -XSUM}} & Validation & \textcolor{black}{2.4913 } & \textcolor{black}{0.4879 }& \textcolor{black}{0.2297}& \textcolor{black}{0.4351} & \textcolor{black}{0.3786 }& 0.8452 & 0.8693 & 0.8569 \\
\cline{2-10}
 & Test & 2.4673  & 0.4867 & 0.2240 & 0.4324 & 0.3694 & 0.8419 & 0.8682 & 0.8546 \\
\hline

\multirow{\textbf{T5}} & Validation & 2.4859 & 0.4991 & 0.2363 & 0.4375 & 0.3814 & 0.8452 & 0.8693 & 0.8569 \\
\cline{2-10}
 & Test & 2.4674 & 0.4969 & 0.2235 & 0.4329 & 0.3787 & 0.8419 & 0.8682 & 0.8546 \\
\hline

\multirow {\textbf{FLAN-T5}} & Validation & 2.3082 & 0.5179 & 0.2508 & 0.4552 & 0.4063 & 0.8524 & 0.8743 & 0.8631 \\
\cline{2-10}
 & Test & 2.2869 & 0.5115 & 0.2372 & 0.4486 & 0.3994 & 0.8488 & 0.8725 & 0.8603 \\
\hline

\multirow{\textbf{BART}} & Validation & 2.7342 & 0.5194 & 0.2474 & 0.4417 & 0.4096 & 0.8691 & 0.8766 & 0.8728 \\
\cline{2-10}
 & Test & 2.7282 & 0.5121 & 0.2310 & 0.4325 & 0.3978 & 0.8665 & 0.8746 & 0.8705 \\
\hline


\end{longtable}



\subsection{Experiments: 4. LoRA Parameter-Efficient Fine-Tuning}
From previous experiments, we have obtained a better result using an experiment with full parameter fine-tuning. Even though we have been getting a good result in the full parameter fine-tuning, we need to compare it with parameter-efficient fine-tuning (LoRA) with domain-specific clinical data. The LoRA configuration employs a rank ($r$) of 16 and an alpha value of 32, creating low-rank decomposition matrices that adapt only a small subset of the model's parameters. \textcolor{red}{In all models, LoRA adapters are applied to the attention projection layers (\texttt{q\_proj, k\_proj, v\_proj, o\_proj}) and the feed-forward projection layers (\texttt{gate\_proj, up\_proj, down\_proj}) with a dropout of 0.1 (\texttt{lora\_dropout=0.05}), while all remaining parameters are frozen.} LoRA is particularly effective when applied to the attention mechanism in transformer-based architectures.



LoRA \parencite{liao_dynamic_2025} is a prevalent parameter-efficient fine-tuning approach for large-scale models. The method proposes isolating the weight deltas from fine-tuning and approximating them using low-rank matrices. During inference, both the (frozen) pre-trained model and the low-rank deltas, referred to as \textit{adapters}, are forward-passed, and their activations are aggregated. Let $\bm{W} \in \mathbb{R}^{d \times k}$ be a pre-trained weight matrix. LoRA approximates the modifications from fine-tuning as:
\begin{equation}
\Delta \bm{W} \approx \bm{B} \bm{A}, \quad \bm{B} \in \mathbb{R}^{d \times r},\ \bm{A} \in \mathbb{R}^{r \times k},\ r \ll \min(d, k)
\end{equation}

Consequently, inference on an input $\bm{x} \in \mathbb{R}^{d}$ is represented as:

\begin{equation}
\bm{W} \bm{x} + \bm{B} \bm{A} \bm{x} \approx (\bm{W} + \Delta \bm{W}) \bm{x}
\end{equation}

Here, $\bm{A}$ and $\bm{B}$ are directly updated using backpropagation. LoRA is typically applied to the query and value matrices within the self-attention layers of the pre-trained transformer. To optimize for supervised tasks, an additional classification head is appended to the final layer of the model. 

\subsubsection{PEGASUS-XSUM Using LoRA Parameter-Efficient Fine-Tuning Performance}
\textcolor{red}{LoRA parameter-efficient fine-tuning enabled PEGASUS-XSUM to exceed both the zero-shot baseline and full parameter fine-tuning techniques. PEGASUS-XSUM employing LoRA demonstrated improved performance across all assessment measures}, as seen in Figure \ref{fig:Pegasus_lora_training}.  The LoRA method notably excelled, outperforming substantial parameter fine-tuning. This signifies that the system excels at preserving clinical terminology and bigram sequences. The BERTScore metrics demonstrated superior semantic comprehension, producing F1 scores of 0.8699 for validation and 0.8729 for the test. These indicated the highest semantic similarity scores among parameter-efficient algorithms. Figure \ref{fig:Pegasus_lora_loss} depicts the advancement of the instruction. The training and validation losses consistently diminished.  

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with LoRa training.png}
        \caption{PEGASUS-XSUM  LoRa Training}
        \label{fig:Pegasus_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with LoRa loss.png}
        \caption{PEGASUS-XSUM  LoRa Loss}
        \label{fig:Pegasus_lora_loss}
    \end{minipage}
\end{figure}

\subsubsection{T5 using LoRA Parameter-Efficient Fine-Tuning Performance}
T5's effectiveness using LoRA parameter-efficient fine-tuning shows a significant improvement in abstractive medical text summarization compared to its zero-shot and complete fine-tuning options. As shown in Table \ref{tab:model_performance_longtable}, LoRA fine-tuning improved its performance on the test set. The semantic understanding, assessed through BERTScore, was impressive with LoRA. The data, shown in the bar chart in Figure \ref{fig:t5_lora_training}, indicate T5's strong summarizing skills when properly tuned. Figure \ref{fig:t5_lora_loss} shows a consistent reduction in both training and validation loss over 5 epochs, with validation loss stabilizing around 2.2496, suggesting effective learning and strong generalization without significant overfitting. LoRA fine-tuning of T5 is a highly effective and efficient approach for this task, delivering performance that meets or surpasses full fine-tuning while utilizing the considerable parameter efficiency of LoRA. This T5 performs less compared to BART and PEGASUS-XSUM using LoRa results.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with LoRa training.png}
        \caption{T5 with LoRa Training}
        \label{fig:t5_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with LoRa loss.png}
        \caption{T5 with LoRa Loss}
        \label{fig:t5_lora_loss}
    \end{minipage}
\end{figure}
\subsubsection{FLAN-T5 Using LoRA Parameter-Efficient Fine-Tuning Performance}
As seen in Figure \ref{fig:Flan_lora_training}, Flan-T5 truly stands out when utilizing LoRA parameter-efficient fine-tuning for abstractive medical text summarization. Furthermore, the loss curve in Figure \ref{fig:Flan_lora_loss} illustrates a consistent and sharp decline in both training and validation loss over 5 epochs, with validation loss stabilizing at a very low 2.0386. This shows that Flan-T5 with LoRA is a strong and useful way to make high-quality, short, and semantically rich medical summaries. 
\vspace{-2}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with LoRa training.png}
        \caption{FLAN with LoRa Training}
        \label{fig:Flan_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with LoRa loss.png}
        \caption{FLAN with LoRa Loss}
        \label{fig:Flan_lora_loss}
    \end{minipage}
\end{figure}

\subsubsection{BART Using LoRA Parameter-Efficient Fine-Tuning Performance}
BART exhibited outstanding performance using LoRA parameter-efficient fine-tuning, attaining significant enhancements over both zero-shot baselines and full parameter fine-tuning methods while retaining a competitive position among the assessed models. Figure \ref{fig:Bart_lora_training} demonstrates that BART with LoRA achieved impressive results across all evaluation metrics. BART with LoRA achieved remarkable ROUGE-L scores, demonstrating significant structural coherence and exceeding the complete parameter fine-tuning performance. Figure \ref{fig:Bart_lora_loss} illustrates stable training dynamics, with training and validation losses smoothly converging, and signifying a reliable and effective learning path free from overfitting. Figure \ref{fig:Bart_lora_loss} showcases typical training dynamics, where validation losses steadily converge from initial values to final values of 0.2 and 0.9, respectively, highlighting effective learning advancement, but the validation loss does not converge as the training loss does.


\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with LoRa training.png}
        \caption{BART with LoRa Training}
        \label{fig:Bart_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with LoRa loss.png}
        \caption{BART with LoRa Loss}
        \label{fig:Bart_lora_loss}
    \end{minipage}
\end{figure}


\subsubsection{ Mistral-7B Fine-Tuning with LoRA}
Mistral-7B showcased remarkable performance via LoRA parameter-efficient fine-tuning, positioning itself among the leading models and highlighting the advanced abilities of large language models in summarizing clinical texts. As shown in Figure \ref{fig:Misteral_lora_training}, Mistral-7B with LoRA performed exceptionally well on all evaluation metrics, achieving ROUGE-1 scores of 0.6500 (validation) and 0.6507 (test), signifying notable advancements compared to conventional transformer models and highlighting the improved clinical comprehension abilities of large language models. Similarly, the Figure \ref{fig:Misteral_lora_loss} showcases typical training dynamics, where both training and validation losses steadily converge from initial values around 1.6 and 1.35 to final values of 0.2 and 0.9, respectively, highlighting effective learning advancement, but the validation loss does not converge as the training loss does. 

\textcolor{red}{Compared with traditional transformer models, the Mistral-7B with LoRA achieved higher scores across all assessment parameters. The improvement in clinical accuracy and semantic similarity is reflected in the increased BERTScore F1.} These results indicate that Mistral-7B with LoRA is an exceptional choice for clinical text summarization, as it provides state-of-the-art performance while maintaining the advantages of LoRA fine-tuning in terms of parameter efficacy. Consequently, it is the optimal choice for healthcare institutions that require both exceptional computational efficiency and accuracy.


\subsubsection{LLaMA-3-8B Fine-Tuning with LoRA}
\textcolor{red}{LLaMA-3-8B achieved the highest scores among all evaluated models and training methods using LoRA fine-tuning.} As shown in Figure \ref{fig:Llama_lora_training}, LLaMA-3-8B with LoRA achieved the best results on all evaluation metrics, with the highest ROUGE-1 scores of 0.6970 (validation) and 0.7022 (test), outperforming any other model and significantly exceeding both traditional transformers and other large language models. The model achieved impressive METEOR and ROUGE-2 scores (see Table \ref{tab:model_performance_longtable}), indicating it was very good at keeping important clinical terms and complex medical phrases, showing a strong understanding of clinical language structures. Figure \ref{fig:Llama_lora_loss} shows the loss and training dynamics of the training features that make LLaMA-3-8B stand out from all the other models that were tested. We've seen that it's interesting; LLaMA-3-8B had the lowest loss rates throughout the whole training cycle. Its training and validation losses slowly decreased. 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Misteral with LoRa training.png}
        \caption{Mistral-7B with LoRa Training}
        \label{fig:Misteral_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Misteral with LoRa loss.png}
        \caption{Mistral-7B with LoRa Loss}
        \label{fig:Misteral_lora_loss}
    \end{minipage}
\end{figure}
 \textcolor{red}{In general, LLaMA-3-8B with LoRA achieved higher scores compared to other evaluated models, including both transformers and Mistral-7B.} In comparison to the second-best model (Mistral-7B with LoRA), LLaMA-3-8B demonstrated enhancements of 7.9\% in ROUGE-1 (0.7022 vs 0.6507), 21.2\% in ROUGE-2 (0.5312 vs 0.4383), 11.5\% in ROUGE-L (0.6718 vs 0.6027), and 11.0\% in METEOR (0.6787 vs 0.6112). As a result, the highest result in BERTScore F1, from 0.9060 to 0.9180, signifies the peak semantic precision reached in this research. These great results, along with low training losses and the smart use of parameters by LoRA (only 0.8\% of total parameters), make LLaMA-3-8B with LoRA the best choice for summarizing clinical notes, offering high accuracy while being efficient enough for healthcare use.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Llama with LoRa training.png}
        \caption{LLaMA-3-8B with LoRa Training}
        \label{fig:Llama_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Llama with LoRa loss.png}
        \caption{LLaMA-3-8B with LoRa Loss}
        \label{fig:Llama_lora_loss}
    \end{minipage}
\end{figure}

\textcolor{red}{The LoRA approach achieved higher scores than the full parameter baseline across multiple dimensions. These results suggest that LoRA provides an effective training strategy that achieves comparable or improved performance while requiring fewer trainable parameters and computational resources for clinical text summarization applications.} Figure \ref{fig:placeholder}, shows the generated summary of one patient's record using fine-tuned LLAMA-3-8B.


\begin{longtable} [htbp]{|p{1.6cm}|p{1.3cm}|p{0.9cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.45cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}|}
\caption{Performance metrics for different models across validation and test stages, including loss, ROUGE scores, METEOR, and BERTScore metrics with LoRA Fine-Tuning.\label{tab:model_performance_longtable}} \\
\hline
\textbf{Model} & \textbf{Stage} & \textbf{Loss} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERT- \linebreak Score P} & \textbf{BERT- \linebreak Score R} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endfirsthead
\multicolumn{10}{c}{{\tablename\ \thetable{} ....Continued from previous page}} \\
\hline
\textbf{Model} & \textbf{Stage} & \textbf{Loss} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERT- \linebreak Score P} & \textbf{BERT- \linebreak Score  R} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endhead
\hline \multicolumn{10}{|r|}{{....Continued on next page}} \\ \hline
\endfoot
\hline
\endlastfoot
\multirow{\textbf{PEGASUS \linebreak -XSUM}} & Validation & 3.3920 & 0.4860 & 0.2122 & 0.4186 & 0.3831 & 0.8702 & 0.8696 & 0.8699 \\
\cline{2-10}
 & Test & 3.2557 & 0.4988 & 0.2312 & 0.4321 & 0.3993 & 0.8729 & 0.8729 & 0.8729 \\
\hline
\multirow{\textbf{T5}} & Validation & 2.2496 & 0.5135 & 0.2523 & 0.4546 & 0.4113 & 0.8641 & 0.8754 & 0.8696 \\
\cline{2-10}
 & Test & 2.1861 & 0.5313 & 0.2752 & 0.4742 & 0.4310 & 0.8685 & 0.8791 & 0.8737 \\
\hline
\multirow{\textbf{FLAN-T5}} & Validation & 2.1090 & 0.5327 & 0.2700 & 0.4733 & 0.4380 & 0.8695 & 0.8814 & 0.8752 \\
\cline{2-10}
 & Test & 2.0386 & 0.5517 & 0.2969 & 0.4934 & 0.4580 & 0.8740 & 0.8853 & 0.8795 \\
\hline
\multirow{\textbf{BART}} & Validation & 2.6070 & 0.5235 & 0.2459 & 0.4464 & 0.4228 & 0.8652 & 0.8787 & 0.8716 \\
\cline{2-10}
 & Test & 2.5197 & 0.5390 & 0.2678 & 0.4637 & 0.4368 & 0.8691 & 0.8821 & 0.8753 \\
\hline

\multirow{\textbf{Mistral-7B}} & Validation & 1.4029 & 0.6500 & 0.4333 & 0.5991 & 0.6067 & 0.8990 & 0.9108 & 0.9049 \\
\cline{2-10}
 & Test & 1.3824 & 0.6507 & 0.4383 & 0.6027 & 0.6112 & 0.9004 & 0.9117 & 0.9060 \\
\hline

\multirow{\textbf{LLaMA-3B}} & Validation & \textbf{0.7848} & \textbf{0.6970} & \textbf{0.5230} & \textbf{0.6659 }& \textbf{0.6709} & \textbf{0.9104} & \textbf{0.9224} & \textbf{0.9163 }\\
\cline{2-10}
 & Test & \textbf{0.9875} & \textbf{0.7022} & \textbf{0.5312} & \textbf{0.6718} & \textbf{0.6787} & \textbf{0.9124} & \textbf{0.9238} & \textbf{0.9180} \\
\hline
\end{longtable}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\linewidth]{summary sample.png}
    \caption{A sample generated summary of a clinical note. The figure illustrates an example of a discharge summary generated by our model, showing how abstractive strategies summarize key patient information while maintaining clinical accuracy.}
    \label{fig:placeholder}
\end{figure}
\subsection{Statistical Significance}




\subsubsection{Pairwise Significance Tests}
All pairwise comparisons between LoRA-based LLMs (LLaMA-3-8B, Mistral-7B) and traditional transformer models (BART, T5, FLAN-T5, PEGASUS) showed statistically significant differences (p < 0.001). Similarly, comparisons between LoRA fine-tuning and full parameter fine-tuning for the same base models demonstrated significant performance improvements with LoRA adaptation (p < 0.001).

These results confirm that the observed performance gains of parameter-efficient LoRA fine-tuning over full fine-tuning, and of large language models over traditional encoder-decoder transformers, are statistically robust and not attributable to random variation.
\subsection{ Model Performance Comparison  Across Different Training Approaches}
The radar chart presented in Figure \ref{fig:radarchar} offers an in-depth performance comparison of four distinct models assessed across various training and fine-tuning approaches. Throughout our experiments, LexRank, an extractive summarization method, consistently achieved the highest performance among the models evaluated. Similarly, the LLaMA-3-8B model, utilized in its pre-trained state without any fine-tuning, exhibited strong capabilities in summarization tasks. Moreover, the FLAN-T5 model, which underwent comprehensive fine-tuning, demonstrated remarkable adaptability and effectiveness, underscoring the benefits of extensive training. Additionally, the LLaMA-3-8B model enhanced with LoRA, a technique focused on parameter-efficient fine-tuning, also yielded impressive results, further illustrating its ability to optimize performance with minimal adjustments. This visualization not only highlights the comparative strengths of these methodologies but also serves as a valuable reference for researchers and practitioners seeking to select appropriate model architecture and training strategies for text summarization endeavors. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{comprehensive_radar_chart.png}
    \caption{Performance comparison of representative models across different training approaches using radar chart visualization. The chart displays seven evaluation metrics for four model categories: extractive methods, pre-trained models without fine-tuning, full fine-tuning, and PEFT with LoRA.}
    \label{fig:radarchar}
\end{figure}
\subsection{Evaluation Pipeline Using Graphical User Interface}

In addition to the modeling and evaluation pipeline, we developed a Graphical User Interface (GUI) using the Tkinter framework and Hugging Face to enable clinical and research professionals to interact with the summarization system intuitively. We pushed our final fine-tuned models into Hugging Face and connected with the GUI. The GUI supports multiple functionalities, allowing users to upload or browse medical text files, extract clinical named entities using a cTAKES-based pipeline, and generate summaries with an emphasis on sentences containing clinical entities (disease/disorder mentions, medication mentions, and procedures). The interface also integrates advanced NLP visualizations and analyses, including named entity recognition, chunk parsing, constituency and dependency parsing, and semantic role labeling, to provide interpretability and linguistic insight. These features are particularly useful for medical informatics researchers and clinicians who require transparency and explainability in text analytics workflows.

To enhance real-world usability, the system enables text input from URLs, direct pasting, or file upload. Processed text is analyzed and displayed in an interactive output window, making the GUI a practical extension of our summarization engine. By bridging backend summarization logic with a deployable frontend interface, this tool underscores the translational potential of our work and supports further exploration in clinical environments. A full implementation of the GUI, including cTAKES integration and summarization functionalities, has been made available as part of our supplementary materials to demonstrate the applied value of our contributions beyond experimental results.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.1\linewidth]{Graphical User Interface.png}
    \caption{Thinker-based Graphical User Interface for Summarization, illustrating the interactive layout where users can input clinical notes, configure summarization options, and view generated outputs.}
    \label{Comparison of results}
\end{figure}


\section{Discussion}
\textcolor{red}{In this research, we evaluate fine-tuning strategies, particularly LoRA, for transformer models in abstractive medical text summarization. Our experiments indicate that LoRA fine-tuning matches or exceeds full parameter fine-tuning performance in most evaluated conditions while consuming fewer computational resources. These findings provide evidence that challenges the assumption that more trainable parameters necessarily lead to higher performance.} We systematically evaluated graph-based extractive methods along with four traditional transformer models (BART, T5, Pegasus-xsum, and Flan-T5) and two large language models (LLaMA-3-8B and Mistral-7B). Our results clearly indicate the significant impact of fine-tuning on summarization quality in the specialized medical field. Unlike zero-shot approaches, which performed considerably worse in all areas, both complete fine-tuning and LoRA fine-tuning showed significant improvements, highlighting the necessity for adjustments tailored to the medical domain. Consequently, fine-tuning large language models using domain-specific datasets leads to better performance compared to the results obtained before applying fine-tuning, as evidenced by the findings in the previous results section.

This study is crucial for clinical decision support as it provides an efficient and accurate method for summarizing complex medical records. We utilized LoRA fine-tuning to develop a model capable of reading numerous patient discharge summaries and generating concise abstracts. This capability is particularly valuable for doctors who face an overwhelming amount of information and need to quickly identify the most important details to make informed decisions. Our approach not only accelerates and improves the quality of information retrieval, but it also makes this advanced technology accessible to hospitals and clinics with limited budgets and computing resources by significantly reducing the required processing power.

Additionally, the research clearly evaluated the notable computational advantages of LoRA. LoRA reduced the trainable parameters by approximately 99\% compared to full fine-tuning, leading to a significant decrease in training time (averaging more than five times quicker) and lowered GPU memory consumption. The enhancements in efficiency are crucial for real-world applications and ongoing development in healthcare environments with limited resources.

To provide a visual synthesis of these findings, Figure \ref{fig:summary} presents a comparative summary of model performance across the four summarization strategies: extractive, zero-shot, full fine-tuning, and LoRA. The histogram clearly illustrates the progressive improvements in ROUGE and BERTScore F1 metrics, with LoRA fine-tuning consistently outperforming other approaches. Notably, LLaMA-3-8B and Mistral-7B with LoRA demonstrate the highest performance across all evaluation dimensions, highlighting the effectiveness of parameter-efficient tuning in clinical summarization. Deploying  AI systems in medical environments necessitates not just precision but also effectiveness, user-friendliness, and mobility. Although complete model fine-tuning is efficient, it requires significant computing resources that typically exceed the budgets of community hospitals and clinics. 
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth,height=0.45\linewidth ]{summarypng.png}
    \captionPerformance Comparison of Summarization Models Across Different Fine-Tuning Strategies. This figure compares extractive (cTAKES), zero-shot, full fine-tuning, and LoRA fine-tuning approaches across multiple models (BART, T5, Pegasus-XSUM, Flan-T5, LLaMA-3-8B, Mistral-7B) using ROUGE and BERTScore F1 metrics, highlighting LoRA’s consistent efficiency.}
    \label{fig:summary}
\end{figure}

Beyond this, we thoroughly examined how our research questions are addressed. We analyzed the methodologies employed and highlighted the significant findings that provide insight into each question posed. \\ 
\textbf{RQ1:}  Across all architectures, fine-tuning substantially improved summarization quality compared to zero-shot inference. LLMs with LoRA consistently achieved the highest scores (for instance, LLaMA-3-8B with ROUGE-1: 0.7022, BERTScore F1: 0.9180), outperforming both transformers with LoRA and full parameter fine-tuning. This finding indicates that performance gains are not solely a function of model size but also of efficient adaptation strategies. The ranking observed that LLMs with LoRA significantly exceeded extractive baselines, showing that neural abstractive techniques offer essential benefits in clinical comprehension, semantic retention, and content generation that rule-based extractive strategies fail to deliver.\\
\textbf{RQ2:}  While extractive methods such as LexRank and TextRank are computationally inexpensive, their performance remained limited (example: ROUGE-1 $\approx$ 0.25), reflecting poor alignment with abstractive gold summaries. In contrast, neural abstractive approaches produced semantically richer and more coherent summaries, even under zero-shot conditions. Fine-tuned models amplified this advantage, demonstrating that abstractive approaches are essential for capturing clinical reasoning and narrative flow—capabilities unattainable with sentence ranking alone.

\textbf{RQ3:} LoRA achieves superior results with only 0.8-4.8\% trainable parameters, while also significantly outperforming both computationally lightweight extractive methods and resource-intensive complete fine-tuning by striking the optimal equilibrium between performance and computational efficiency. LoRA achieves 2.6× better performance than extractive methods with moderate computational requirements and 62-74\% GPU memory reduction compared to full fine-tuning, in contrast to extractive algorithms, which require minimal resources but deliver inadequate clinical performance (ROUGE-1: ~0.25). This makes high-performance clinical summarization accessible to resource-constrained healthcare areas. 

\textbf{RQ4:}  Zero-shot summarization using structured prompts exhibits varying effectiveness across different model architectures, with large language models (LLMs) demonstrating a superior baseline understanding of clinical contexts compared to traditional transformers. Our tailored prompting strategies ranged from straightforward task descriptions for conventional models to complex system-user prompt combinations for LLMs, facilitating zero-shot performance. Notably, LLaMA-3-8B and Mistral-7B significantly surpassed traditional transformers and extractive baselines. The use of structured prompts not only enhanced factual coherence but also reduced hallucinations by constraining output formats and aligning models with clinical terminology rules. Subsequent fine-tuning led to substantial improvements in clinical accuracy and coherence, with LoRA fine-tuning yielding the most remarkable result in LLaMA-3-8B. The improvements in BERTScore from zero-shot to LoRA fine-tuning reflect enhanced semantic accuracy and reduced hallucination rates. This underscores how fine-tuning builds upon the foundation established by structured prompts to achieve superior factual consistency and medical coherence, which are essential for clinical applications.
\textcolor{red}{
\subsection{Summary of Key Findings}
This study yields three principal findings. First, LLaMA-3-8B with LoRA achieves the highest summarization performance (ROUGE-1: 0.7022, BERTScore F1: 0.9180), surpassing all other models and training strategies. Second, LoRA consistently outperforms full fine-tuning across all architectures while requiring only 0.8\% of trainable parameters. Third, the performance ranking follows a consistent pattern: LLMs with LoRA exceed transformers with LoRA, which outperform full fine-tuning, followed by zero-shot inference, with extractive methods achieving the lowest scores.\\
\textbf{Takeaways:} (i) Parameter-efficient fine-tuning provides both computational efficiency and improved summarization quality for clinical text. (ii) LLMs demonstrate superior clinical language understanding compared to traditional transformers. (iii) LoRA reduces GPU memory requirements by 62--74\% and training time by approximately 5-times, making clinical AI accessible to healthcare organizations with limited computational resources.}

\subsection{Comparison with Previous Works }

As seen from Table \ref{tab:big_comparison_limitations_long}, previous works in clinical text summarization have often been constrained by their focus on singular models, limited comparative scopes, and a general neglect of parameter-efficient methods like LoRA, frequently failing to report crucial computational metrics. Graph-based extractive methods have been looked at, but they usually don't perform well because they struggle to capture the more complex way humans write clinical summaries. \textcolor{red}{In contrast, our work provides a benchmark across multiple models and strategies, evaluating both traditional transformers and large language models in zero-shot, full fine-tuning, and LoRA adaptation settings. This approach empirically demonstrates LoRA's ability to achieve comparable or improved abstractive summarization quality while reducing computational requirements, with our models achieving higher ROUGE and METEOR scores than previously reported results on the MIMIC-IV dataset.} \\


Beyond the overall improvements, we also looked at scenarios where our approach works especially well and where it struggles. Compared with earlier models, for example, FLAN-T5 and BART \parencite{van_veen_clinical_2023}, which dropped sharply in performance when facing domain gaps, our LLaMA-3-8B with LoRA model showed enhanced performance (ROUGE-2: 0.531 versus 0.11). This suggests that LoRA helps capture clinically meaningful language even when the surface wording differs and excels previous works. We also checked that extractive pipelines such as SBERT + PageRank \parencite{aftiss_biomdsum_2024} could score well on certain semantic metrics but often fail to maintain consistent summary structure. In contrast, our models achieved a stronger balance across both lexical and semantic measures. Still, we found limitations: ambiguous abbreviations and sections outside discharge summaries remain challenging, and the models can show deficiencies. Overall, these cases highlight both the clear advantages of LoRA and the areas that still need more work in the clinical summarization domain.

\begin{longtable}{p{2.1cm}p{1.8cm}p{2.2cm}p{0.55cm}p{0.55cm}p{0.55cm}p{0.55cm}p{0.65cm}p{3.5cm}}
\caption{ Compares our proposed LoRA-enhanced models with some of the previous clinical text summarization methods. The table shows that our models consistently achieve higher scores across evaluation metrics while also being more computationally efficient. This demonstrates that our approach not only outperforms prior methods but also provides a practical solution for resource-constrained clinical settings.}\label{tab:big_comparison_limitations_long} \\
\toprule
\textbf{Authors} & \textbf{Dataset} & \textbf{Approach} & \textbf{R1} & \textbf{R2} & \textbf{RL} & \textbf{M} & \textbf{BF} & \textbf{Limitations}\\
\midrule
\endfirsthead


\toprule
\bottomrule
\endlastfoot
The study in \parencite{sun_generative_2025} & MIMIC-
IV & Llama-3,GPT-4 & NA & NA & NA & NA & 0.81 & Hallucination risk  \\
The study in \parencite{zhao_improving_2024} & MIMIC-
III & Starling-LM, Llama-3,GPT3.5 & NA & NA & 0.2903 & NA & 0.4691 &Small test set; no automatic metric analysis  \\
The study in \parencite{make_2024} & MIMIC-
IV & Seq2Seq & 0.3016 & 0.773 & 0.284 & NA & NA &Small test set; Complex pipeline; limited to BHC/DI sections  \\
The study in\parencite{van_veen_clinical_2023} & MIMIC-
IV & FLAN-T5,BART(zero- shot) & 0.27 & 0.11 & 0.25 & 0.17 & 0.81 & Domain gap
affects lexical overlap  \\
The study in \parencite{wei_finetuned_2022} & Clinical corpora & Instruction tuning (FLAN-T5) & 0.52 & 0.30 & 0.50 & 0.33 & 0.91 & Limited clinical-specific evaluation  \\


The study in \parencite{hu_lora_2022} & Clinical corpora & GPT & 0.532 & 0.292 & 0.45 & NA & NA & Limited clinical-specific evaluations. \\
Lewis \textit{et al.}~\parencite{lewis_bart_2019} & MIMIC, OpenI & Denoising autoencoder (BART) & 0.4416 & 0.2128 & 0.4090 & NA & NA & Domain shift risks. \\


The study in \parencite{aftiss_biomdsum_2024} & Cochrane, MS$^2$ & SBERT+K-means+PageRank + LED & 0.294 & 0.065 & 0.183 & 0.859 & NA & Domain-specific terms underrepresented; SBERT dependence. \\

The study in \parencite{song_hybrid_2021} & CORD-19 & Seq2Seq + TextRank with $\alpha$-weighting & 0.301 & 0.077 & 0.284 & NA & NA & Requires tuning $\alpha$ for coherence. \\

The study in \parencite{nguyen_hybrid_2021} & MEDIQA-AnS & TF-IDF, LexRank + BART & 0.300 & 0.220 & 0.250 & NA & NA & Depends on filtering thresholds and token limits. \\
\hline

\addlinespace
\multicolumn{9}{l}{\textbf{Our Proposed Method}}\\
LLaMA-3-8B + LoRA & MIMIC-IV v1.2 & Abstractive; LLaMA-3-8B + LoRA & \textbf{0.702} & \textbf{0.531} & \textbf{0.671} & \textbf{0.678} & \textbf{0.918} & Highest overall; needs LLM infra but only \textasciitilde0.8\% trainable params.  \\
Mistral-7B + LoRA & MIMIC-IV v1.2 & Abstractive; Mistral-7B + LoRA & 0.650 & 0.438 & 0.602 & 0.6112 & 0.906 & Slightly lower than LLaMA-3; higher VRAM than transformers.  \\
\end{longtable}


\subsection{Practical Implications}
The findings of this study carry significant practical implications for the development and implementation of abstractive medical text summarization systems, especially in resource-constrained healthcare environments. Firstly, the efficacy of LoRA addresses the challenges faced in resource-limited settings. Many hospitals, clinics, and smaller research laboratories often lack access to advanced GPUs and extensive computational clusters required for the comprehensive fine-tuning of models with billions of parameters. Secondly, LoRA facilitates rapid iteration and deployment. In the fast-evolving domain of medicine, new research, clinical guidelines, and patient data are continually emerging. The ability to quickly fine-tune models based on new datasets or adjust to shifting clinical requirements is crucial. Full fine-tuning, with its protracted training durations, can become a significant bottleneck. By significantly reducing training time, LoRA enables much quicker experimental cycles. Finally, LoRA greatly enhances the concept of model portability. Since LoRA adapters are relatively small megabytes, they are highly portable. This portability simplifies the process of sharing fine-tuned models with collaborators, distributing them as part of software packages, and deploying them on edge devices or in the cloud.

\subsection{Theoretical Implications}
Our research offers a significant theoretical contribution by challenging the long-standing assumption in deep learning that an increased number of trainable parameters leads to enhanced performance. We provide empirical evidence demonstrating that LoRA fine-tuning, which adjusts a smaller subset of a model's parameters, not only matches but often outperforms the results achieved through full fine-tuning. This suggests a paradigm shift: targeted adaptation can be more effective than brute-force modification, effectively leveraging the extensive knowledge embedded in the pre-trained model's fixed weights. Theoretically, the approach discussed in this study serves as a form of implicit regularization, helping to prevent overfitting to the fine-tuning dataset while alleviating issues related to catastrophic forgetting. Our findings yield new insights into how large pre-trained models assimilate task-specific knowledge. Rather than learning a new task from the ground up, fine-tuning a large language model appears to create "low-rank updates" that map the existing latent space of the model to a new task.

\subsection{Scalability and Practical Applications}
Another important area of research focused on the scalability of LoRA-enhanced models. Practical applications in healthcare require the model to scale to larger datasets and more complicated medical queries without increasing processing resources. Even as the dataset size and complexity increased, the LoRA-enhanced model maintained good performance while scaling. Real-world AI models must efficiently analyze massive medical data; therefore, scalability is crucial. The study has practical applications in clinical decision support systems, patient education, and automated medical recording. Reliable and contextually appropriate responses from the LoRA-enhanced model help healthcare workers make educated decisions, improve patient outcomes, and streamline administrative duties. 

\subsection{Limitations and Future Research Directions}
In this study, our dataset is comprised exclusively of discharge summaries written in English, which may limit the generalizability to outpatient settings and non-English clinical notes. While this research offers valuable insights into the efficacy of LoRA for abstractive medical text summarization, it is essential to acknowledge several limitations that should be considered in future studies. In addition to LoRA, other PEFT techniques, such as adapters, prompt tuning, and prefix tuning, are worth exploring. Given the rapid advancement in the field of large language models, with some now encompassing hundreds of billions or even trillions of parameters, future research could investigate the application of LoRA to even more extensive models to further enhance performance and efficiency. It is also important to highlight a common limitation in summarization studies regarding the reliance on automatic evaluation metrics such as ROUGE, METEOR, and BERTScore. These metrics may not effectively assess the accuracy, clinical relevance, brevity, and overall clarity of the summaries for healthcare professionals. A summary achieving high ROUGE scores may still contain factual inaccuracies or omit critical information, which could have serious consequences in a medical setting. Therefore, human judgment is crucial for evaluating medical summaries. Future efforts should include a professional clinical review to ensure that the summaries are accurate and applicable in real-world scenarios.
\section{Conclusion}
\textcolor{red}{This research evaluates LoRA parameter-efficient fine-tuning for clinical text summarization and provides evidence that increased trainable parameters do not necessarily correlate with improved performance.} By systematically evaluating graph-based extractive and 20 tests spanning six architectures, traditional transformers (BART, T5, PEGASUS-XSUM, FLAN-T5), and LLMs  (Mistral-7B, LLaMA-3-8B), we can demonstrate that parameter efficiency improves performance rather than degrades it. FLAN-T5 and BART with LoRA are emerging as the top performers among traditional transformers, and LLaMA-3-8B with LoRA is achieving the highest overall performance in the entire study, achieving strong performance in clinical text summarization. From this study, we confirmed that LoRA consistently outperformed both zero-shot inference and full parameter fine-tuning across all evaluated architectures. Based on this wide-ranging experiment, effectiveness suggests a specific performance ranking: LLMs utilizing LoRA surpass traditional transformers with LoRA, which outperform full fine-tuning, which is superior to zero-shot neural techniques, which significantly outperform extractive methods. LLM models demonstrated good understanding of clinical information, with their zero-shot results being better than traditional graph-based extractive methods and similar to fine-tuned classical transformers, indicating that their clinical reasoning is due to a lot of pre-training. The computational efficiency achievements are equally remarkable, with LoRA enabling  GPU memory reduction and storage reduction, transforming clinical AI from a resource-intensive endeavor to an accessible technology for healthcare organizations of all sizes. This study represents a change in clinical AI, shifting from resource-intensive methods to intelligent adaptation methods, extending beyond clinical text summarizing into the wider area of domain-specific AI adaptation. The success of LoRA, which reached its goals by concentrating on learning, maintaining knowledge, and using implicit regularization, establishes parameter-efficient fine-tuning as the new standard for specialized AI applications that need to perform well and can be used in real-world settings. Advanced clinical NLP is accessible to healthcare organizations worldwide due to its strong performance, computational efficiency, and deployment flexibility. 

\section*{Abbreviations}
\begin{flushleft}
\begin{longtable}{l l}
\textbf{AI} \dotfill & Artificial Intelligence \\
\textbf{BART} \dotfill & Bidirectional and Auto-Regressive Transformers \\
\textbf{BERTScore} \dotfill & BERT-based evaluation score for text similarity \\
\textbf{BHC/DI} \dotfill & Behavioral Health / Discharge Instructions \\
\textbf{CITI} \dotfill & Collaborative Institutional Training Initiative \\
\textbf{cTAKES} \dotfill & Clinical Text Analysis and Knowledge Extraction System \\
\textbf{EHRs} \dotfill & Electronic Health Records \\
\textbf{FLAN-T5} \dotfill & Fine-tuned Language Net T5 \\
\textbf{GPT} \dotfill & Generative Pre-trained Transformer \\
\textbf{ICD-10} \dotfill & International Classification of Diseases, 10th Revision \\
\textbf{LED} \dotfill & Longformer Encoder-Decoder \\
\textbf{LLaMA-3-8B} \dotfill & Large Language Model Meta AI (8B parameters) \\
\textbf{LLM} \dotfill & Large Language Model \\
\textbf{LoRA} \dotfill & Low-Rank Adaptation \\
\textbf{MIMIC} \dotfill & Medical Information Mart for Intensive Care \\
\textbf{Mistral-7B} \dotfill & Large Language Model (7B parameters) \\
\textbf{PEGASUS-XSUM} \dotfill & Pre-training with Extracted Gap-sentences for Abstractive Summarization \\
\textbf{PEFT} \dotfill & Parameter-Efficient Fine-Tuning \\
\textbf{PHI} \dotfill & Protected Health Information \\
\textbf{PLMs} \dotfill & Pretrained Language Models \\
\textbf{SBERT} \dotfill & Sentence-BERT \\
\textbf{T5} \dotfill & Text-to-Text Transfer Transformer \\
\textbf{TF-IDF} \dotfill & Term Frequency–Inverse Document Frequency \\
\textbf{UMLS} \dotfill & Unified Medical Language System 
\end{longtable}
\end{flushleft}

\vspace{-4.5em}

\section*{Acknowledgments} This work is partly supported by the Finnish Research Council Profi 7 Hybrid Intelligence, which is gratefully acknowledged. The first author is supported by a doctoral grant from the University of Oulu Graduate School (UniOGS). The authors also gratefully acknowledge the computational resources provided by CSC – IT Center for Science, Finland, which made this research possible. The experiments were conducted using Puhti and Mahti supercomputing clusters.

\section*{Declaration of Competing Interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
\section*{Data Availability}
We used data from the MIMIC-IV database, accessed under ethical approval and data use agreements. Due to privacy regulations and ethical considerations inherent to clinical data, the dataset cannot be made publicly available. 
\section*{Model Availability}
The fine-tuned models used in this study are publicly available in \href{https://huggingface.co/Aleka12} {huggingface}  to ensure reproducibility and support future research.

\printbibliography
\end{document}