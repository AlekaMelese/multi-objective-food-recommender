\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb, bm, mathtools}
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage{caption}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[a4paper, margin=1in]{geometry}
%\usepackage{authblk}
\usepackage[colorlinks=true, citecolor=blue, linkcolor=red, urlcolor=blue]{hyperref}
\usepackage{newunicodechar}
\newunicodechar{⋆}{$\star$}

\usepackage{xcolor}  % For color
%\usepackage{cite}  % For IEEE style citations
%\bibliographystyle{ieeetr}
%\let\oldcite\parencite  % Save the original \parencite command
\renewcommand{\parencite}[1]{\textcolor{blue}{\oldcite{#1}}}  % Redefine \parencite to include color

% Biblatex with APA
\usepackage{xcolor}  % For color
\usepackage[backend=biber, style=apa, sorting=nyt]{biblatex}
\DeclareLanguageMapping{english}{english-apa}
\addbibresource{references.bib}
\addbibresource{Rabuil.bib}
% ---- Make all citations blue automatically ----
\AtBeginDocument{%
  \DeclareCiteCommand{\parencite}[\mkbibparens]
    {\textcolor{blue}{\usebibmacro{cite:init}\usebibmacro{prenote}}}
    {\textcolor{blue}{\usebibmacro{citeindex}\usebibmacro{cite}}}
    {\multicitedelim}
    {\usebibmacro{postnote}}%
}
% Force all citations to be parenthetical
%\let\parencite\parencite

\captionsetup[table]{labelformat=simple, labelsep=colon, textfont=normalfont, labelfont=bf}
\captionsetup[figure]{labelformat=simple, labelsep=colon, textfont=normalfont, labelfont=bf}

\title{\textbf{PEFT: Parameter-Efficient Clinical Note Summarization: Evaluating Fine-Tuning Strategies Across Transformers and Large Language Models}}



%\author[1]{Aleka Melese Ayalew*}
%\author[1] {Md Rabiul Hasan}
%\author[1]{Tapio Seppänen}
%\affil[1]{Center for Machine Vision and Signal Processing, University of Oulu, 90014, %Finland\\
%\textit{*Corresponding Author Email: \href{mailto:aleka.melese@oulu.fi}{Aleka.Melese@oulu.fi}}}
\date{} % Removes the default date
\begin{document}
\maketitle
%\textit{\textbf{Institutional Email Addresses:} Aleka.Ayalew@oulu.fi (Aleka Melese Ayalew %),MdRabiul.Hasan@oulu.fi (Md Rabiul Hasan), Tapio.Seppanen@oulu.fi (Tapio Seppänen), and 
% Mourad.Oussalah@oulu.fi (Mourad
%Oussalah)}
%\vspace{1.5cm}
\begin {abstract}

Analyzing and summarizing critical information from electronic health records places a significant burden on clinicians. While transformer models and Large Language Models (LLMs) are effective for this task, the computational cost and stringent performance requirements in healthcare present major challenges. This work addresses these challenges by evaluating parameter-efficient fine-tuning (PEFT) techniques for abstractive clinical note summarization.
\\
\\
We conducted a comprehensive evaluation on the MIMIC-IV-Ext-BHC v1.2.0 dataset, containing 270,033 clinical notes, comparing four transformer models (T5, PEGASUS-XSUM, BART, and FLAN-T5) and two LLMs (Mistral-7B and LLaMA-3-8B) across zero-shot, full fine-tuning, and LoRA fine-tuning strategies. Additionally, we integrated LexRank, clinical NLP functionalities from cTAKES, and domain-specific semantic similarity techniques into graph-based extractive summarization. \textcolor{red}{Our experiments demonstrated that LoRA achieved higher scores than full fine-tuning on key metrics while reducing the computational burden.} Specifically, our LLaMA-3-8B model, fine-tuned with LoRA, achieved a ROUGE-1 score of 0.7022, ROUGE-2 of 0.5312, ROUGE-L of 0.6718, METEOR of 0.6787, and BERTScore F1 of 0.9180 on the test set. \textcolor{red}{This represents a 2.6$\times$ improvement in ROUGE-1 over the leading extractive baseline, though direct comparison is limited by fundamental differences between extractive and abstractive approaches. Notably, these results were accomplished using approximately 99\% fewer trainable parameters than full fine-tuning.} Our research demonstrates that LoRA provides an effective and scalable solution for deploying sophisticated language models in resource-constrained medical environments, offering a robust framework to enhance healthcare professionals' information access and accelerate clinical decision-making.\\


\textbf{Keywords:} Clinical Note Summarization, LLMs, LoRA, Parameter-Efficient Fine-tuning, Transformer Models, cTAKES

\end{abstract}

%\tableofcontents 
%\newpage

\section{Introduction}
The use of electronic health records (EHRs) has rapidly evolved over the past decade, transforming how health data is managed globally. In high-income countries, EHR adoption surpasses 95\% in most hospitals, while lower-resource regions are actively improving their digital health infrastructure \parencite{hijazi_exploring_2025}. Each patient interaction generates various free-text documents, including diagnostic notes, progress reports, radiological impressions, consultation findings, operation records, pathology results, and discharge summaries. This widespread EHR implementation has increased clinical documentation burdens, with physicians often spending up to two hours on documentation for every hour of patient care, and nurses dedicating as much as 60\% of their time to record-keeping. This situation contributes to a high perceived workload and clinician burnout \parencite{kobayashi_factors_2025}. Clinicians and healthcare administrators are frequently overwhelmed by the sheer volume of data, which complicates the extraction of vital insights, the integration of recent medical advancements, and the thorough evaluation of patient histories. These challenges can result in delayed diagnoses, ineffective treatment options, and slow communication of medical information, ultimately impacting patient outcomes and professional satisfaction.\\

Automatic summarization offers a practical solution to the challenges posed by extensive clinical documentation. Manual summarization is prone to bias, errors, and high costs \parencite{supriyono_survey_2024}. By efficiently extracting essential information from lengthy records, automated summarization can support timely, evidence-based decision-making, reduce the workload of healthcare professionals, and facilitate research by transforming complex data into clear and actionable insights \parencite{maleki_varnosfaderani_role_2024}.\\


Recent advancements in transformer-based language models and large language models have significantly enhanced their ability to summarize text and perform a variety of natural language processing tasks. Traditional transformer models such as BART (Bidirectional and Auto-Regressive Transformers), T5 (Text-to-Text Transfer Transformer), PEGASUS-XSUM (Pre-training with Extracted Gap-Sentences for Abstractive Summarization), and FLAN-T5 (Fine-tuned Language Net-T5) have demonstrated strong performance in general-domain summarization tasks. Furthermore, large language models like Mistral-7B and LLaMA-3-8B have improved our capacity to understand and reason through complex medical scenarios, offering innovative approaches to interpreting clinical content.\\

However, the medical field presents unique challenges distinct from general text processing applications. Clinical summarization techniques generally fall into two categories. \textit{Extractive summarization} selects the most relevant sentences or passages from source texts without significantly altering the text. This preserves the original terminology and evidential basis, but it can introduce readability and flow issues.  For instance, \parencite{ando_exploring_2022} demonstrated that extractive methods often suffer from coherence problems when concatenated sentences fail to form a natural narrative, limiting their usefulness in complex clinical documentation. By contrast, \textit{abstractive summarization} seeks to generate new sentences that capture the essential meaning of the original text. While this allows greater flexibility, abstractive methods pose interpretability challenges due to their reliance on opaque neural architectures \parencite{shakil_abstractive_2024}. At the same time, \parencite{almohaimeed_abstractive_2025} emphasized their clinical utility, showing that abstractive approaches can synthesize information from multiple sources and provide patient-friendly explanations. Taken together, these perspectives suggest that while extractive methods preserve accuracy and traceability, abstractive methods offer greater adaptability and clinical value, albeit at the cost of transparency. Clinical note summarization presents several domain-specific challenges that hinder the straightforward use of general-purpose language models. Pre-trained language models (PLMs), which are trained on general corpora, often struggle to handle specialized medical language and the structure of medical text \parencite{luo_pre-trained_2024}.\\


By connecting the burden of documentation to the complexity of clinical areas, our study is designed to evaluate how extractive and abstractive summarization, combined with parameter-efficient fine-tuning techniques, can effectively address these challenges. Some of these challenges are described as follows.  \textbf{(i)} Medical texts employ a significant amount of technical language, abbreviations, and specialized terms that may not be well-represented in general pre-training corpora, such as proprietary drug names and diagnostic codes like ICD-10 \parencite{luo_pre-trained_2024}. Clinical notes often have medication names, diagnostic codes, anatomical terms, and procedure descriptions that require specialized training to interpret \parencite{yang_tcm-gpt_2024}. \textbf{(ii)} Clinical documents contain dense medical information with complex relationships between symptoms, diagnoses, treatments, and outcomes \parencite{he_pretraining-based_2025}. \textbf{(iii)} Clinical applications require high accuracy, unlike general text summarization, because they could affect patient safety and care quality \parencite{van_veen_clinical_2023}. Inaccurate or incomplete medical summaries could lead to misdiagnosis, inappropriate treatment decisions, or adverse patient outcomes \parencite{rodziewicz_medical_2025}. \textbf{(iv)} Clinical applications follow strict privacy laws like the Health Insurance Portability and Accountability Act (HIPAA), which makes it difficult to obtain vast clinical datasets for training and testing models \parencite{khalid_privacy-preserving_2023} \parencite{yadav_data_2023}. \textbf{(v)} Finally, limited computational resources pose a real challenge for many healthcare organizations. Community hospitals often lack the resources needed for full fine-tuning of transformer models with hundreds of millions of parameters \parencite{nerella_transformers_2024}. Together, these challenges motivate our study, which systematically evaluates both extractive and abstractive methods, incorporating parameter-efficient approaches to deliver accurate and computationally feasible summarization solutions for clinical contexts.\\

This study aims to systematically evaluate parameter-efficient fine-tuning (PEFT) approaches across both traditional transformers and large language models in the clinical summarization domain. By doing so, it addresses the lack of comparative, resource-conscious, and clinically relevant evaluations identified in prior research.  Some of the issues highlighted in the prior studies include: \textbf{(i)} Most studies examine only a small number of models or fine-tuning approaches. This limitation restricts the ability to make well-informed decisions regarding the most suitable model for practical application based on the available data \parencite{alves_benchmarking_2025}. \textbf{(ii) Insufficient Exploration of Parameter-Efficient Methods:} There is a lack of rigorous examination of parameter-efficient techniques, such as LoRA or adapters, in the clinical setting. Research often defaults to full fine-tuning methods that require substantial computational resources \parencite{liao_parameter-efficient_2023}, but their systematic evaluation across both traditional transformers and large language models in clinical contexts remains limited. \textbf{(iii)} Prior work has not systematically applied PEFT techniques to transformer models in clinical contexts. Our work demonstrates that transformer models with PEFT yield promising results, which is beneficial for further research. \textbf{(iv)} When models are only tested on a small number of datasets and metrics, like using only one automatic evaluation metric, the results are not valid outside of the study and cannot be reliably generalized. \textbf{(v)} Previous research often evaluates only a limited selection of models, ranging from specialized transformers to general-purpose large language models, which does not provide comprehensive clinical insights  \parencite{van_yperen_lats_2025}. 

\subsection{The Promise of Parameter-Efficient Fine-Tuning}
Natural Language Processing (NLP) has undergone a transformative shift due to LLMs. Their capabilities include text generation, language translation, question answering, and code generation. Performance improvements have been noticeable after scaling these models, which can include billions or even trillions of parameters. Nevertheless, this rapid growth is increasingly impractical for several uses due to numerous issues including memory constraints, computational efficiency, and scalability. Traditional fine-tuning techniques involve adjusting the entire network or a subset of it to transfer knowledge to downstream tasks \parencite{pratap_fine_2025}. However, this procedure frequently necessitates duplicating and updating the weights of the model for every task, which requires a significant amount of computational and memory resources. Furthermore, the use of such methods can result in catastrophic forgetting, which is a situation in which the model has lost previously learned knowledge when it is fine-tuned for new tasks. Large models that have been pre-trained have a difficult time maintaining their capacity to generalize to out-of-distribution tasks, even though they have a solid starting point. In addition to this, the process is made even more complicated by the possibility of overfitting target datasets \parencite{tu_overview_2024}. \\

Through a carefully designed multi-pronged experimental approach, our research directly addresses the problems with traditional fine-tuning and domain-specific clinical note summarization. First, we evaluate six transformer architectures: BART, T5, PEGASUS-XSUM, FLAN-T5, Mistral-7B, and LLaMA-3-8B across three fine-tuning regimes: zero-shot, full parameter fine-tuning, and parameter-efficient LoRA adaptation. This design yields a model-strategy evaluation matrix. This results in a matrix of 20 experimental conditions, \textcolor{red}{enabling systematic cross-model comparisons across training strategies.} Second, we place particular emphasis on rigorous benchmarking of parameter-efficient methods by quantifying LoRA's trainable parameter count, memory footprint, and wall-time relative to accuracy, thereby addressing the prior neglect of efficient approaches in the literature. Our evaluation protocol is multi-dimensional: in addition to standard ROUGE-1/2/L scores, we employ METEOR and BERTScore (F1, Recall, and Precision) to comprehensively assess lexical overlap, semantic fidelity, and paraphrastic quality, while stratified bootstrap confidence intervals furnish statistical robustness. This study addresses these gaps through a comprehensive evaluation of fine-tuning approaches for clinical text summaries. This study makes many important contributions to the fields of medical informatics and natural language processing: 
\\

\textbf{Methodological:} 
\begin{itemize}
    \item We conduct a systematic and exhaustive comparison of zero-shot, full fine-tuning, and LoRA strategies across a diverse set of state-of-the-art transformer models and large language models for abstractive medical text summarization.
    \item We propose and compare the clinical text summarization effectiveness of graph-based extractive summarization and traditional transformers (BART, T5, PEGASUS-XSUM, FLAN-T5) to that of modern large language models (Mistral-7B, LLaMA-3-8B) using different training methods with prompt engineering. This includes an initial assessment of graph-based extractive summarization as a baseline, offering a holistic perspective.
\end{itemize}
\textbf{Empirical:} 
\begin{itemize}
    \item \textcolor{red}{The proposed method achieves higher evaluation scores in abstractive medical text summaries, particularly with LLMs. LoRA achieves comparable or higher scores than full fine-tuning while using substantially fewer computational resources, facilitating deployment in clinical contexts.}
    \item We validate the effectiveness and robustness of our approach. At the same time, we provide detailed model-specific analysis, identifying which transformer architectures and LLMs are particularly well-suited for efficient adaptation using LoRA in the medical domain.
\end{itemize}
\textbf{Practical:} 
\begin{itemize}
    \item \textcolor{red}{The proposed fine-tuned models achieved higher scores than previously reported approaches on abstractive clinical note summarization, with consistent results on both validation (ROUGE-1: 0.6970, ROUGE-L: 0.6659, BERTScore F1: 0.9163) and test sets (ROUGE-1: 0.7022, ROUGE-L: 0.6718, BERTScore F1: 0.9180), while reducing computational resources.}
\end{itemize}



The rest of the paper is organized as follows: Section 2 presents related work. It also defines summarization, fine-tuning techniques, and the role of LLMs in healthcare. The methods, dataset, data preprocessing, and prompt strategies are defined in Section 3. Section 4 defines evaluation techniques. Section 5 describes implementation details, including model configuration, experimental design, and fine-tuning strategies. Section 6 presents experimental results and statistical significance analyses. Section 7 discusses limitations and future research directions. Section 8 concludes the paper. 

\section{Related Work}
\subsection{Extractive Summarization}

Extractive summarization has been commonly applied to MIMIC-III and MIMIC-IV clinical datasets due to their abundant structured and unstructured notes. On MIMIC-III, \parencite{wang_wonder_2024} developed an extractive summarization with semantic enhancement through a topic-injection-based BERT model. The study measured semantic similarity between two pieces of text. \parencite{saeed_sumex_2024} employs a hybrid framework for semantic textual similarity and explanation generation.  Recent research work by  \parencite{holm_local_2023} has developed FactReranker, a fact-based reranking method. Domain-specific information ensures factual consistency in these approaches. Extractive models are simple and fast, but these studies reveal that high-quality clinical summaries require more abstractive, semantically aware production processes. In summary, extractive summarization approaches remain attractive for their simplicity and efficiency, but these techniques fail to capture the coherence and interpretability required for clinical use. This limitation explains why subsequent work has shifted attention toward abstractive methods, particularly zero-shot prompting and fine-tuned neural models, which can generate more fluent summaries.
 

\subsection{Abstractive Summarization}
\subsubsection{Zero-Shot Methods}
Zero-shot summarization has attracted attention because it avoids the cost of task-specific training, but its effectiveness in clinical domains remains uneven.
For discharge summaries \parencite{van_veen_clinical_2023} reported only modest lexical overlap (ROUGE-1: 0.27) using instruction-tuned FLAN-T5 and BART. This finding underscores the difficulty of applying general instruction-tuned models to long, heterogeneous narratives that contain multiple clinical sections. In contrast, \parencite{ganzinger_automated_2025} achieved state-of-the-art results on radiology impressions by leveraging in-context prompts with clinically similar exemplars. The promising results in this narrower setting reflect the shorter output length and more templated style of radiology reports, which reduce the burden of domain adaptation. A similar domain specificity is evident in the work of \parencite{hu_zero-shot_2024}, who demonstrated competitive zero-shot information extraction from radiology reports using ChatGPT, albeit with performance highly dependent on prompt design and with occasional factual inaccuracies.
These studies suggest that zero-shot approaches can perform effectively in tasks that are limited and structurally homogeneous, but their usefulness decreases for lengthier, multi-section clinical papers where factual consistency and coherence are crucial. This contrast motivates our decision to move beyond zero-shot prompting and to evaluate parameter-efficient fine-tuning strategies so that they can better adapt large models to the linguistic and structural complexity of discharge summaries.

\subsubsection{Full Parameter Fine-Tuning}
Full parameter fine-tuning remains the conventional approach for adapting large models to biomedical text, and it consistently improves summary quality. However, it also introduces significant computational demands that limit its practicality in many healthcare settings. \parencite{lv_full_2024} attempted to reduce memory costs through the Low Memory Optimization (LOMO) algorithm, while \parencite{liu_hift_2024} proposed a hierarchical strategy that updates only subsets of parameters at each training step. Both methods highlight that even when memory usage is reduced, full fine-tuning still requires substantial resources. \parencite{qin_federated_2024} further illustrated this point by exploring federated fine-tuning for billion-parameter models, a direction that reduces data-sharing risks but amplifies communication overhead.\\
\\
These studies confirm not only the effectiveness of full fine-tuning, which is already well established, but also highlight the persistent challenges regarding its efficiency. Even improved optimizers or federated protocols do not fundamentally resolve the cost of updating all parameters. For clinical summarization, where hospitals and research units often operate under strict computational and privacy constraints, such approaches are difficult to scale. This observation strengthens the rationale for evaluating parameter-efficient alternatives such as LoRA, which promise comparable or superior performance at a fraction of the resource cost. Large language model-based approaches demonstrate significant promise in the field of healthcare; however, they require further adaptation to become practical for routine implementation in healthcare settings.

\subsubsection{Large Language Models in Healthcare}
LLMs in healthcare are utilized for many different purposes, some of them including clinical note summarization, question-answering, dialog summarization, and automating different issues  \parencite{luo_chatgpt_2025} that have been trained or tested on MIMIC datasets. Medical experts judged their model as being as accurate as a human at retrieving clinically significant concepts. \parencite{huang_survey_2025} have shown that customized LLMs may capture domain-specific semantics with great precision. The investigation is limited, however, because it focuses exclusively on discharge summaries and doesn't provide a full range of benchmarking for real-world deployment circumstances. In another work, \parencite{wu_epfl-make_2024} introduced MEDISCHARGE, an instruction-tuned LLM pipeline that uses the Meditron-7B model as a starting point. Their method addressed the challenge of short inputs by dynamically choosing important parts of MIMIC-IV clinical notes to make Brief Hospital Course and Discharge Instruction summaries. The model outperformed baseline approaches in BLEU, ROUGE, and BERTScore tests, showing that it can make short, relevant summaries. Although the framework performs well, its intricacy and focus on discharge situations make it less useful for a wider range of clinical narratives. These studies reveal that LLMs have a lot of potential for summarizing MIMIC-derived clinical narratives, but there are still problems with generalizability, factuality, and standardization. To move forward with real-world adoption, we still need evaluation methodologies that are aligned with the clinic and adaptations that are relevant to the domain.\\

Overall, the literature shows a clear pattern: extractive methods often lack coherence, zero-shot prompting struggles with longer notes, and full fine-tuning, though effective, is costly to run. Large language model–based approaches show promise but are not yet practical for routine use without adaptation. Parameter-efficient fine-tuning offers a middle ground, maintaining or improving performance while sharply lowering training and inference costs. In this study, we compare LoRA with both full fine-tuning and zero-shot approaches across several transformer models to fill this gap and show a more scalable route for clinical text summarization.

\subsubsection{Parameter-Efficient Fine-Tuning}

Recent advances in large pretrained language models have revolutionized natural language processing, but fine-tuning these massive models on domain-specific tasks such as clinical text summarization remains computationally expensive and resource-intensive \parencite{maazallahi_advancing_2025}. This challenge is especially pronounced in clinical settings, where computational resources may be limited, and data privacy concerns restrict large-scale model retraining. \parencite{jiang_res-tuning_2023} and \parencite{ruan_tte_2025} have shown that PEFT approaches effectively address these challenges by substantially reducing the number of trainable parameters while maintaining or even improving task performance. Later, \parencite{german-morales_transfer_2025} proposed Low-Rank Adaptation, which is a well-known PEFT approach that uses low-rank matrices to approximate the weight updates of the model's transformer layers. By freezing the original model weights and learning only the low-rank decomposition matrices, LoRA significantly reduces the fine-tuning parameter count, often by more than 95\% without degrading performance. Adapter tuning proposed by \parencite{wang_parameter-efficient_2025} is another type of PEFT that adds small bottleneck feed-forward networks (adapters) to transformer layers and trains them while keeping the main model parameters the same.


\subsection{Problems Encountered in the Previous Studies}

Transformer models and LLMs have demonstrated substantial promise in general healthcare applications. However, their usefulness in clinical text summaries has received scant attention.  Mainly, we categorized them into three groups of problems encountered in the previous studies as described below: 
\begin{enumerate}[label=\Alph*)]
    \item \textbf{Model Coverage:} Much of the previous and current research examined individual model families or focused on a single fine-tuning method, providing minimal insight into the comparative strengths and trade-offs between designs. Similarly, most previous studies focused on a single model family (example: BART-based models) or one fine-tuning method. This left a lack of systematic cross-architecture comparisons that evaluate the trade-offs between different traditional transformers and LLMs.
    \item \textbf{Parameter-Efficient Fine-Tuning:} Parameter-efficient techniques such as LoRA are understudied in clinical settings, and when they are researched, evaluations frequently omit crucial parameters such as memory utilization, training time, or deployment cost.
    \item \textbf{Evaluation Limitations:} Extractive approaches, while computationally light, struggle with abstractive gold summaries written by physicians, resulting in low ROUGE scores and poor semantic alignment. Existing study evaluation techniques are likewise mainly based on surface-level lexical overlaps, frequently neglecting deeper semantic fidelity and clinical factual consistency.
\end{enumerate}


\subsection{Research Objectives and Problem Definition}
To address these gaps, our work undertakes a comprehensive, cross-architecture evaluation across transformer and LLM models with the help of prompt engineering and applies three training strategies, incorporating lexical, semantic, and clinical metrics, including cTAKES-extracted entities. In doing so, we combine a clinically informed extractive pipeline with LoRA-based abstractive generation to bridge performance with practicality. By quantifying both summary quality and computational efficiency, our study shows a clinically useful and cost-effective methodology that improves the quality of medical text summarization by looking at both the quality of the summary and the cost of the computation. The research aims to demonstrate that LoRA is not only useful but also a superior strategy for abstractive medical text summarization, particularly for LLMs. This shows that high-quality summaries can be achieved with significantly fewer computational resources. 
This study aims to address several key gaps in the existing literature on clinical text summarization. Our primary objective is to systematically evaluate the performance and computational efficiency of modern language models and fine-tuning strategies. We aim to identify an optimal, robust solution for resource-constrained healthcare environments, thereby establishing a new performance benchmark for clinical text summarization. To accomplish this study, we formulate the following research questions: 

\begin{itemize}

\item \textbf{RQ1: Comparative Performance of Fine-Tuning Strategies in Transformers vs. LLMs:} How do different fine-tuning strategies (zero-shot inference, full parameter tuning, and LoRA) influence the summarization performance of conventional transformer models compared to large language models on clinical text?
 \item \textbf{RQ2: Extractive vs. Abstractive Baselines:} How do neural abstractive summarization approaches (transformers and LLMs) with fine-tuning compare against classical graph-based extractive summarization methods (LexRank, TextRank, LSA, Luhn) in terms of coherence, factual accuracy, and clinical usability?
 \item \textbf{RQ3: Viability of LoRA for Resource-Constrained Environments:} To what extent does LoRA parameter-efficient fine-tuning improve practical viability by reducing trainable parameters, training time, and GPU memory requirements, while maintaining or enhancing summarization quality in resource-constrained healthcare environments? 

\item \textbf{RQ4: Zero-Shot Summarization Efficacy:} How effective is zero-shot summarization using structured prompts as a baseline for clinical text, and to what degree does subsequent fine-tuning (LoRA or full) improve performance in terms of factual accuracy and coherent medical summaries that minimize hallucination?
\end{itemize}

\section{Methods}
\subsection{Proposed System Architecture}
This study uses the MIMIC-IV-Ext-BHC v1.2.0 dataset displayed in Figure \ref{fig:methodology} to create a two-stage clinical summarizing system that combines extractive and abstractive approaches. First, a comprehensive preprocessing pipeline was applied to discharge summaries to remove protected health information (PHI), normalize the text, expand clinical abbreviations, and apply tokenizer-aware chunking algorithms. cTAKES was employed to identify key clinical entities such as diseases, medications, and procedures, with entity-rich sentences prioritized for ranking using LexRank, TextRank, LSA, and Luhn.   \\

For abstractive summarization, we evaluated zero-shot inference, full parameter fine-tuning, and parameter-efficient LoRA fine-tuning across four transformer models (BART, T5, PEGASUS-XSUM, and FLAN-T5) and two LLMs (Mistral-7B and LLaMA-3-8B). Structured model-specific prompting techniques guided zero-shot and fine-tuned generation. Performance was assessed using lexical (ROUGE, METEOR) and semantic (BERTScore) metrics. The best-performing model from both paradigms was integrated into a GUI that supports entity extraction and summarization. This end-to-end methodology balances clinical fidelity, efficiency, and scalability for real-world deployment in resource-constrained healthcare environments. Mathematically, abstractive summarization can be framed as a \textit{conditional sequence generation} task. 
The source clinical note is represented as a sequence of tokens:
\begin{equation}
X = (x_1, x_2, \ldots, x_n)
\label{eq:source_tokens}
\end{equation}
and the goal is to produce a summary, also as a sequence of tokens:
\begin{equation}
Y = (y_1, y_2, \ldots, y_m)
\label{eq:summary_tokens}
\end{equation}
The model generates this summary \textbf{one token at a time}, predicting each new token $y_t$ based on:
\begin{enumerate}
    \item The \textbf{entire source document} $X$
    \item All previously generated tokens in the summary, denoted as $y_{<t}$
\end{enumerate}
The objective is to find the summary $Y^*$ that \textbf{maximizes the conditional probability} of the summary given the source document:
\begin{equation}
Y^* = \arg\max_Y P(Y \mid X; \theta)
\label{eq:objective}
\end{equation}
Using the \textbf{chain rule of probability}, this joint probability can be decomposed into a product of step-by-step probabilities:
\begin{equation}
P(Y \mid X; \theta) = \prod_{t=1}^m P(y_t \mid X, y_{<t}; \theta)
\label{eq:chain_rule}
\end{equation}
where: $P(y_t \mid X, y_{<t}; \theta)$ is the probability of generating token $y_t$ next, given the document and all tokens generated so far, and $\theta$ represents the model's learned parameters (weights). 
 The model starts with no summary tokens and predicts the first word $y_1$ based only on $X$. It then predicts $y_2$ using both $X$ and $y_1$, then $y_3$ using $X$, $y_1$, and $y_2$, and so on, until the clinical summary is complete. This formulation enables the model to produce \textit{novel phrasing} and \textit{reorganized structure} instead of simply copying texts from the clinical source.
\begin{figure}
    \centering
    \includegraphics[width=1.1\linewidth]{methodology diagram.jpeg}
    \caption{Overview of the Proposed Architecture Clinical Text Summarization }
    \label{fig:methodology}
\end{figure}
\subsection{Dataset}

The experiments conducted in this study utilize the MIMIC-IV-Ext-BHC v1.2.0 clinical note dataset \parencite {aali_mimic-iv-ext-bhc_nodate}, a comprehensive and publicly available collection of deidentified free-text clinical notes curated by the MIT Laboratory for Computational Physiology, with a revision date of February 3, 2025. This dataset encompasses a wide range of documents, including discharge summaries, radiology reports, nursing notes, physician notes, and other forms of clinical documentation. All records within the MIMIC-IV-Note dataset have undergone an extensive de-identification process to remove any protected health information, such as patient names, dates, locations, and contact details.\\

This study primarily focuses on the discharge summaries subset, which consists of 270,033 records, input token length (2267 $\pm 914$), and target token length (564 $\pm 410$). Each note is paired with a corresponding Brief Hospital Course (BHC) summary, serving as the ground-truth target for our abstractive summarization task. These notes have an average token length of 2,267. For all experiments, the dataset was split into training, validation, and test sets to ensure robust model generalization and evaluation \parencite{ayalew_explainable_2025}. These documents summarize a patient's hospital course, diagnoses, treatments, and discharge instructions. Due to their rich semantic content and relatively structured format, discharge summaries serve as an ideal source for clinical text summarization tasks. Each note in the dataset is deidentified using a hybrid rule-based and neural Named Entity Recognition (NER) system. All Protected Health Information (PHI) is replaced with standardized placeholders, and notes are linked via anonymized identifiers:
\begin{itemize}
    \item \texttt{note\_id}: Unique identifier for each note.
    \item \texttt{input:} preprocessed discharge note text. The text provides a detailed narrative of the patient's hospitalization, including sections like medical history, treatments, and discharge instructions.
    \item \texttt{target}: A concise summary of the patient's hospital course, including major events, treatments, and outcomes during the hospital stay.
    \item \texttt{input\_tokens}: Contains the input token length.
    \item \texttt{target\_tokens}: Contains the target token length.
\end{itemize}

All data usage complies with the PhysioNet Credentialed Health Data License. Access was granted upon completion of CITI certification and a signed data use agreement \parencite{chicco_venus_2025}.  While the size of our dataset may appear modest compared to general-domain corpora, the MIMIC-IV datasets are a substantial and widely accepted benchmark for clinical NLP research due to their high quality and domain-specific complexity. The dataset is well-suited for medical informatics research, domain-adapted LLM training for clinical summarization, and efficient healthcare delivery applications. It provides a robust, realistic, and highly scalable foundation for our research, particularly for clinical note summarization. As a result, this dataset represents the latest version and is well-suited for conducting this research. 
\subsection{Data Preprocessing}

Given the unstructured and noisy nature of clinical notes, a comprehensive preprocessing pipeline was implemented using \texttt{pandas}, \texttt{nltk}, and other libraries (see Algorithm \ref{alg:data_preprocessing1}). The following key steps were applied to prepare the discharge summaries for both extractive and abstractive summarization experiments:

\begin{enumerate}
 \item \textbf{Filtering and Text Normalization:} Notes with missing or empty \texttt{input} or \texttt{target} fields were filtered out. Basic text normalization included converting to lowercase, removing excess whitespace and newline characters, and ensuring punctuation regularity. Notes with missing or empty input or target fields were removed to ensure only complete examples were retained. Formally, given the original dataset:
\begin{equation}
\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N
\end{equation}
Where $x_i$ is the input note and $y_i$ is the target summary, filtering removes incomplete pairs:
\begin{equation}
\mathcal{D}' = \{(x_i, y_i) \in \mathcal{D} \mid x_i \neq \varnothing \ \land \ y_i \neq \varnothing \}
\end{equation}

This guarantees that training data always contains meaningful input–output pairs, preventing models from learning spurious patterns. After filtering, we standardized the remaining text by converting all characters to lowercase, collapsing multiple whitespaces (newlines), and ensuring consistent punctuation. This is represented by the normalization function:
\begin{equation}
\mathcal{N}(t) = \mathrm{punct\_norm} \left( \mathrm{ws\_norm} \left( \mathrm{lower}(t) \right) \right)
\end{equation}
Where $\text{lower}(t)$ converts to lowercase, $\text{ws\_norm}(t)$ collapses whitespace and line breaks, and $\text{punct\_norm}(t)$ enforces regular punctuation. Applying normalization gives:
\begin{equation}
\mathcal{D}'' = \{(\mathcal{N}(x_i), \mathcal{N}(y_i)) \mid (x_i, y_i) \in \mathcal{D}' \}
\end{equation}
This reduces vocabulary fragmentation (for example, “Hypertension” and “hypertension” become the same token) and ensures tokenizer stability across models. 


\textbf{Practical Impact.} These preprocessing steps ensure that the dataset is clean, consistent, and clinically meaningful. Filtering improves reliability by avoiding incomplete cases, while normalization helps models focus on medical content rather than formatting inconsistencies. Together, these steps improve summarization accuracy and make the system safer for clinical deployment, as models trained on consistent data are less likely to misinterpret critical medical terms.
\item\textbf{{Handling Special Characters and Medical Abbreviation Expansion:}}
Clinical notes often contain a high density of abbreviations, symbols, and irregular formatting. To make the dataset clearer and more consistent, we implemented a specialized preprocessing pipeline to handle these issues. We first defined a curated \textbf{medical abbreviation lexicon}:  
\begin{equation}
\mathcal{A} = \{(a_k, e_k)\}_{k=1}^K
\end{equation}

Where $a_k$ is an abbreviation (for example, ``SOB'') and $e_k$ its expanded form (``shortness of breath'').  This lexicon acts as a lookup table that ensures ambiguous abbreviations are consistently expanded into their full medical meaning when appropriate. Next, we defined the \textbf{abbreviation expansion function}:  
\begin{equation}
\mathcal{E}(t) = \text{replace}_{a_k \in \mathcal{A}}(a_k \rightarrow e_k, \ t)
\end{equation}

This function replaces $a_k$ in text $t$ with its expanded form $e_k$, while keeping conventional abbreviations unchanged if they are well recognized.  For example, ``HTN'' is expanded to ``hypertension,'' but abbreviations like ``MRI'' remain unchanged since they are widely understood by clinicians and tokenizers. This reduces ambiguity and improves the interpretability of generated summaries. We also introduced the \textbf{special character normalization function}:  

\begin{equation}
\mathcal{S}(t) = \mathrm{char\_norm}(t)
\end{equation}

Where it $\mathrm{char\_norm}(\cdot)$ removes or standardizes non-standard symbols, irregular formatting, and extraneous special characters. This step cleans up noisy characters (for example, stray ``@@@'' or inconsistent ``+/-'' usage) and enforces consistent formatting, making the dataset cleaner and easier for models to process. Finally, the preprocessed dataset after abbreviation expansion and special character handling is:  

\begin{equation}
\mathcal{D}''' = \{(\mathcal{S}(\mathcal{E}(x_i)), \ \mathcal{S}(\mathcal{E}(y_i))) \mid (x_i, y_i) \in \mathcal{D}'' \}
\end{equation}

This means that every input note and summary pair is standardized by expanding abbreviations and normalizing special characters before model training.


\textbf{Practical Impact:}  
These steps ensure consistent handling of medical abbreviations, reduce textual noise, and improve clarity. This not only helps models better capture clinical meaning but also increases the safety and interpretability of the generated summaries for healthcare professionals.


 \item \textbf{Chunking Strategy:} To address the input length limitations of transformer models, a sentence-level, tokenizer-aware chunking strategy was applied to MIMIC-IV clinical notes, but only when the tokenized input exceeded \texttt{1024} tokens. In such cases, the \texttt{chunk\_text} function segmented the document into smaller overlapping chunks, each containing at most \texttt{1024} tokens (\texttt{MAX\_INPUT\_TOKENS}), with a \texttt{100}-token overlap (\texttt{CHUNK\_OVERLAP}) to preserve contextual continuity between chunks. This overlap is especially essential in medical notes, where clinically relevant material can span several phrases. By conditionally chunking only longer notes, the technique minimizes unnecessary fragmentation while maintaining narrative coherence. This selective preprocessing ensures conformity with model input constraints while preserving key semantic structure, thereby enabling more accurate and context-aware summarization across diverse clinical note lengths. More specifically, given the preprocessed dataset $t$ from the above steps, we can define the tokenization function as \mathcal{T}(t) and the token length as:
\begin{equation}
L(t) = |\mathcal{T}(t)|
\end{equation}

Where $\mathcal{T}(t)$ is the sequence of tokens for text $t$, $L(t)$ is its length in terms of the number of tokens in text $t$. $L(t)$ is important because transformer models can only handle inputs up to a fixed size. Given the maximum token limit $T_{\max}$ and the overlap size $O$, the chunking function is defined as follows:
\begin{equation}
\mathcal{C}(t) =
\begin{cases}
[t], & \text{if } L(t) \leq T_{\max} \\
\{ \mathcal{T}(t)[1:T_{\max}], \ \mathcal{T}(t)[T_{\max}-O+1:2T_{\max}-O], \ \dots \}, & \text{if } L(t) > T_{\max}
\end{cases}
\end{equation}

If a note is short enough, it is kept as a single sequence. Otherwise, it is split into overlapping token windows of length $T_{\max}$ with an overlap of $O$ tokens. This ensures that clinical information spanning across segment boundaries is preserved, enabling more coherent and accurate summarization. The final chunked dataset is:
\begin{equation}
\mathcal{D}^{\mathrm{chunk}} =
\big\{ (\mathcal{C}(x_i), \ \mathcal{C}(y_i)) \ \big|\ (x_i, y_i) \in \mathcal{D}''' \big\}
\end{equation}

\textbf{Practical Impact:}  

This strategy ensures that long clinical documents are processed without exceeding model limits, while overlap between chunks preserves clinical context across boundaries. As a result, the summarization system remains accurate and context-aware, regardless of input length.
   
\item \textbf{Truncation/Padding Strategies:} Transformer models have a maximum input sequence length, and clinical notes often exceed this limit (1024 tokens). To handle this, we applied truncation for longer sequences and padding for shorter ones. Truncation was carefully designed to retain the most clinically relevant information, usually found at the beginning of notes, such as patient demographics and admission details. Padding ensures uniform sequence lengths, preventing alignment issues during model training. Target summaries were similarly normalized to match input lengths, facilitating consistent model outputs. Assuming we have the chunked dataset from the above steps, we define the truncation function as follows:
\begin{equation}
\mathrm{trunc}(t) =
\begin{cases}
\mathcal{T}(t)[1:T_{\max}], & \text{if } L(t) > T_{\max} \\
\mathcal{T}(t), & \text{otherwise}
\end{cases}
\end{equation}
If a note exceeds $T_{\max}$ tokens, only the first $T_{\max}$ tokens are retained, preserving the most essential patient information. Otherwise, the text remains unchanged.

Define the padding function:
\begin{equation}
\mathrm{pad}(t) =
\begin{cases}
\mathcal{T}(t) \ \Vert \ \mathrm{[PAD]}^{\,T_{\max} - L(t)}, & \text{if } L(t) < T_{\max} \\
\mathcal{T}(t), & \text{otherwise}
\end{cases}
\end{equation}
Where $\mathrm{[PAD]}$ is the padding token, and $\Vert$ denotes sequence concatenation.  
If a note has fewer $T_{\max}$ tokens, padding tokens are appended until it reaches the maximum length. This ensures all sequences are uniform, avoiding model errors and maintaining consistent input formatting.

The final length-normalized dataset is:
\begin{equation}
\mathcal{D}^{\mathrm{final}} =
\big\{ (\mathrm{pad}(\mathrm{trunc}(x_i)), \ \mathrm{pad}(\mathrm{trunc}(y_i))) \ \big|\ (x_i, y_i) \in \mathcal{D}^{\mathrm{chunk}} \big\}
\end{equation}

\textbf{Practical impact:} This preprocessing guarantees that all model inputs and outputs meet transformer length requirements while prioritizing clinically essential content, improving the reliability and accuracy of summarization.

\item \textbf{Section Header and Tag Normalization:} To reduce noise and guide the summarization process, only semantically rich sections of the note were retained. Therefore, to improve structural consistency and facilitate section-aware downstream processing, clinical notes frequently embedded with irregular or system-generated tags were adjusted using a rule-based heuristic method. The tag replacement dictionary encompasses more than 22 distinct medical section headers commonly found in MIMIC-IV. This step standardizes section boundaries, normalizes irregular tags, and removes formatting artifacts, improving readability and ensuring compatibility with section-aware summarization workflows. 
\begin{algorithm}[H]
\caption{Data Preprocessing for Clinical Notes}
\label{alg:data_preprocessing1}
\begin{algorithmic}[1]
\Require Raw Clinical Dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$, Token Limit $\tau$
\Ensure Cleaned Dataset $\tilde{\mathcal{D}} = \{(\tilde{x}_i, y_i)\}_{i=1}^N$
\For{each $(x_i, y_i) \in \mathcal{D}$}
    \State Remove PHI fields using regex placeholders
    \State Expand medical abbreviations using domain lexicon
    \State Normalize tags, headers, and remove formatting artifacts
    \If{$\text{TokenCount}(x_i) > \tau$}
        \State Chunk $x_i$ into overlapping segments of length $\tau$
    \EndIf
    \State Tokenize $x_i$ into sentences using \texttt{nltk.sent\_tokenize}
    \State Standardize section headers via dictionary mapping
    \State $\tilde{x}_i \gets x_i$ \Comment{Store preprocessed note}
\EndFor
\State \Return $\tilde{\mathcal{D}}$
\end{algorithmic}
\end{algorithm}


\subsection{Prompting Engineering Strategies}
To optimize the language models' generation capabilities for clinical text summarization, we designed model-specific prompt engineering strategies that consider the distinct characteristics of each architecture, as well as its pre-training objectives. Each prompt was meticulously crafted with task-specific instructions that are explicit and describe the clinical summary aim. Additionally, organized output criteria that are customized to healthcare documentation standards were incorporated into each prompt. We employ a zero-shot design to provide the language model with structured input-output examples, establishing a consistent format for clinical summarization. These observed improvements indicate that providing the model with representative clinical examples helps constrain its output format, enhancing factual coherence and reducing hallucinations. The supplementary materials, specifically (Supplementary File S1), offer comprehensive and detailed strategies for prompt engineering across all models. These strategies are designed to enhance reproducibility in future research, ensuring that other researchers can replicate and validate the findings effectively.

\section{Evaluation Metrics} 
\subsection{ROUGE Scores}
We employ ROUGE-N and ROUGE-L metrics to quantify n-gram and longest common subsequence overlap between the generated summary \( G \) and the reference summary \( R \).

\begin{itemize}
    \item \textbf{ROUGE-N (ROUGE-1 and ROUGE-2):} ROUGE-1: Measures the intersection (overlap) of unigrams (individual words) between the produced and reference summaries. It primarily evaluates the existence of significant keywords. ROUGE-2: Assesses the intersection of bigrams (word pairs). 
    \begin{equation}
    \text{ROUGE-N} = \frac{\sum_{S \in \{R\}} \sum_{\text{gram}_n \in S} \min \left( \text{Count}_{G}(\text{gram}_n), \text{Count}_{R}(\text{gram}_n) \right)}{\sum_{S \in \{R\}} \sum_{\text{gram}_n \in S} \text{Count}_{R}(\text{gram}_n)}
    \label{eq:rouge-n}
    \end{equation}
    
    \item \textbf{ROUGE-L:} Used to evaluate the longest common subsequence of the produced and benchmark summaries. It measures structural similarity at the sentence level and indicates how effectively the produced summary maintains the primary concepts in their original sequence.
    \begin{equation}
    \text{ROUGE-L} = \frac{LCS(G, R)}{\text{length}(R)}
    \label{eq:rouge-l}
    \end{equation}
\end{itemize}

\subsection{METEOR Score}
METEOR is a measure created for assessing machine translation, although it is also significantly relevant to summarization. In contrast to ROUGE, METEOR evaluates not just precise word matches but also matches derived from stemmed words, synonyms (utilizing WordNet), and paraphrases. METEOR assesses exact and fuzzy matching through stemming, synonymy, and word order. It combines unigram precision \( P \) and recall \( R \), penalized by fragmentation \( Pen \):
\begin{equation}
F_{mean} = \frac{10 \cdot P \cdot R}{R + 9 \cdot P}
\label{eq:fmean}
\end{equation}
\begin{equation}
\text{METEOR} = F_{mean} \cdot (1 - Pen)
\label{eq:meteor}
\end{equation}
\subsection{BERTScore}
The BERTScore is a more contemporary and semantically aware evaluation metric that uses contextual embeddings from pre-trained BERT models. It compares the semantic similarity of tokens in the candidate summary and the reference summary. Therefore, to measure semantic similarity, we utilize BERTScore using contextual embeddings from a transformer model:
\begin{equation}
\text{Precision} = \frac{1}{|G|} \sum_{g \in G} \max_{r \in R} \cos(g, r)
\label{eq:bert-p}
\end{equation}
\begin{equation}
\text{Recall} = \frac{1}{|R|} \sum_{r \in R} \max_{g \in G} \cos(r, g)
\label{eq:bert-r}
\end{equation}
\begin{equation}
\text{F1} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\label{eq:bert-f1}
\end{equation}
\section{Implementation Details}
All experiments were conducted using the Python programming language with essential libraries such as PyTorch and the Hugging Face Transformers library, with the PEFT library facilitating LoRA implementation. The training was performed on NVIDIA A100 and NVIDIA  A1000 GPUs, employing memory management techniques such as gradient checkpointing to reduce peak memory usage. For fine-tuning experiments, the dataset was partitioned into training, validation, and test sets with a standard ratio (70\% training, 15\% validation, 15\% test) \parencite{ayalew_smart_2025}. The validation set was used for hyperparameter tuning and early stopping to prevent overfitting, while the test set was held out and used only for final performance evaluation to ensure an unbiased assessment of generalization capabilities \parencite{ayalew_smart_2024}. In this proposed work, we employed the "large" version of each transformer model (with a maximum sequence length of 1024 tokens) to maintain a consistent capacity across the different architectures. The model was trained for 5 epochs using a learning rate of $2 \times 10^{-5}$, a per-device batch size of 4, and 8 gradient accumulation steps. For the generation process, we utilized 3 beams, applying a length penalty of 1.0 and a repetition penalty of 1.1. Additionally, a no-repeat n-gram size of 3 was enforced to avoid redundancy. The generation process was further refined with early stopping set to True, an overlap of 100, and a dynamic batch size ranging from 6 to 12. \textcolor{red}{As seen from 1, we present a detailed comparison of the trainable and total parameters, peak GPU memory usage, and training duration for both full fine-tuning and the LoRA technique across various models. The findings illustrate that LoRA not only utilizes a significantly reduced number of trainable parameters, thereby minimizing the overall model complexity, but it also exhibits a marginally lower peak GPU memory consumption. Additionally, the training time required for LoRA is comparatively shorter than that of full fine-tuning, making it a more efficient approach for model adaptation in resource-constrained environments.}

\begin{table}[H]
\caption{Compute and Efficiency Comparison of Full Fine-Tuning vs. LoRA Across Models.}
\label{tab:compute_metrics}
\small
\renewcommand{\arraystretch}{1.2}
\textcolor{red}{
\begin{tabular}{|
p{2.2cm}|
p{2.1cm}|
p{2.2cm}|
p{2.2cm}|
p{2.5cm}|
p{2.4cm}|
}
\hline
\textbf{Model} 
& \textbf{Fine-Tuning Type} 
& \textbf{Trainable Parameters} 
& \textbf{Total Parameters} 
& \textbf{GPU Memory \linebreak Usage (Peak)} 
& \textbf{Training Time} \\
\hline
\multirow{T5} & Full & 755M & 755M & 8.26 GiB & 6h 36m \\
\cline{2-6}
 & LoRA & 17.3M & 755M & 2.96 GiB & 2h 10m \\
\hline
\multirow{Flan-T5} & Full & 801M & 801M & 8.77 GiB & 6h 47m \\
\cline{2-6}
 & LoRA & 18.3M & 801M & 3.14 GiB & 2h 18m \\
\hline
\multirow{BART} & Full & 411M & 411M & 1.57 GiB & 4h 49m \\
\cline{2-6}
 & LoRA & 4.7M & 411M & 0.77 GiB & 1h 39m \\
\hline
\multirow{Pegasus-XSum} & Full & 571M & 571M & 8.26 GiB & 7h 28m \\
\cline{2-6}
 & LoRA & 0.78M & 571M & 2.15 GiB & 2h 35m \\
\hline
Mistral-7B & LoRA & 41.9M & 7.28B & 27.46 GiB & 5h 06m \\
\hline
Llama-2-7B & LoRA & 39.9M & 6.77B & 25.57 GiB & 5h 04m \\
\hline
\end{tabular}
}
\end{table}

\section{Experimental Results}
We conducted four categories of experiments. The first experiment focused on extractive summarization utilizing cTAKES. The second experiment, detailed in Algorithm \ref{alg:abstractive}, explored abstractive summarization using both transformer and LLM models without employing fine-tuning techniques. The third experiment involved full parameter fine-tuning exclusively for transformer models, as we opted not to perform full parameter fine-tuning for LLMs due to limitations in computational resources. Finally, the fourth experiment examined parameter-efficient fine-tuning via LoRA for both transformer and LLM models.

\begin{algorithm}[H]
\caption{Abstractive Summarization with Zero-Shot, Full, and LoRA Fine-Tuning}
\label{alg:abstractive}
\begin{algorithmic}[1]
\Require Cleaned Dataset $\tilde{\mathcal{D}} = \{(\tilde{x}_i, y_i)\}_{i=1}^N$, Transformers $\mathcal{M}_T$, LoRA Models $\mathcal{M}_L$
\Ensure Abstractive Summaries $S_Z$, $S_F$, $S_L$
\State \textbf{Zero-Shot Inference}
\For{each $T_j \in \mathcal{M}_T$}
    \For{each $\tilde{x}_i$}
        \State $s_{ij}^{(Z)} \gets T_j(\texttt{prompt}(\tilde{x}_i))$
    \EndFor
\EndFor
\State $S_Z \gets \{s_{ij}^{(Z)}\}$

\State \textbf{Full Fine-Tuning}
\For{each $T_j \in \mathcal{M}_T$}
    \State Initialize model parameters $\theta$
    \For{epoch $= 1$ to $E$}
        \For{each batch $(x_b, y_b)$}
            \State $\mathcal{L}_{CE} \gets$ CrossEntropy$(T_j(x_b), y_b)$
            \State $\theta \gets \theta - \eta \nabla_\theta \mathcal{L}_{CE}$
        \EndFor
    \EndFor
    \State $S_F \gets T_j(\tilde{\mathcal{D}})$
\EndFor

\State \textbf{LoRA Fine-Tuning}
\For{each $L_j \in \mathcal{M}_L$}
    \State Freeze $\theta$; initialize low-rank adapters $(A, B)$
    \For{epoch $= 1$ to $E$}
        \For{each batch $(x_b, y_b)$}
            \State $\Delta W = B A$ \Comment{LoRA update}
            \State $\mathcal{L}_{LoRA} \gets$ CrossEntropy$((\theta + \Delta W)(x_b), y_b)$
            \State Update $A, B$ using $\nabla \mathcal{L}_{LoRA}$
        \EndFor
    \EndFor
    \State $S_L \gets L_j(\tilde{\mathcal{D}})$
\EndFor
\State \Return $S_Z$, $S_F$, $S_L$
\end{algorithmic}
\end{algorithm}



\subsection{Experiments: 1. Graph-based extractive summarizer algorithms performance using cTAKES} 

In the first phase, we evaluated four classical graph-based summarization algorithms: LexRank, TextRank, Latent Semantic Analysis (LSA), and Luhn, on the MIMIC-IV discharge summaries. Each algorithm constructs a sentence similarity graph using TF-IDF cosine similarity and ranks sentences accordingly. To enrich clinical relevance, we used cTAKES to extract named entities such as \textit{ Disease/Disorder, Medication, Anatomical Site, Procedure, and Sign/Symptom} as shown in Figure \ref{fig:cTAKES Named Entities}. We combine graph-based centrality, clinical named entity recognition, and semantic similarity. Sentences containing these entities were prioritized for summarization, enhancing the informativeness of the extractive input. \textcolor{black}{To make the similarity graph more clinically aware, we went beyond simple TF–IDF cosine similarity and added two sources of biomedical semantic information. First, we utilized one of the domain-specific semantic similarity techniques, BioSentVec, a sentence-level embedding model trained on PubMed and clinical notes, which reflects how medical terms and phrases relate to each other in real clinical writing \parencite{dhawan_healthcare_2024}. This helps the system recognize that two sentences may carry the same clinical meaning even if they use different wording (for example, “SOB” vs. “shortness of breath”). Second, we integrate the cTAKES-identified UMLS domain-specific concepts in each sentence into their CUI2Vec embeddings \parencite{giancani_quality_2023}. Because CUI2Vec is built from electronic medical records and claims data, it captures common medical co-occurrence patterns such as which diseases typically appear together, which drugs are used for which conditions, or which findings are associated with particular diagnoses. By combining lexical TF-IDF overlaps, BioSentVec sentence similarity, and cui2vec concept similarity, the graph reflects domain-specific relationships that matter in clinical text rather than generic language patterns.}

\textcolor{black}{Even with these additions, the overall extractive performance remains modest. This is expected because MIMIC-IV discharge and radiology notes are highly compressed, clinically dense, and often shift topics quickly. The gold summaries are also fully abstractive and written by clinicians, meaning they reorganize, paraphrase, and condense the narrative in ways an extractive method cannot match. As a result, even a clinically enriched extractive model cannot reach the coherence and semantic alignment that the abstractive models achieved after fine-tuning.} For instance, LexRank, the best-performing method, achieved ROUGE-1 of 0.273 (see Table \ref{tab:summary_performance1}). These relatively low scores can be attributed to the abstractive nature of the gold summaries, written by medical professionals and often paraphrased, synthesized, or reordered the source content. As extractive methods rely on surface-level sentence overlap, they struggled to match these abstract summaries word for word, particularly in semantic alignment and flow. 
\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\linewidth]{cTAKES Named Entities.jpeg}
    \caption{cTAKES-based Clinical Named Entity Extraction}
    \label{fig:cTAKES Named Entities}
\end{figure}


\begin{longtable}{|p{1.6cm}|p{1.4cm}|p{1.4cm}|p{1.5cm}|p{1.5cm}|p{1.7cm}|p{1.7cm}|p{1.5cm}|}
\caption{Summary performance metrics for different extractive models.\label{tab:summary_performance1}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endfirsthead
\multicolumn{8}{c}{{\tablename\ \thetable{} ....Continued from previous page}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERTScore F1} \\
\hline
\endhead
\hline \multicolumn{8}{|r|}{{....Continued on next page}} \\ \hline
\endfoot
\hline
\endlastfoot
\textbf{LexRank}   & 0.273 & 0.066 & 0.248 & 0.131 & 0.143 & 0.158 & 0.151 \\
\hline
\textbf{TextRank} & 0.250 & 0.061 & 0.240 & 0.126 & 0.138 & 0.135 & 0.134 \\
\hline
\textbf{LSA}      & 0.246 & 0.044 & 0.219 & 0.122 & 0.132 & 0.131 & 0.132 \\
\hline
\textbf{Luhn}     & 0.209 & 0.035 & 0.197 & 0.107 & 0.110 & 0.102 & 0.106 \\

\end{longtable}





\subsection{Experiments: 2. Without Fine-Tuning with Zero-Shot Inference}

\subsubsection{PEGASUS-XSUM AND FLAN-T5 Performance}
In our preliminary experiments, which were conducted without any modifications to the models, FLAN-T5 and PEGASUS-XSUM showed different performance. As demonstrated in Figure \ref{fig:FLAN_lora_training}, FLAN-T5 emerged as a standout performer, achieving exceptional results on traditional lexical overlap metrics. In contrast, PEGASUS-XSUM, as depicted in Figure \ref{fig:PEGASUS_lora_loss}, consistently fell short of expectations, recording lower performance metrics relative to its counterparts. This disparity is further illustrated in Table \ref{tab:summary_performance}, where PEGASUS-XSUM underperformed across all evaluation metrics compared to the other model variants.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan.png}
        \caption{FLAN Without Fine-tuning}
        \label{fig:FLAN_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus.png}
        \caption{PEGASUS-XSUM Without Fine-tuning}
        \label{fig:PEGASUS_lora_loss}
    \end{minipage}
\end{figure}
\subsubsection{BART and T5 Results}
A zero-shot evaluation of BART and T5 revealed that BART achieved the highest overall performance among the four models assessed. This finding indicates that BART performs well in summarizing medical texts without requiring fine-tuning. Specifically, BART outperformed both PEGASUS-XSUM and FLAN-T5 according to traditional lexical overlap metrics. Further evidence of BART's superiority is reflected in the statistical results, which indicate that it scores higher in assessment measures compared to T5, as illustrated in Figures \ref{fig:bart_lora_training} and \ref{fig:T5_lora_loss}.
%\vspace{-0.8cm}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart.png}
        \caption{BART Without Fine-tuning}
        \label{fig:bart_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.44\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5.png}
        \caption{T5 Without Fine-tuning}
        \label{fig:T5_lora_loss}
    \end{minipage}
\end{figure}

\subsubsection{Mistral-7B and LLaMA-3-8B }
As seen from Figure \ref{fig:Mistral} and Figure \ref{fig:Llama}, Mistral-7B and LLaMA-3-8B performed better than the traditional transformer models.  The evaluation outcomes of Mistral-7B and LLaMA-3-8B through zero-shot inference show improvements compared to conventional transformer models like PEGASUS-XSUM, T5, FLAN-T5, and BART. Both models, without fine-tuning, attained improved scores on most metrics, which suggests their superior generalization abilities and comprehension of clinical text.  
\vspace{-4}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.43\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Misteral.png}
        \caption{Mistral-7B Without Fine-tuning}
        \label{fig:Mistral}
    \end{minipage}
    \hfill
    \begin{minipage}{0.43\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Llama.png}
        \caption{LLaMA-3-8B Without Fine-tuning}
        \label{fig:Llama}
    \end{minipage}
\end{figure}


\small
\begin{longtable}{|p{1.6cm}|p{1.4cm}|p{1.4cm}|p{1.5cm}|p{1.5cm}|p{1.7cm}|p{1.7cm}|p{1.7cm}|}
\caption{Summary performance metrics for different models without fine-tuning, including ROUGE scores, METEOR, and BERTScore metrics.\label{tab:summary_performance}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERTScore F1} \\
\hline
\endfirsthead
\multicolumn{8}{c}{{\tablename\ \thetable{} ...Continued from previous page}} \\
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERTScore Precision} & \textbf{BERTScore Recall} & \textbf{BERTScore F1} \\
\hline
\endhead
\hline \multicolumn{8}{|r|}{{....Continued on next page}} \\ \hline
\endfoot
\hline
\endlastfoot

\textbf{PEGASUS \linebreak -XSUM} & 0.22 & 0.05 & 0.11 & 0.16 & 0.77 & 0.81 & 0.796 \\
\hline
\textbf{FLAN-T5} & 0.25 & 0.04 & 0.12 & 0.15 & 0.80 & 0.80 & 0.80 \\
\hline
\textbf{T5} & 0.25 & 0.05 & 0.12 & 0.15 & 0.81 & 0.81 & 0.81 \\
\hline
\textbf{BART} & 0.27 & 0.06 & 0.14 & 0.17 & 0.811 & 0.814 & 0.817 \\
\hline
\textbf{Mistral-7B} & 0.3621 & 0.1088 & 0.1804 & 0.2270 & 0.8154 & 0.8141 & 0.8273 \\
\hline
\textbf{LLaMA-3-8B} & 0.3855 & 0.0981 & 0.1666 & 0.2671 & 0.8232 & 0.8177 & 0.8235 \\
\hline

\end{longtable}

\subsection{Experiments: 3. Full Parameter Fine-Tuning}
From previous experiments, we obtained limited improvement using zero-shot inference without fine-tuning. The zero-shot evaluation revealed the limited effectiveness of models in clinical text summarization without domain-specific adaptation, achieving modest ROUGE scores that indicated the necessity for task-specific training. Therefore, we do the following experiments using full parameter fine-tuning techniques.

\subsubsection{PEGASUS-XSUM Using Full Parameter Fine-Tuning}
Based on the complete evaluation results in Table \ref{tab:fullparameter_performance} and the related performance visuals, PEGASUS-XSUM outperformed the previous zero-shot baseline after adjusting all its parameters. Figure \ref{fig:Pegasus2_lora_training} shows that the PEGASUS-XSUM model performed better on all evaluation metrics after full parameter fine-tuning. Figure 6 shows that the training dynamics demonstrated stable and consistent convergence throughout the fine-tuning process. The losses for both training and validation decreased steadily over five epochs. As shown in Figure \ref{fig:Pegasus2_lora_loss}, the training loss decreased from 3.2 to 2.55, and the validation loss decreased from 2.85 to 2.49, which shows that the model learned well without overfitting. The fact that both loss curves went down at the same time and were very close to each other showed that the model could generalize well to new clinical texts. 
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with full training.png}
        \caption{PEGASUS-XSUM with full parameter Training}
        \label{fig:Pegasus2_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with full loss.png}
        \caption{PEGASUS-XSUM with full parameter Loss}
        \label{fig:Pegasus2_lora_loss}
    \end{minipage}
\end{figure}
\subsubsection{T5 Using Full Parameter Fine-Tuning}
T5 performed well after comprehensive parameter fine-tuning, generating competitive results across all assessment criteria and displaying consistent learning dynamics throughout the training procedure. As shown in Figure \ref{fig:T52_full_training}, T5 achieved ROUGE-1 scores of 0.4991 (validation) and 0.4969 (test), demonstrating gains over its zero-shot baseline, but lower than FLAN-T5 and BART. Figure \ref{fig:T52_full_loss}  shows outstanding training dynamics, with both training and validation losses converging smoothly. The parallel decrease of loss curves without considerable divergence implies consistent learning and strong generalizability to previously encountered clinical literature.  
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with full training.png}
        \caption{T5 with full parameter Training}
        \label{fig:T52_full_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.46\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with full loss.png}
        \caption{T5 with full parameter Loss}
        \label{fig:T52_full_loss}
    \end{minipage}
\end{figure}
\subsubsection{FLAN-T5 Using Full Parameter Fine-Tuning}
Following the optimization of all its parameters, FLAN-T5 achieved better scores across all evaluation metrics. As illustrated in Figure \ref{fig:Flan_full_training}, FLAN-T5 achieved higher ROUGE-1 scores and outperformed both the zero-shot and fully parameter fine-tuned results of its counterparts, T5 and PEGASUS-XSUM. Furthermore, Figure \ref{fig:Flan_full_loss} illustrates the model's training dynamics. The loss curves, which descend in a harmonious parallel without showing signs of divergence, indicate that the model is not only learning effectively but also possesses a strong capacity for generalization.
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with full training.png}
        \caption{FLAN with full parameter Training}
        \label{fig:Flan_full_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with full loss.png}
        \caption{FLAN with full parameter Loss}
        \label{fig:Flan_full_loss}
    \end{minipage}
\end{figure}
\subsubsection{BART Using Full Parameter Fine-Tuning}
\textcolor{red}{After fine-tuning all its parameters, BART achieved the highest scores among full-parameter fine-tuned transformer models.} It achieved the highest scores across several important evaluation measures. Table \ref{tab:fullparameter_performance} and Figure \ref{fig:Bart2_full_training} show that BART achieved the highest ROUGE-1 scores. BART achieved the highest METEOR scores and BERTScore, reflecting improved semantic understanding and content retention. The training dynamics in Figure \ref{fig:Bart2_full_loss} illustrate excellent training dynamics, with both training and validation losses converging smoothly. Despite this, BART has less convergence in training and validation losses compared to other models. This illustration shows stable learning without overfitting. BART's denoising autoencoder pre-training architecture is particularly well-suited for clinical text processing, as it can achieve the most comprehensive performance across semantic and lexical evaluation dimensions. This capability makes it the premier choice for full parameter fine-tuning in medical text summarization applications.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with full training.png}
        \caption{BART with full parameter Training}
        \label{fig:Bart2_full_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with full loss.png}
        \caption{BART with full parameter Loss}
        \label{fig:Bart2_full_loss}
    \end{minipage}
\end{figure}
In general, and based on our experimental results, domain-specific full parameter fine-tuning demonstrates advantages for clinical text summarization, transforming general-purpose transformer models into medical text processing systems. The comprehensive adaptation of all model parameters enables learning of clinical terminology, medical reasoning patterns, and domain-specific linguistic structures that are essential for accurate healthcare documentation summarization. This method helps models gain understanding of medical relationships, patient treatment sequences, and the hierarchical nature of clinical information. This leads to summaries that preserve important medical context and clinical accuracy. Table \ref{tab:fullparameter_performance} shows that the experimental results show performance improvements for all of the transformer architectures that were tested when compared to their zero-shot baselines in terms of all evaluation metrics.


\small
\begin{longtable} [htbp]{|p{1.57cm}|p{1.3cm}|p{0.8cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.45cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}|}
\caption{Performance metrics for different models across validation and test stages (full parameter fine-tuning), including loss, ROUGE scores, METEOR, and BERTScore metrics.\label{tab:fullparameter_performance}} \\
\hline
\textbf{Model} & \textbf{Stage} & \textbf{Loss} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERT- \linebreak Score Precision} & \textbf{BERT- \linebreak Score Recall} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endfirsthead
\multicolumn{10}{c}{{\tablename\ \thetable{} ....Continued from previous page}} \\
\hline

\endhead
 \hline
\endfoot
\hline
\endlastfoot

\multirow{\textbf{PEGASUS \linebreak -XSUM}} & Validation & \textcolor{black}{2.4913 } & \textcolor{black}{0.4879 }& \textcolor{black}{0.2297}& \textcolor{black}{0.4351} & \textcolor{black}{0.3786 }& 0.8452 & 0.8693 & 0.8569 \\
\cline{2-10}
 & Test & 2.4673  & 0.4867 & 0.2240 & 0.4324 & 0.3694 & 0.8419 & 0.8682 & 0.8546 \\
\hline

\multirow{\textbf{T5}} & Validation & 2.4859 & 0.4991 & 0.2363 & 0.4375 & 0.3814 & 0.8452 & 0.8693 & 0.8569 \\
\cline{2-10}
 & Test & 2.4674 & 0.4969 & 0.2235 & 0.4329 & 0.3787 & 0.8419 & 0.8682 & 0.8546 \\
\hline

\multirow {\textbf{FLAN-T5}} & Validation & 2.3082 & 0.5179 & 0.2508 & 0.4552 & 0.4063 & 0.8524 & 0.8743 & 0.8631 \\
\cline{2-10}
 & Test & 2.2869 & 0.5115 & 0.2372 & 0.4486 & 0.3994 & 0.8488 & 0.8725 & 0.8603 \\
\hline

\multirow{\textbf{BART}} & Validation & 2.7342 & 0.5194 & 0.2474 & 0.4417 & 0.4096 & 0.8691 & 0.8766 & 0.8728 \\
\cline{2-10}
 & Test & 2.7282 & 0.5121 & 0.2310 & 0.4325 & 0.3978 & 0.8665 & 0.8746 & 0.8705 \\
\hline


\end{longtable}



\subsection{Experiments: 4. LoRA Parameter-Efficient Fine-Tuning}
From previous experiments, we obtained improved results with full parameter fine-tuning. To evaluate computational efficiency, we compare full parameter fine-tuning with parameter-efficient fine-tuning (LoRA) on domain-specific clinical data. The LoRA configuration employs a rank ($r$) of 16 and an alpha value of 32, creating low-rank decomposition matrices that adapt only a small subset of the model's parameters. \textcolor{red}{In all models, LoRA adapters are applied to the attention projection layers (\texttt{q\_proj, k\_proj, v\_proj, o\_proj}) and the feed-forward projection layers (\texttt{gate\_proj, up\_proj, down\_proj}) with a dropout of 0.1 (\texttt{lora\_dropout=0.05}), while all remaining parameters are frozen.} LoRA is particularly effective when applied to the attention mechanism in transformer-based architectures.



LoRA \parencite{liao_dynamic_2025} is a prevalent parameter-efficient fine-tuning approach for large-scale models. The method proposes isolating the weight deltas from fine-tuning and approximating them using low-rank matrices. During inference, both the (frozen) pre-trained model and the low-rank deltas, referred to as \textit{adapters}, are forward-passed, and their activations are aggregated. Let $\bm{W} \in \mathbb{R}^{d \times k}$ be a pre-trained weight matrix. LoRA approximates the modifications from fine-tuning as:
\begin{equation}
\Delta \bm{W} \approx \bm{B} \bm{A}, \quad \bm{B} \in \mathbb{R}^{d \times r},\ \bm{A} \in \mathbb{R}^{r \times k},\ r \ll \min(d, k)
\end{equation}

Consequently, inference on an input $\bm{x} \in \mathbb{R}^{d}$ is represented as:

\begin{equation}
\bm{W} \bm{x} + \bm{B} \bm{A} \bm{x} \approx (\bm{W} + \Delta \bm{W}) \bm{x}
\end{equation}

Here, $\bm{A}$ and $\bm{B}$ are directly updated using backpropagation. LoRA is typically applied to the query and value matrices within the self-attention layers of the pre-trained transformer. To optimize for supervised tasks, an additional classification head is appended to the final layer of the model. 

\subsubsection{PEGASUS-XSUM Using LoRA Parameter-Efficient Fine-Tuning Performance}
\textcolor{red}{LoRA parameter-efficient fine-tuning enabled PEGASUS-XSUM to exceed both the zero-shot baseline and full parameter fine-tuning techniques. PEGASUS-XSUM employing LoRA demonstrated improved performance across all assessment measures}, as seen in Figure \ref{fig:Pegasus_lora_training}.  The LoRA method outperformed full parameter fine-tuning. This indicates that the system preserves clinical terminology and bigram sequences. The BERTScore metrics showed improved semantic alignment, producing F1 scores of 0.8699 for validation and 0.8729 for the test. These indicated the highest semantic similarity scores among parameter-efficient algorithms. Figure \ref{fig:Pegasus_lora_loss} depicts the advancement of the instruction. The training and validation losses consistently diminished.  

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with LoRa training.png}
        \caption{PEGASUS-XSUM  LoRa Training}
        \label{fig:Pegasus_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Pegasus with LoRa loss.png}
        \caption{PEGASUS-XSUM  LoRa Loss}
        \label{fig:Pegasus_lora_loss}
    \end{minipage}
\end{figure}

\subsubsection{T5 using LoRA Parameter-Efficient Fine-Tuning Performance}
T5 using LoRA parameter-efficient fine-tuning showed improved performance in abstractive medical text summarization compared to its zero-shot and full fine-tuning options. As shown in Table \ref{tab:model_performance_longtable}, LoRA fine-tuning improved its performance on the test set. The semantic understanding, assessed through BERTScore, improved with LoRA. The data, shown in the bar chart in Figure \ref{fig:t5_lora_training}, indicate T5's summarization performance when tuned. Figure \ref{fig:t5_lora_loss} shows a consistent reduction in both training and validation loss over 5 epochs, with validation loss stabilizing around 2.2496, suggesting effective learning and generalization without overfitting. LoRA fine-tuning of T5 delivers performance that matches or exceeds full fine-tuning while utilizing parameter efficiency. T5 performed lower than BART and PEGASUS-XSUM using LoRA.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with LoRa training.png}
        \caption{T5 with LoRa Training}
        \label{fig:t5_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{T5 with LoRa loss.png}
        \caption{T5 with LoRa Loss}
        \label{fig:t5_lora_loss}
    \end{minipage}
\end{figure}
\subsubsection{FLAN-T5 Using LoRA Parameter-Efficient Fine-Tuning Performance}
As seen in Figure \ref{fig:Flan_lora_training}, Flan-T5 showed improved performance when utilizing LoRA parameter-efficient fine-tuning for abstractive medical text summarization. Furthermore, the loss curve in Figure \ref{fig:Flan_lora_loss} illustrates a consistent decline in both training and validation loss over 5 epochs, with validation loss stabilizing at 2.0386. This shows that Flan-T5 with LoRA generates medical summaries with improved semantic quality. 
\vspace{-2}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with LoRa training.png}
        \caption{FLAN with LoRa Training}
        \label{fig:Flan_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Flan with LoRa loss.png}
        \caption{FLAN with LoRa Loss}
        \label{fig:Flan_lora_loss}
    \end{minipage}
\end{figure}

\subsubsection{BART Using LoRA Parameter-Efficient Fine-Tuning Performance}
BART showed improved performance using LoRA parameter-efficient fine-tuning, achieving improvements over both zero-shot baselines and full parameter fine-tuning methods. Figure \ref{fig:Bart_lora_training} demonstrates that BART with LoRA achieved improved results across all evaluation metrics. BART with LoRA achieved higher ROUGE-L scores, demonstrating structural coherence and exceeding the full parameter fine-tuning performance. Figure \ref{fig:Bart_lora_loss} illustrates stable training dynamics, with training and validation losses smoothly converging, and signifying a reliable and effective learning path free from overfitting. Figure \ref{fig:Bart_lora_loss} showcases typical training dynamics, where validation losses steadily converge from initial values to final values of 0.2 and 0.9, respectively, highlighting effective learning advancement, but the validation loss does not converge as the training loss does.


\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with LoRa training.png}
        \caption{BART with LoRa Training}
        \label{fig:Bart_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bart with LoRa loss.png}
        \caption{BART with LoRa Loss}
        \label{fig:Bart_lora_loss}
    \end{minipage}
\end{figure}


\subsubsection{ Mistral-7B Fine-Tuning with LoRA}
Mistral-7B showed improved performance via LoRA parameter-efficient fine-tuning, positioning itself among the leading models. As shown in Figure \ref{fig:Misteral_lora_training}, Mistral-7B with LoRA performed well on all evaluation metrics, achieving ROUGE-1 scores of 0.6500 (validation) and 0.6507 (test), indicating improvements compared to conventional transformer models. Similarly, the Figure \ref{fig:Misteral_lora_loss} showcases typical training dynamics, where both training and validation losses steadily converge from initial values around 1.6 and 1.35 to final values of 0.2 and 0.9, respectively, highlighting effective learning advancement, but the validation loss does not converge as the training loss does. 

\textcolor{red}{Compared with traditional transformer models, the Mistral-7B with LoRA achieved higher scores across all assessment parameters. The improvement in clinical accuracy and semantic similarity is reflected in the increased BERTScore F1.} These results indicate that Mistral-7B with LoRA is an exceptional choice for clinical text summarization, as it provides state-of-the-art performance while maintaining the advantages of LoRA fine-tuning in terms of parameter efficacy. Consequently, it is the optimal choice for healthcare institutions that require both exceptional computational efficiency and accuracy.


\subsubsection{LLaMA-3-8B Fine-Tuning with LoRA}
\textcolor{red}{LLaMA-3-8B achieved the highest scores among all evaluated models and training methods using LoRA fine-tuning.} As shown in Figure \ref{fig:Llama_lora_training}, LLaMA-3-8B with LoRA achieved the highest results on all evaluation metrics, with ROUGE-1 scores of 0.6970 (validation) and 0.7022 (test), outperforming other models including traditional transformers and large language models. The model achieved higher METEOR and ROUGE-2 scores (see Table \ref{tab:model_performance_longtable}), indicating it preserved important clinical terms and complex medical phrases, reflecting understanding of clinical language structures. Figure \ref{fig:Llama_lora_loss} shows the loss and training dynamics of LLaMA-3-8B. LLaMA-3-8B had the lowest loss rates throughout the training cycle, with training and validation losses decreasing steadily. 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Misteral with LoRa training.png}
        \caption{Mistral-7B with LoRa Training}
        \label{fig:Misteral_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Misteral with LoRa loss.png}
        \caption{Mistral-7B with LoRa Loss}
        \label{fig:Misteral_lora_loss}
    \end{minipage}
\end{figure}
 \textcolor{red}{In general, LLaMA-3-8B with LoRA achieved higher scores compared to other evaluated models, including both transformers and Mistral-7B.} In comparison to the second-best model (Mistral-7B with LoRA), LLaMA-3-8B demonstrated enhancements of 7.9\% in ROUGE-1 (0.7022 vs 0.6507), 21.2\% in ROUGE-2 (0.5312 vs 0.4383), 11.5\% in ROUGE-L (0.6718 vs 0.6027), and 11.0\% in METEOR (0.6787 vs 0.6112). The BERTScore F1 increased from 0.9060 to 0.9180, indicating improved semantic precision. These results, along with low training losses and the parameter efficiency of LoRA (only 0.8\% of total parameters), make LLaMA-3-8B with LoRA a suitable choice for summarizing clinical notes, offering accuracy while maintaining computational efficiency for healthcare applications.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Llama with LoRa training.png}
        \caption{LLaMA-3-8B with LoRa Training}
        \label{fig:Llama_lora_training}
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Llama with LoRa loss.png}
        \caption{LLaMA-3-8B with LoRa Loss}
        \label{fig:Llama_lora_loss}
    \end{minipage}
\end{figure}

\textcolor{red}{The LoRA approach achieved higher scores than the full parameter baseline across multiple dimensions. These results suggest that LoRA provides an effective training strategy that achieves comparable or improved performance while requiring fewer trainable parameters and computational resources for clinical text summarization applications.} Figure \ref{fig:placeholder}, shows the generated summary of one patient's record using fine-tuned LLAMA-3-8B.


\begin{longtable} [htbp]{|p{1.6cm}|p{1.3cm}|p{0.9cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.45cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}|}
\caption{Performance metrics for different models across validation and test stages, including loss, ROUGE scores, METEOR, and BERTScore metrics with LoRA Fine-Tuning.\label{tab:model_performance_longtable}} \\
\hline
\textbf{Model} & \textbf{Stage} & \textbf{Loss} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERT- \linebreak Score P} & \textbf{BERT- \linebreak Score R} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endfirsthead
\multicolumn{10}{c}{{\tablename\ \thetable{} ....Continued from previous page}} \\
\hline
\textbf{Model} & \textbf{Stage} & \textbf{Loss} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{METEOR} & \textbf{BERT- \linebreak Score P} & \textbf{BERT- \linebreak Score  R} & \textbf{BERT- \linebreak Score F1} \\
\hline
\endhead
\hline \multicolumn{10}{|r|}{{....Continued on next page}} \\ \hline
\endfoot
\hline
\endlastfoot
\multirow{\textbf{PEGASUS \linebreak -XSUM}} & Validation & 3.3920 & 0.4860 & 0.2122 & 0.4186 & 0.3831 & 0.8702 & 0.8696 & 0.8699 \\
\cline{2-10}
 & Test & 3.2557 & 0.4988 & 0.2312 & 0.4321 & 0.3993 & 0.8729 & 0.8729 & 0.8729 \\
\hline
\multirow{\textbf{T5}} & Validation & 2.2496 & 0.5135 & 0.2523 & 0.4546 & 0.4113 & 0.8641 & 0.8754 & 0.8696 \\
\cline{2-10}
 & Test & 2.1861 & 0.5313 & 0.2752 & 0.4742 & 0.4310 & 0.8685 & 0.8791 & 0.8737 \\
\hline
\multirow{\textbf{FLAN-T5}} & Validation & 2.1090 & 0.5327 & 0.2700 & 0.4733 & 0.4380 & 0.8695 & 0.8814 & 0.8752 \\
\cline{2-10}
 & Test & 2.0386 & 0.5517 & 0.2969 & 0.4934 & 0.4580 & 0.8740 & 0.8853 & 0.8795 \\
\hline
\multirow{\textbf{BART}} & Validation & 2.6070 & 0.5235 & 0.2459 & 0.4464 & 0.4228 & 0.8652 & 0.8787 & 0.8716 \\
\cline{2-10}
 & Test & 2.5197 & 0.5390 & 0.2678 & 0.4637 & 0.4368 & 0.8691 & 0.8821 & 0.8753 \\
\hline

\multirow{\textbf{Mistral-7B}} & Validation & 1.4029 & 0.6500 & 0.4333 & 0.5991 & 0.6067 & 0.8990 & 0.9108 & 0.9049 \\
\cline{2-10}
 & Test & 1.3824 & 0.6507 & 0.4383 & 0.6027 & 0.6112 & 0.9004 & 0.9117 & 0.9060 \\
\hline

\multirow{\textbf{LLaMA-3B}} & Validation & \textbf{0.7848} & \textbf{0.6970} & \textbf{0.5230} & \textbf{0.6659 }& \textbf{0.6709} & \textbf{0.9104} & \textbf{0.9224} & \textbf{0.9163 }\\
\cline{2-10}
 & Test & \textbf{0.9875} & \textbf{0.7022} & \textbf{0.5312} & \textbf{0.6718} & \textbf{0.6787} & \textbf{0.9124} & \textbf{0.9238} & \textbf{0.9180} \\
\hline
\end{longtable}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\linewidth]{summary sample.png}
    \caption{A sample generated summary of a clinical note. The figure illustrates an example of a discharge summary generated by our model, showing how abstractive strategies summarize key patient information while maintaining clinical accuracy.}
    \label{fig:placeholder}
\end{figure}

\subsection{Statistical Significance}
\label{sec:statistical_significance}

When evaluating model performance, numerical differences in evaluation metrics require statistical validation to determine whether they represent meaningful improvements or fall within the expected margin of error. For instance, when comparing ROUGE-1 scores of 0.7022 versus 0.6507, formal significance testing is needed to assess whether this difference is statistically meaningful rather than an artifact of sampling variation. Therefore, we conducted comprehensive statistical significance testing for all key model comparisons in our study. We employed two complementary non-parametric statistical methods on the test set, selected for their robustness and appropriateness for NLP evaluation metrics:
\begin{enumerate}
\item \textbf{Bootstrap Confidence Intervals:} We computed 95\% confidence intervals for all evaluation metrics using 10,000 bootstrap resampling iterations. This approach provides robust uncertainty estimates without requiring assumptions about the underlying distribution of metric scores, which is particularly appropriate given that ROUGE and BERTScore distributions may be non-normal in clinical text.

\item \textbf{Paired Bootstrap Permutation Test:} For pairwise model comparisons, we used paired bootstrap permutation testing with 10,000 iterations. This method accounts for the correlation between model predictions on the same samples and provides exact p-values for statistical significance.
\end{enumerate}
\subsubsection{Bootstrap Confidence Intervals for All Models}

Table~\ref{tab:ci_all_models} presents 95\% bootstrap confidence intervals for all models across four key metrics: ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore F1. Each cell shows the point estimate (mean score) in the first row and the confidence interval bounds in brackets in the second row. These intervals quantify the uncertainty in our performance estimates and allow us to assess the stability and reliability of each model's results.

\begin{table}[htbp]
\caption{95\% Bootstrap Confidence Intervals for All Models}
\label{tab:ci_all_models}
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{BERTScore F1} \\
\hline
\multicolumn{5}{|c|}{\textbf{Large Language Models (LoRA)}} \\
\hline
LLaMA-3-8B-LoRA & 0.7022 & 0.5312 & 0.6718 & 0.9180 \\
 & [0.6922, 0.7123] & [0.5187, 0.5439] & [0.6610, 0.6827] & [0.9145, 0.9216] \\
\hline
Mistral-7B-LoRA & 0.6507 & 0.4383 & 0.6027 & 0.9060 \\
 & [0.6407, 0.6608] & [0.4258, 0.4510] & [0.5919, 0.6137] & [0.9020, 0.9101] \\
\hline
\multicolumn{5}{|c|}{\textbf{Encoder-Decoder Transformers (LoRA)}} \\
\hline
FLAN-T5-LoRA & 0.5517 & 0.2969 & 0.4934 & 0.8795 \\
 & [0.5417, 0.5619] & [0.2847, 0.3094] & [0.4825, 0.5044] & [0.8744, 0.8847] \\
\hline
BART-LoRA & 0.5390 & 0.2678 & 0.4637 & 0.8753 \\
 & [0.5290, 0.5492] & [0.2568, 0.2790] & [0.4528, 0.4747] & [0.8700, 0.8807] \\
\hline
T5-LoRA & 0.5313 & 0.2752 & 0.4742 & 0.8737 \\
 & [0.5213, 0.5415] & [0.2638, 0.2868] & [0.4633, 0.4852] & [0.8684, 0.8792] \\
\hline
PEGASUS-LoRA & 0.4988 & 0.2312 & 0.4321 & 0.8729 \\
 & [0.4888, 0.5090] & [0.2217, 0.2409] & [0.4212, 0.4431] & [0.8675, 0.8784] \\
\hline
\multicolumn{5}{|c|}{\textbf{Encoder-Decoder Transformers (Full Fine-Tuning)}} \\
\hline
BART-Full & 0.5121 & 0.2310 & 0.4325 & 0.8705 \\
 & [0.5021, 0.5223] & [0.2215, 0.2407] & [0.4216, 0.4435] & [0.8650, 0.8761] \\
\hline
FLAN-T5-Full & 0.5115 & 0.2372 & 0.4486 & 0.8603 \\
 & [0.5015, 0.5217] & [0.2274, 0.2472] & [0.4377, 0.4596] & [0.8544, 0.8664] \\
\hline
T5-Full & 0.4969 & 0.2235 & 0.4329 & 0.8546 \\
 & [0.4869, 0.5071] & [0.2143, 0.2329] & [0.4220, 0.4439] & [0.8485, 0.8609] \\
\hline
PEGASUS-Full & 0.4867 & 0.2240 & 0.4324 & 0.8546 \\
 & [0.4767, 0.4969] & [0.2148, 0.2334] & [0.4215, 0.4434] & [0.8485, 0.8609] \\
\hline
\end{tabular}
\end{table}

The confidence intervals reveal several important patterns regarding the reliability and comparative performance of the evaluated models.\\
First, all models exhibit narrow confidence intervals, with typical widths of ±1\% for ROUGE metrics and ±0.3\% for BERTScore F1. For example, LLaMA-3-8B-LoRA's ROUGE-1 interval [0.6922, 0.7123] spans only 2.01 percentage points, indicating stable performance across different bootstrap samples of the test set. This narrow range suggests that the model's performance is consistent and not heavily influenced by particular subsets of the test data.

Second, examining the intervals across model categories reveals clear performance tiers. The large language models (LLaMA-3-8B-LoRA and Mistral-7B-LoRA) occupy distinctly higher performance ranges compared to encoder-decoder transformers. LLaMA-3-8B-LoRA's ROUGE-1 interval [0.6922, 0.7123] does not overlap at all with the highest-performing traditional transformer, FLAN-T5-LoRA [0.5417, 0.5619]. This complete separation of confidence intervals provides initial visual evidence that the performance difference is not within the margin of error, even before conducting formal significance tests.

Third, when comparing LoRA versus full fine-tuning for the same base model, we observe non-overlapping intervals in most cases. For instance, BART-LoRA's ROUGE-1 interval [0.5290, 0.5492] is entirely above BART-Full's interval [0.5021, 0.5223], with no overlap between the two ranges. This pattern holds across FLAN-T5 and T5 as well, suggesting that LoRA's advantages over full fine-tuning are statistically robust rather than within sampling variation.

Fourth, the confidence intervals for ROUGE-2 are generally wider than those for ROUGE-1, which may reflect the greater variability in bigram matching compared to unigram matching. For example, LLaMA-3-8B-LoRA's ROUGE-2 interval [0.5187, 0.5439] spans 2.52 percentage points, compared to 2.01 points for ROUGE-1. This pattern is consistent with prior NLP research showing that higher-order n-gram metrics tend to have higher variance.

Finally, BERTScore F1 intervals are consistently narrower than ROUGE intervals across all models, with most spanning less than 0.7 percentage points. This tighter precision may reflect BERTScore's use of contextual embeddings, which could provide more stable semantic similarity estimates compared to exact lexical matching. These narrow intervals suggest high confidence in the reported BERTScore values.

\subsubsection{Statistical Significance Tests: LoRA vs. Full Parameter Fine-Tuning}

Table~\ref{tab:sig_lora_vs_full} presents pairwise significance tests comparing LoRA against full fine-tuning for each encoder-decoder model architecture. For each comparison, the table shows the LoRA model's score, the absolute improvement, the relative improvement percentage, and the p-value from the paired bootstrap permutation test. All comparisons use 10,000 permutation iterations to ensure robust p-value estimates.
\begin{table}[htbp]
\caption{Statistical Significance: LoRA vs. Full Fine-Tuning. 
All comparisons are statistically significant with $p < 0.001$ based on a paired bootstrap permutation test with 10{,}000 iterations.}
\label{tab:sig_lora_vs_full}
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Comparison} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{BERTScore F1} \\
\hline
FLAN-T5-LoRA & 0.5517 & 0.2969 & 0.4934 & 0.8795 \\
vs FLAN-T5-Full & +0.0402 (+7.9\%) & +0.0597 (+25.2\%) & +0.0448 (+10.0\%) & +0.0192 (+2.2\%) \\
\hline
T5-LoRA & 0.5313 & 0.2752 & 0.4742 & 0.8737 \\
vs T5-Full & +0.0344 (+6.9\%) & +0.0517 (+23.1\%) & +0.0413 (+9.5\%) & +0.0191 (+2.2\%) \\
\hline
BART-LoRA & 0.5390 & 0.2678 & 0.4637 & 0.8753 \\
vs BART-Full & +0.0269 (+5.3\%) & +0.0368 (+15.9\%) & +0.0312 (+7.2\%) & +0.0048 (+0.6\%) \\
\hline
PEGASUS-LoRA & 0.4988 & 0.2312 & 0.4321 & 0.8729 \\
vs PEGASUS-Full & +0.0121 (+2.5\%) & +0.0072 (+3.2\%) & -0.0003 (-0.1\%) & +0.0183 (+2.1\%) \\
\hline
\end{tabular}
\end{table}



The statistical significance tests reveal consistent patterns across all four model architectures examined. First, all pairwise comparisons between LoRA and full fine-tuning yield p-values below 0.001, providing strong evidence that the observed performance differences are unlikely to be due to random variation. This finding holds across all models despite varying magnitudes of improvement, suggesting that LoRA's advantages are statistically robust.

Second, examining the magnitude of improvements reveals notable variation across models. FLAN-T5-LoRA shows the largest gains, with +7.9\% relative improvement in ROUGE-1 and +25.2\% improvement in ROUGE-2. T5-LoRA shows similar patterns with +6.9\% ROUGE-1 and +23.1\% ROUGE-2 improvements. BART-LoRA achieves more modest but still statistically significant gains of +5.3\% in ROUGE-1 and +15.9\% in ROUGE-2. PEGASUS-LoRA shows the smallest improvements at +2.5\% ROUGE-1, though the p < 0.001 result confirms this difference remains statistically meaningful.

Third, a consistent pattern emerges across all models: the relative improvements in ROUGE-2 substantially exceed those in ROUGE-1. For example, while FLAN-T5-LoRA improves ROUGE-1 by 7.9\%, ROUGE-2 improves by 25.2\%—more than three times larger. This pattern suggests that LoRA may be particularly effective at preserving bigram sequences, which could be clinically relevant for maintaining multi-word medical terminology. The preservation of these bigrams may contribute to better clinical accuracy in generated summaries.

Fourth, PEGASUS-LoRA presents an interesting case where performance patterns vary across metrics. While showing modest improvements in ROUGE-1 (+2.5\%) and ROUGE-2 (+3.2\%), PEGASUS-LoRA exhibits a marginal decrease in ROUGE-L (-0.1\%). However, it achieves a +2.1\% improvement in BERTScore F1, suggesting that semantic quality improvements may not always align with lexical overlap metrics. This finding highlights the importance of using multiple evaluation metrics to capture different aspects of summarization quality.

Finally, BERTScore F1 improvements are generally smaller in relative terms (+0.6\% to +2.2\%) compared to ROUGE improvements, which may reflect the fact that BERTScore is already near its ceiling for both LoRA and full fine-tuning (scores in the 0.85-0.88 range). The statistical significance of even these smaller improvements indicates that LoRA provides measurable semantic quality gains beyond what full fine-tuning achieves.

These results suggest that LoRA fine-tuning offers statistically significant and consistent advantages over full fine-tuning for the encoder-decoder models tested. The magnitude of improvement varies by model architecture, with FLAN-T5 and T5 showing the largest gains, but all four models demonstrate statistically robust improvements that exceed the margin of error.

\subsubsection{Statistical Significance Tests: LLMs vs. Traditional Transformers}

Table~\ref{tab:sig_llm_vs_transformer} presents significance tests comparing large language models (LLaMA-3-8B-LoRA and Mistral-7B-LoRA) against traditional encoder-decoder transformers (BART-Full and FLAN-T5-Full). These comparisons address whether decoder-only LLM architectures offer performance advantages for clinical summarization beyond what encoder-decoder models can achieve.

\begin{table}[htbp]
\caption{Statistical Significance: Large Language Models vs. Traditional Transformers.
All comparisons are statistically significant with $p < 0.001$ based on a paired bootstrap permutation test with 10,000 iterations.}
\label{tab:sig_llm_vs_transformer}
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Comparison} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{BERTScore F1} \\
\hline
LLaMA-3-8B-LoRA & 0.7022 & 0.5312 & 0.6718 & 0.9180 \\
vs BART-Full & +0.1901 (+37.1\%) & +0.3002 (+130.0\%) & +0.2393 (+55.3\%) & +0.0475 (+5.5\%) \\
\hline
LLaMA-3-8B-LoRA & 0.7022 & 0.5312 & 0.6718 & 0.9180 \\
vs FLAN-T5-Full & +0.1907 (+37.3\%) & +0.2940 (+123.9\%) & +0.2232 (+49.8\%) & +0.0577 (+6.7\%) \\
\hline
Mistral-7B-LoRA & 0.6507 & 0.4383 & 0.6027 & 0.9060 \\
vs BART-Full & +0.1386 (+27.1\%) & +0.2073 (+89.7\%) & +0.1702 (+39.4\%) & +0.0355 (+4.1\%) \\
\hline
Mistral-7B-LoRA & 0.6507 & 0.4383 & 0.6027 & 0.9060 \\
vs FLAN-T5-Full & +0.1392 (+27.2\%) & +0.2011 (+84.8\%) & +0.1541 (+34.4\%) & +0.0457 (+5.3\%) \\
\hline
\end{tabular}
\end{table}


The statistical comparisons reveal consistent performance advantages of large language models over traditional encoder-decoder transformers across all metrics examined. These findings warrant detailed examination given their implications for clinical NLP system design.

First, LLaMA-3-8B-LoRA shows large performance gains over both BART-Full and FLAN-T5-Full. Against BART-Full, LLaMA-3-8B-LoRA achieves +37.1\% relative improvement in ROUGE-1 (from 0.5121 to 0.7022) and +130.0\% improvement in ROUGE-2 (from 0.2310 to 0.5312). Against FLAN-T5-Full, the improvements are similar at +37.3\% ROUGE-1 and +123.9\% ROUGE-2. The magnitude of these improvements is notably larger than the LoRA vs. full fine-tuning comparisons, suggesting that architectural differences between decoder-only LLMs and encoder-decoder transformers may have greater impact on clinical summarization performance than the choice of fine-tuning strategy.

Second, Mistral-7B-LoRA also shows advantages over traditional transformers, though not as large as LLaMA-3-8B-LoRA. Mistral-7B achieves +27.1\% ROUGE-1 over BART-Full and +27.2\% over FLAN-T5-Full, with ROUGE-2 improvements of +89.7\% and +84.8\% respectively. These results indicate that the LLM architecture advantage is not specific to LLaMA-3-8B but appears to be a generalizable characteristic of decoder-only models fine-tuned for clinical summarization. The consistency of improvements across two different LLM implementations (LLaMA-3-8B and Mistral-7B) provides additional confidence that these findings are not artifacts of a particular model implementation.

Finally, the BERTScore F1 improvements, while smaller in relative terms (+4.1\% to +6.7\%), are still statistically significant and may be clinically meaningful. Moving from BERTScore F1 of 0.8705 (BART-Full) to 0.9180 (LLaMA-3-8B-LoRA) represents an absolute gain of 0.0475 points, which in semantic similarity metrics could correspond to noticeable differences in clinical content fidelity. These improvements suggest that LLMs not only match reference summaries better lexically but also capture semantic meaning more accurately.

These findings collectively suggest that LLM architectures offer performance advantages over encoder-decoder transformers for clinical summarization, with improvements that are both statistically significant and large in magnitude. 

\subsubsection{ Statistical Analysis Interpretation}

Our comprehensive statistical analysis addresses the fundamental question of whether observed performance differences represent meaningful improvements or fall within the expected margin of error. The results provide several key insights:

\textbf{1. Universal statistical significance:} All 13 pairwise comparisons conducted in this study yield p-values below 0.001, providing strong statistical evidence that the observed performance differences are unlikely to be due to random sampling variation alone. This consistency across diverse model comparisons (LoRA vs. full fine-tuning, LLMs vs. transformers) suggests that the findings are statistically robust. 

\textbf{2. LoRA demonstrates consistent advantages over full fine-tuning:} Across all four transformer models tested (BART, T5, FLAN-T5, PEGASUS), LoRA fine-tuning achieves statistically significant improvements compared to full parameter fine-tuning. The ROUGE-1 improvements range from +2.5\% (PEGASUS) to +7.9\% (FLAN-T5), while ROUGE-2 improvements range from +3.2\% to +25.2\%. These results suggest that parameter-efficient LoRA adaptation can achieve competitive or superior performance compared to full fine-tuning while using only 0.8\% to 4.8\% of trainable parameters. The consistency of improvements across different model architectures provides evidence that LoRA's advantages may generalize across encoder-decoder transformer families.

\textbf{3. Large language models show substantial advantages over encoder-decoder transformers:} The performance differences between LLMs (LLaMA-3-8B, Mistral-7B) and traditional transformers (BART, FLAN-T5) are notably larger in magnitude than the differences between LoRA and full fine-tuning. ROUGE-1 improvements range from +27\% to +37\%, while ROUGE-2 improvements reach +85\% to +130\%. Combined with completely non-overlapping confidence intervals and p $< 0.001$, these findings suggest that architectural differences between decoder-only LLMs and encoder-decoder transformers may have a substantial impact on clinical summarization performance. 

\textbf{4. Performance stability across test samples:} The narrow bootstrap confidence intervals (typically ±1\% for ROUGE metrics, ±0.3\% for BERTScore F1) indicate that model performance is relatively stable across different subsamples of the test set. 

 

\subsection{ Model Performance Comparison  Across Different Training Approaches}
The radar chart presented in Figure \ref{fig:radarchar} offers an in-depth performance comparison of four distinct models assessed across various training and fine-tuning approaches. Throughout our experiments, LexRank, an extractive summarization method, consistently achieved the highest performance among the models evaluated. Similarly, the LLaMA-3-8B model, utilized in its pre-trained state without any fine-tuning, exhibited strong capabilities in summarization tasks. Moreover, the FLAN-T5 model, which underwent comprehensive fine-tuning, demonstrated adaptability and effectiveness, underscoring the benefits of extensive training. Additionally, the LLaMA-3-8B model enhanced with LoRA, a technique focused on parameter-efficient fine-tuning, also yielded improved results, further illustrating its ability to optimize performance with minimal adjustments. This visualization not only highlights the comparative strengths of these methodologies but also serves as a valuable reference for researchers and practitioners seeking to select appropriate model architecture and training strategies for text summarization endeavors. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{comprehensive_radar_chart.png}
    \caption{Performance comparison of representative models across different training approaches using radar chart visualization. The chart displays seven evaluation metrics for four model categories: extractive methods, pre-trained models without fine-tuning, full fine-tuning, and PEFT with LoRA.}
    \label{fig:radarchar}
\end{figure}
\subsection{Evaluation Pipeline Using Graphical User Interface}

In addition to the modeling and evaluation pipeline, we developed a Graphical User Interface (GUI) using the Tkinter framework and Hugging Face to enable clinical and research professionals to interact with the summarization system intuitively. We pushed our final fine-tuned models into Hugging Face and connected with the GUI. The GUI supports multiple functionalities, allowing users to upload or browse medical text files, extract clinical named entities using a cTAKES-based pipeline, and generate summaries with an emphasis on sentences containing clinical entities (disease/disorder mentions, medication mentions, and procedures). The interface also integrates advanced NLP visualizations and analyses, including named entity recognition, chunk parsing, constituency and dependency parsing, and semantic role labeling, to provide interpretability and linguistic insight. These features are particularly useful for medical informatics researchers and clinicians who require transparency and explainability in text analytics workflows.

To enhance real-world usability, the system enables text input from URLs, direct pasting, or file upload. Processed text is analyzed and displayed in an interactive output window, making the GUI a practical extension of our summarization engine. By bridging backend summarization logic with a deployable frontend interface, this tool underscores the translational potential of our work and supports further exploration in clinical environments. A full implementation of the GUI, including cTAKES integration and summarization functionalities, has been made available as part of our supplementary materials to demonstrate the applied value of our contributions beyond experimental results.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.1\linewidth]{Graphical User Interface.png}
    \caption{Tkinter-based Graphical User Interface for Summarization, illustrating the interactive layout where users can input clinical notes, configure summarization options, and view generated outputs.}
    \label{Comparison of results}
\end{figure}


\section{Discussion}
\textcolor{red}{In this research, we evaluate fine-tuning strategies, particularly LoRA, for transformer models in abstractive medical text summarization. Our experiments indicate that LoRA fine-tuning matches or exceeds full parameter fine-tuning performance in most evaluated conditions while consuming fewer computational resources. These findings provide evidence that challenges the assumption that more trainable parameters necessarily lead to higher performance.} We systematically evaluated graph-based extractive methods along with four traditional transformer models (BART, T5, Pegasus-xsum, and Flan-T5) and two large language models (LLaMA-3-8B and Mistral-7B). Our results indicate the impact of fine-tuning on summarization quality in the specialized medical field. Unlike zero-shot approaches, which performed worse in all areas, both complete fine-tuning and LoRA fine-tuning showed improvements, highlighting the necessity for adjustments tailored to the medical domain. Consequently, fine-tuning large language models using domain-specific datasets leads to better performance compared to the results obtained before applying fine-tuning, as shown in the results section.

This study is important for clinical decision support as it provides an efficient and accurate method for summarizing complex medical records. We utilized LoRA fine-tuning to develop a model capable of reading numerous patient discharge summaries and generating concise abstracts. This capability is particularly valuable for doctors who face an overwhelming amount of information and need to quickly identify the most important details to make informed decisions. Our approach not only accelerates and improves the quality of information retrieval, but it also makes this advanced technology accessible to hospitals and clinics with limited budgets and computing resources by reducing the required processing power.

Additionally, the research evaluated the computational advantages of LoRA. LoRA reduced the trainable parameters by approximately 99\% compared to full fine-tuning, leading to a decrease in training time (averaging approximately 5× faster) and lowered GPU memory consumption. The improvements in efficiency are important for real-world applications and ongoing development in healthcare environments with limited resources.

Beyond this, we thoroughly examined how our research questions are addressed. We analyzed the methodologies employed and highlighted the findings that provide insight into each question posed. \\ 
\textbf{RQ1:}  Across all architectures, fine-tuning improved summarization quality compared to zero-shot inference. LLMs with LoRA consistently achieved the highest scores (for instance, LLaMA-3-8B with ROUGE-1: 0.7022, BERTScore F1: 0.9180), outperforming both transformers with LoRA and full parameter fine-tuning. This finding indicates that performance gains are not solely a function of model size but also of efficient adaptation strategies. The ranking observed that LLMs with LoRA exceeded extractive baselines, showing that neural abstractive techniques offer essential benefits in clinical comprehension, semantic retention, and content generation that rule-based extractive strategies fail to deliver.\\
\textbf{RQ2:}  While extractive methods such as LexRank and TextRank are computationally inexpensive, their performance remained limited (example: ROUGE-1 $\approx$ 0.25), reflecting poor alignment with abstractive gold summaries. In contrast, neural abstractive approaches produced semantically richer and more coherent summaries, even under zero-shot conditions. Fine-tuned models amplified this advantage, demonstrating that abstractive approaches are essential for capturing clinical reasoning and narrative flow—capabilities unattainable with sentence ranking alone.

\textbf{RQ3:} LoRA achieves superior results with only 0.8-4.8\% trainable parameters, while also outperforming both computationally lightweight extractive methods and resource-intensive complete fine-tuning by striking the optimal equilibrium between performance and computational efficiency. LoRA achieves 2.6× better performance than extractive methods with moderate computational requirements and 62-74\% GPU memory reduction compared to full fine-tuning, in contrast to extractive algorithms, which require minimal resources but deliver inadequate clinical performance (ROUGE-1: ~0.25). This makes high-performance clinical summarization accessible to resource-constrained healthcare areas. 

\textbf{RQ4:}  Zero-shot summarization using structured prompts exhibits varying effectiveness across different model architectures, with large language models (LLMs) demonstrating a stronger baseline understanding of clinical contexts compared to traditional transformers. Our tailored prompting strategies ranged from straightforward task descriptions for conventional models to complex system-user prompt combinations for LLMs, facilitating zero-shot performance. Notably, LLaMA-3-8B and Mistral-7B surpassed traditional transformers and extractive baselines. The use of structured prompts not only enhanced factual coherence but also reduced hallucinations by constraining output formats and aligning models with clinical terminology rules. Subsequent fine-tuning led to improvements in clinical accuracy and coherence, with LoRA fine-tuning yielding the highest performance in LLaMA-3-8B. The improvements in BERTScore from zero-shot to LoRA fine-tuning reflect enhanced semantic accuracy and reduced hallucination rates. This underscores how fine-tuning builds upon the foundation established by structured prompts to achieve improved factual consistency and medical coherence, which are essential for clinical applications.

\textcolor{red}{
\subsection{Why LoRA Works Well for Clinical Summarization}
In this study, we demonstrate that LoRA consistently outperforms full fine-tuning across all encoder--decoder architectures (+2.5\% to +7.9\% ROUGE-1, $p < 0.001$). This performance advantage extends beyond computational efficiency, prompting a deeper investigation into the mechanisms underlying LoRA's effectiveness in clinical text summarization.
\\
Our analysis suggests that the primary factor is LoRA's implicit regularization effect. By constraining weight updates to a low-rank subspace ($r = 16$), LoRA limits the hypothesis space explored during fine-tuning, effectively preventing overfitting to the limited clinical data. This observation aligns with the intrinsic dimensionality hypothesis proposed by \parencite{aghajanyan_intrinsic_2021}, which shows that successful fine-tuning occurs within low-dimensional subspaces, where updating a small subspace achieves most of the performance of full fine-tuning. In our experiments, LoRA models achieved lower validation losses than fully fine-tuned models (FLAN-T5-LoRA: 2.0386 vs.\ FLAN-T5-Full: 2.2869; T5-LoRA: 2.1861 vs.\ T5-Full: 2.4674), suggesting improved generalization rather than memorization.
\\
Here, we also find that LoRA's low-rank update space is particularly well-suited for capturing clinical concept shifts. Adaptation from general language to the medical domain primarily involves updating specific latent dimensions encoding medical concepts rather than restructuring all model representations \parencite{tang_evaluating_2023}. Our results support this interpretation: ROUGE-2 improvements are disproportionately larger than ROUGE-1 improvements across all models (T5: +23.1\% vs.\ +6.9\%; FLAN-T5: +25.2\% vs.\ +7.9\%; BART: +15.9\% vs.\ +5.3\%). Since ROUGE-2 measures bigram overlap, these gains indicate effective preservation of multi-word medical phrases. The consistency across transformer architectures further suggests that LoRA captures fundamental clinical knowledge patterns rather than architecture-specific artifacts.
\\
In addition, our study shows that LoRA preserves pre-trained knowledge during adaptation. By freezing the original weights ($\mathbf{\textit{W}}$) and training only the low-rank adapters ($\mathbf{\textit{A}}$ and $\mathbf{\textit{B}}$), LoRA maintains the general language understanding acquired during pre-training. This design reduces destructive interference between general language capabilities and specialized medical knowledge \parencite{yang_low-rank_2025}, which is crucial for clinical summarization requiring both precise terminology and coherent language generation.
\\
Overall, these three mechanisms, such as regularization, clinical subspace alignment, and knowledge preservation, operate synergistically in our experiments. Clinical discharge summarization demands precise preservation of medical terminology rather than creative language generation, and LoRA’s constrained adaptation space naturally aligns with this requirement. In this study, we show that LoRA effectively learns targeted clinical vocabulary mappings without disrupting the structured language patterns essential for accurate medical documentation.
}
\textcolor{red}{
\subsection{Summary of Key Findings}
This study presents a comprehensive evaluation of parameter-efficient fine-tuning (PEFT) for clinical text summarization using the MIMIC-IV-Ext-BHC dataset. We systematically compare four transformer models (BART, T5, PEGASUS-XSUM, FLAN-T5) and two large language models (LLaMA-3-8B, Mistral-7B) across zero-shot inference, full fine-tuning, and LoRA fine-tuning strategies. This study yields three principal findings. First, LLaMA-3-8B with LoRA achieves the highest performance across all evaluation metrics (ROUGE-1: 0.7022, BERTScore F1: 0.9180), surpassing both traditional transformers and alternative training approaches. Second, LoRA consistently outperforms full fine-tuning across all six architectures while reducing trainable parameters by approximately 97--99\%, demonstrating that parameter efficiency enhances rather than compromises summarization quality. Third, the results reveal a clear performance hierarchy: LLMs with LoRA outperform transformers with LoRA, which exceed full fine-tuning approaches, followed by zero-shot inference, with graph-based extractive methods yielding the lowest scores. Combined with approximately three times faster training, these efficiency gains enable clinical AI deployment in resource-constrained healthcare environments.}

\subsection{Comparison with Previous Works }

As seen from Table \ref{tab:big_comparison_limitations_long}, previous works in clinical text summarization have often been constrained by their focus on singular models, limited comparative scopes, and a general neglect of parameter-efficient methods like LoRA, frequently failing to report crucial computational metrics. Graph-based extractive methods have been evaluated, but they perform poorly because they cannot capture the complex way humans write clinical summaries. \textcolor{red}{In contrast, our work provides a benchmark across multiple models and strategies, evaluating both traditional transformers and large language models in zero-shot, full fine-tuning, and LoRA adaptation settings. This approach empirically demonstrates LoRA's ability to achieve comparable or improved abstractive summarization quality while reducing computational requirements, with our models achieving higher ROUGE and METEOR scores than previously reported results on the MIMIC-IV dataset.} \\


Beyond the overall improvements, we also examined scenarios where our approach works especially well and where it struggles. Compared with earlier models, for example, FLAN-T5 and BART \parencite{van_veen_clinical_2023}, which dropped sharply in performance when facing domain gaps, our LLaMA-3-8B with LoRA model showed enhanced performance (ROUGE-2: 0.531 versus 0.11). This suggests that LoRA helps capture clinically meaningful language even when the surface wording differs and outperforms previous works. We also found that extractive pipelines such as  \parencite{naemi_benchmarking_2025} could score well on certain semantic metrics but often fail to maintain consistent summary structure. In contrast, our models achieved a stronger balance across both lexical and semantic measures. Still, we found limitations: ambiguous abbreviations and sections outside discharge summaries remain challenging, and the models can show deficiencies. Overall, these cases highlight both the advantages of LoRA and the areas that require further investigation in the clinical summarization domain.

\begin{longtable}{p{2.1cm}p{1.8cm}p{2.2cm}p{0.55cm}p{0.55cm}p{0.55cm}p{0.55cm}p{0.65cm}p{3.5cm}}
\caption{Comparison of previous clinical text summarization methods on the MIMIC-IV dataset. Our proposed LoRA-enhanced models achieve highly competitive results across evaluation metrics while also being more computationally efficient.}
\label{tab:big_comparison_limitations_long} \\
\toprule
\textbf{Authors} & \textbf{Dataset} & \textbf{Approach} & \textbf{R1} & \textbf{R2} & \textbf{RL} & \textbf{M} & \textbf{BF} & \textbf{Limitations}\\
\midrule
\endfirsthead

\toprule
\bottomrule
\endlastfoot

The study in \parencite{sun_generative_2025} 
& MIMIC-IV 
& LLaMA-3, GPT-4 
& NA & NA & NA & NA & 0.81 
& Hallucination risk  \\

The study in \parencite{aali_dataset_2025} 
& MIMIC-IV 
& LLaMA2-13B (fine-tuned) 
& NA & NA & NA & NA & NA 
& Small test set; complex pipeline; limited to BHC/DI sections  \\

The study in \parencite{van_veen_clinical_2023} 
& MIMIC-IV 
& FLAN-T5, BART
& 0.27 & 0.11 & 0.25 & 0.17 & 0.81 
& Domain gap affects lexical overlap  \\

\textcolor{red}{The study in  \parencite{damm_wispermed_2024}} 
& \textcolor{red}{MIMIC-IV} 
& \textcolor{red}{Phi3-mini, LLaMA-3.8B, Mistral-7B with LoRA and DES} 
& \textcolor{red}{0.453} & \textcolor{red}{0.201} & \textcolor{red}{0.308} & \textcolor{red}{0.403} & \textcolor{red}{0.438} 
& \textcolor{red}{Dynamic Expert Selection complexity; BHC section only}  \\

\textcolor{red}{The study in \parencite{he_shimo_2024}} 
& \textcolor{red}{MIMIC-IV} 
& \textcolor{red}{ClinicalT5-large with LoRA} 
& \textcolor{red}{0.394} & \textcolor{red}{0.131} & \textcolor{red}{0.312} & \textcolor{red}{0.351} & \textcolor{red}{0.252} 
& \textcolor{red}{Limited to BHC/DI sections; 9th place BioNLP 2024}  \\

\textcolor{red}{The study in \parencite{wu_epfl-make_2024}} 
& \textcolor{red}{MIMIC-IV} 
& \textcolor{red}{Meditron-7B} 
& \textcolor{red}{0.444} & \textcolor{red}{NA} & \textcolor{red}{NA} & \textcolor{red}{NA} & \textcolor{red}{NA} 
& \textcolor{red}{Limited to BHC/DI sections; 2nd place BioNLP 2024}  \\

\textcolor{red}{The study in  \parencite{naemi_benchmarking_2025}} 
& \textcolor{red}{MIMIC-IV} 
& \textcolor{red}{Gemma-3-27B (Extractive)} 
& \textcolor{red}{0.736} & \textcolor{red}{0.529} & \textcolor{red}{0.592} & \textcolor{red}{0.597} & \textcolor{red}{0.832} 
& \textcolor{red}{Automatic evaluation only; no clinician validation; extractive summarization setting} \\

\hline
\addlinespace
\multicolumn{9}{l}{\textbf{Our Proposed Methods}}\\

LLaMA-3-8B + LoRA 
& MIMIC-IV 
& Abstractive; LLaMA-3-8B with LoRA 
& \textbf{0.702} & \textbf{0.531} & \textbf{0.671} & \textbf{0.678} & \textbf{0.918} 
& Highest scores overall  \\

Mistral-7B + LoRA 
& MIMIC-IV 
& Abstractive; Mistral-7B with LoRA 
& 0.650 & 0.438 & 0.602 & 0.611 & 0.906 
& Slightly lower than LLaMA-3B  \\

\end{longtable}



\subsection{Practical Implications}
The findings of this study carry practical implications for the development and implementation of abstractive medical text summarization systems, especially in resource-constrained healthcare environments. First, the efficacy of LoRA addresses the challenges faced in resource-limited settings. Many hospitals, clinics, and smaller research laboratories often lack access to advanced GPUs and extensive computational clusters required for the comprehensive fine-tuning of models with billions of parameters. Second, LoRA facilitates rapid iteration and deployment. In the fast-evolving domain of medicine, new research, clinical guidelines, and patient data are continually emerging. The ability to quickly fine-tune models based on new datasets or adjust to shifting clinical requirements is crucial. Full fine-tuning, with its protracted training durations, can become a bottleneck. By reducing training time, LoRA enables quicker experimental cycles. Third, LoRA enhances the concept of model portability. Since LoRA adapters are relatively small megabytes, they are highly portable. This portability simplifies the process of sharing fine-tuned models with collaborators, distributing them as part of software packages, and deploying them on edge devices or in the cloud.

\subsection{Theoretical Implications}
\textcolor{red}{Our research provides empirical validation that parameter-efficient fine-tuning, previously demonstrated effective in general NLP tasks \parencite{hu_lora_2021,houlsby_parameter-efficient_2019}, extends successfully to the clinical domain.} We demonstrate that LoRA fine-tuning, which adjusts a smaller subset of model parameters, not only matches but consistently outperforms full fine-tuning across all evaluated architectures for clinical text summarization. \textcolor{red}{This confirms that targeted adaptation through low-rank updates effectively leverages pre-trained knowledge for domain-specific tasks, rather than suggesting a new paradigm.} Theoretically, LoRA appears to function as implicit regularization, preventing overfitting to limited clinical training data while mitigating catastrophic forgetting. \textcolor{red}{Our findings support the intrinsic dimensionality hypothesis \parencite{aghajanyan_intrinsic_2021} that task-specific adaptation occurs within low-dimensional subspaces rather than requiring modification of all model parameters.} Rather than learning clinical summarization from the ground up, fine-tuning appears to create low-rank updates that map the existing latent space of the model to clinical domain requirements.

\subsection{Scalability and Practical Applications}
Another important area of research focused on the scalability of LoRA-enhanced models. Practical applications in healthcare require the model to scale to larger datasets and more complicated medical queries without increasing processing resources. Even as the dataset size and complexity increased, the LoRA-enhanced model maintained good performance while scaling. Real-world AI models must efficiently analyze large-scale medical data; therefore, scalability is crucial. The study has practical applications in clinical decision support systems, patient education, and automated medical recording. Reliable and contextually appropriate responses from the LoRA-enhanced model help healthcare workers make educated decisions, improve patient outcomes, and streamline administrative duties. 

\subsection{Limitations and Future Research Directions}
In this study, our dataset is comprised exclusively of discharge summaries written in English, which may limit the generalizability to outpatient settings and non-English clinical notes. While this research offers valuable insights into the efficacy of LoRA for abstractive medical text summarization, it is essential to acknowledge several limitations that should be considered in future studies. In addition to LoRA, other PEFT techniques, such as adapters, prompt tuning, and prefix tuning, are worth exploring. Given the rapid advancement in the field of large language models, with some now encompassing hundreds of billions or even trillions of parameters, future research could investigate the application of LoRA to even more extensive models to further enhance performance and efficiency. \textcolor{red}{A major limitation of this study is the absence of validation by clinical experts.} While automatic evaluation metrics such as ROUGE, METEOR, and BERTScore provide valuable benchmarks for comparing model performance, they cannot \textcolor{red}{adequately} assess \textcolor{red}{clinical accuracy, factual consistency (potential hallucinations) in} generated summaries. A summary achieving high ROUGE scores may still contain factual inaccuracies \textcolor{red}{or omit critical clinical information, which could have serious consequences for patient safety}. \textcolor{red}{For any clinical deployment, validation by healthcare professionals is essential.} Therefore, \textcolor{red}{the highest priority for future work is conducting rigorous human evaluation with clinicians to assess factual accuracy, clinical relevance, completeness, and safety of generated summaries before real-world deployment.}
\section{Conclusion}
\textcolor{red}{This research evaluates LoRA parameter-efficient fine-tuning for clinical text summarization and provides evidence that increased trainable parameters do not necessarily correlate with improved performance.} By systematically evaluating graph-based extractive and 20 tests spanning six architectures, traditional transformers (BART, T5, PEGASUS-XSUM, FLAN-T5), and LLMs  (Mistral-7B, LLaMA-3-8B), we can demonstrate that parameter efficiency improves performance rather than degrades it. \textcolor{red}{Among traditional encoder-decoder transformers, FLAN-T5-LoRA achieves the highest performance across all evaluation metrics, followed by BART-LoRA and T5-LoRA with comparable results, while PEGASUS-XSUM-LoRA ranks lowest. FLAN-T5's instruction-tuning pre-training aligns well with structured summarization tasks, while BART's denoising pre-training objective provides effective adaptation for clinical text generation. LLaMA-3-8B with LoRA achieves the highest overall performance in the entire study, demonstrating strong clinical text summarization capability.} From this study, we confirmed that LoRA consistently outperformed both zero-shot inference and full parameter fine-tuning across all evaluated architectures. Based on this wide-ranging experiment, effectiveness suggests a specific performance ranking: LLMs utilizing LoRA surpass traditional transformers with LoRA, which outperform full fine-tuning, which outperform zero-shot neural techniques, which outperform extractive methods. LLM models demonstrated good understanding of clinical information, with their zero-shot results being better than traditional graph-based extractive methods and similar to fine-tuned classical transformers, indicating that their clinical reasoning is due to extensive pre-training. The computational efficiency improvements are noteworthy, with LoRA enabling GPU memory and storage reduction, reducing resource requirements for clinical AI and making it accessible to healthcare organizations of all sizes. This study demonstrates advances in clinical AI, shifting from resource-intensive methods to intelligent adaptation methods, extending beyond clinical text summarizing into the wider area of domain-specific AI adaptation. The success of LoRA through targeted learning, knowledge preservation, and implicit regularization establishes parameter-efficient fine-tuning as a standard for specialized AI applications requiring effective real-world deployment. Clinical NLP becomes more accessible to healthcare organizations worldwide through improved performance, computational efficiency, and deployment flexibility. 

\section*{Abbreviations}
\begin{flushleft}
\begin{longtable}{l l}
\textbf{AI} \dotfill & Artificial Intelligence \\
\textbf{BART} \dotfill & Bidirectional and Auto-Regressive Transformers \\
\textbf{BERTScore} \dotfill & BERT-based evaluation score for text similarity \\
\textbf{BHC/DI} \dotfill & Behavioral Health / Discharge Instructions \\
\textbf{CITI} \dotfill & Collaborative Institutional Training Initiative \\
\textbf{cTAKES} \dotfill & Clinical Text Analysis and Knowledge Extraction System \\
\textbf{EHRs} \dotfill & Electronic Health Records \\
\textbf{FLAN-T5} \dotfill & Fine-tuned Language Net T5 \\
\textbf{GPT} \dotfill & Generative Pre-trained Transformer \\
\textbf{ICD-10} \dotfill & International Classification of Diseases, 10th Revision \\
\textbf{LED} \dotfill & Longformer Encoder-Decoder \\
\textbf{LLaMA-3-8B} \dotfill & Large Language Model Meta AI (8B parameters) \\
\textbf{LLM} \dotfill & Large Language Model \\
\textbf{LoRA} \dotfill & Low-Rank Adaptation \\
\textbf{MIMIC} \dotfill & Medical Information Mart for Intensive Care \\
\textbf{Mistral-7B} \dotfill & Large Language Model (7B parameters) \\
\textbf{PEGASUS-XSUM} \dotfill & Pre-training with Extracted Gap-sentences for Abstractive Summarization \\
\textbf{PEFT} \dotfill & Parameter-Efficient Fine-Tuning \\
\textbf{PHI} \dotfill & Protected Health Information \\
\textbf{PLMs} \dotfill & Pretrained Language Models \\
\textbf{SBERT} \dotfill & Sentence-BERT \\
\textbf{T5} \dotfill & Text-to-Text Transfer Transformer \\
\textbf{TF-IDF} \dotfill & Term Frequency–Inverse Document Frequency \\
\textbf{UMLS} \dotfill & Unified Medical Language System 
\end{longtable}
\end{flushleft}

\vspace{-4.5em}

\section*{Acknowledgments} This work is partly supported by the Finnish Research Council Profi 7 Hybrid Intelligence, which is gratefully acknowledged. The first author is supported by a doctoral grant from the University of Oulu Graduate School (UniOGS). The authors also gratefully acknowledge the computational resources provided by CSC – IT Center for Science, Finland, which made this research possible. The experiments were conducted using Puhti and Mahti supercomputing clusters.

\section*{Declaration of Competing Interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
\section*{Data Availability}
We used data from the MIMIC-IV database, accessed under ethical approval and data use agreements. Due to privacy regulations and ethical considerations inherent to clinical data, the dataset cannot be made publicly available. 
\section*{Model Availability}
The fine-tuned models used in this study are publicly available in \href{https://huggingface.co/Aleka12} {huggingface}  to ensure reproducibility and support future research.

\printbibliography
\end{document}