\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[colorlinks=true, citecolor=blue, linkcolor=red, urlcolor=blue, pdfborder={0 0 0}]{hyperref}
\usepackage{caption}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{float}
\usepackage[utf8]{inputenc}  % Input encoding
\usepackage[T1]{fontenc}     % Font encoding


%\usepackage[backend=biber, style=ieeetr, sorting=nyt]{biblatex}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[scaled]{helvet}
\renewcommand{\familydefault}{\sfdefault}

\usepackage{cite} % For IEEE style citations
\bibliographystyle{ieeetr}
\usepackage[a4paper, left=1in, right=1in, top=1in, bottom=1in]{geometry}


\captionsetup[table]{labelformat=simple, labelsep=colon, textfont=normalfont, labelfont=bf}
\captionsetup[figure]{labelformat=simple, labelsep=colon, textfont=normalfont, labelfont=bf}


\date{} % Removes the default date

\begin{document}


\begin{itemize}
    \item We provide a systematic and exhaustive comparison of zero-shot, full fine-tuning, and LoRA strategies across a diverse set of state-of-the-art transformer models, large language models, and graph-based extractive summarization.
    \item The proposed model LLMs with LoRA fine-tuning achieved the highest clinical note summarization scores with prompt engineering.
    \item LoRA outperforms complete fine-tuning while using much fewer computing resources.
    \item Design a PEFT that offers a practical, scalable solution for resource-limited healthcare settings.
    \item 	The proposed LLAMA-3-8B shows superior performance in multi-strategy evaluation compared to transformers.
  
\end{itemize}




\maketitle


%\bibliography{references, FRabuil}
\end{document}
