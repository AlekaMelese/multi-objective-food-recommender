{
  "evaluation_type": "Top-K Recommendation Quality",
  "k": 10,
  "rating_threshold": 4.0,
  "alpha_values": [
    0.0,
    0.25,
    0.5,
    0.75,
    1.0
  ],
  "results": [
    {
      "alpha": 0.0,
      "precision_at_k": 0.004083526682134571,
      "recall_at_k": 0.034899458623356534,
      "f1_score": 0.0073115421696785055,
      "avg_health_topk": 0.913731831708594,
      "k": 10,
      "num_users_evaluated": 2155
    },
    {
      "alpha": 0.25,
      "precision_at_k": 0.004083526682134571,
      "recall_at_k": 0.034899458623356534,
      "f1_score": 0.0073115421696785055,
      "avg_health_topk": 0.9137287367794089,
      "k": 10,
      "num_users_evaluated": 2155
    },
    {
      "alpha": 0.5,
      "precision_at_k": 0.004083526682134571,
      "recall_at_k": 0.034899458623356534,
      "f1_score": 0.0073115421696785055,
      "avg_health_topk": 0.9136289798798839,
      "k": 10,
      "num_users_evaluated": 2155
    },
    {
      "alpha": 0.75,
      "precision_at_k": 0.003851508120649652,
      "recall_at_k": 0.03304331013147718,
      "f1_score": 0.006898886257405153,
      "avg_health_topk": 0.9085574633706188,
      "k": 10,
      "num_users_evaluated": 2155
    },
    {
      "alpha": 1.0,
      "precision_at_k": 0.004547563805104409,
      "recall_at_k": 0.038379737045630316,
      "f1_score": 0.008131622514307045,
      "avg_health_topk": 0.5883714041709953,
      "k": 10,
      "num_users_evaluated": 2155
    }
  ]
}